/*************************************************************************
 * Copyright (C) [2024] by Cambricon, Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#include "mlu.h"
#include "kernels/debug.h"
#include "kernels/kernel.h"
#include "kernels/utils/common.h"
#include "kernels/fft/fft.h"
#include "kernels/fft/fft_optm_device/fft_butterfly_ops.h"
#include "kernels/fft/fft_optm_device/fft_twiddle_factors.h"
#include "kernels/fft/fft_optm_device/fft_network_c2c_column.h"

// direction: 1 means IFFT, used to distinguish FFT and IFFT.
#define FFT_INVERSE 1

// Two split dimemsion(batch && L) trversal orders can be selected via
// "L_FIRST". By default "L" is preferred, then "batch".
#define L_FIRST 1
// #define DFTMTX_TLB_SIZE (32 * 16 * 2)
__nram__ char
    nram_buffer[MAX_NRAM_SIZE + REM_FOR_STACK - 32 * 1024 - FFT_MAXFACTORS * 4];
// __mlu_shared__ char sram_buffer[8192];  // radix-1024
__mlu_shared__ char sram_buffer[MAX_SRAM_SIZE];

__wram__ char wram_buffer[MAX_WRAM_SIZE];
// __wram__ char wram_buffer[64*64];

template <typename DT>
struct FFT_CPX_T {
  DT *r;
  DT *i;
};

// struct DFT_TLB_ENTRY {
//   DT *gdram_addr;
//   DT *nram_addr;
// };

// Generate w vector.
template <typename DT>
__mlu_func__ void genWSc1_opt(DT *w_r, DT *w_i, DT *tmp, DT *seq_addr,
                              const int &L, const int &L_sub, const int &part,
                              const int &unit_size, float scale, int n) {
  float inc_value = part * L_sub;
  int size_tmp_bytes = L_sub * unit_size;
  scale = scale / unit_size;
  __bang_add_scalar(tmp, seq_addr, inc_value, size_tmp_bytes);
  __bang_mul_scalar(tmp, tmp, scale, size_tmp_bytes);

#if __BANG_ARCH__ >= 372
  __bang_cos((float *)w_r, (float *)tmp, size_tmp_bytes);
  if (n <= 48000) {
    __bang_sin((float *)w_i, (float *)tmp, size_tmp_bytes);
  } else {
    // This function has higher precision, and the actual test determined n.
    __cn_vector_sin_f32(size_tmp_bytes, (float *)w_i, (float *)tmp);
  }
#endif
}

// Load input data from GDRAM to NRAM. The data source(src_in) is the
// calculation result of "mluOpBatchMatmulBcast". Different "fft_flag" has
// different layout, as follows:
//     RFFT:     src_in[batch, 2, L, powf(2, m)] = w[batch, 2, (L/2 + 1), l] *
//     in_ori[L, powf(2, m)] IRFFT:    src_in[2, 2, batch, L, powf(2, m)] = w[2,
//     batch, L, l] * in_ori[2, L, powf(2, m)] FFT_IFFT: src_in[batch, 2, L,
//     powf(2, m), 2] = w[batch, 2, L, l] * in_ori[L, powf(2, m), 2]
// "2," represents the real and imag part size of "w" or "src_in". According to
// the multicore splitting method, each "load" takes data block [1, *, L_sub,
// powf(2, m), *], using "memcpy_async" with stride. When input type is float,
// the address is used as defined by name: in(y_in_r, z_in_r, ...), ... When
// input type is half, the address interval is the same as float. The reason is
// that we perform bit width promotion calculations, considering the accuracy.
// Some temporarily unused space is used to ensure opertions such as
// "half2float" in "compute()" function.
template <typename DT>
__mlu_func__ void load(DT *y_in_r, DT *y_in_i, DT *z_in_r, DT *z_in_i,
                       DT *x_out1_r, DT *x_out1_i, DT *x_out2_r, DT *x_out2_i,
                       DT *wz_rr, DT *wz_ir, DT *matmul_re_mul_re_addr,
                       const int &n, const int &L, const int &L_sub,
                       const int &part_num, const int &pow_2_m,
                       const int &pow_2_m_half, const int &batch_x_part,
                       const int &fft_flag, const int &batch,
                       const int &op_size_align_via_L_dt,
                       const int &ping_pong) {
#if L_FIRST
  int b = batch_x_part / part_num;
  int part = batch_x_part % part_num;
#else
  int part = batch_x_part / batch;
  int b = batch_x_part % batch;
#endif
  int pingpong_offset = batch_x_part % 2 == 0 ? 0 : ping_pong;
  int L_re = L % L_sub;
  int L_deal = part < (part_num - 1) ? L_sub : (L_re != 0 ? L_re : L_sub);

  if (sizeof(DT) == sizeof(half)) {
    if (fft_flag == RFFT) {
      // Save the real part of RFFT, data shape is [1, 1, L_sub, powf(2, m)].
      x_out1_r = y_in_i;
      // Save the imag part of RFFT, data shape is the same as above.
      x_out1_i = wz_ir;
    } else if (fft_flag == IRFFT) {
      // Save the real * real part of IRFFT, data shape is [1, 1, 1, L_sub,
      // powf(2, m)].
      x_out1_r = x_out2_r;
      // Save the real * imag part of IRFFT, data shape is the same as above.
      x_out1_i = x_out2_i;
      // Save the imag * real part of IRFFT, data shape is the same as above.
      y_in_r = z_in_r;
      // Save the imag * imag part of IRFFT, data shape is the same as above.
      y_in_i = z_in_i;
    } else if (fft_flag == FFT_IFFT) {
      // Save the real * (real + imag) part of FFT_IFFT, data shape is
      // [1, 1, L_sub, powf(2, m), 2].
      y_in_r = y_in_i;
      // Save the imag * (real + imag) part of FFT_IFFT, data shape is the same
      // as above.
      wz_rr = wz_ir;
    }
  }
  x_out1_r += pingpong_offset;
  x_out1_i += pingpong_offset;
  y_in_r += pingpong_offset;
  y_in_i += pingpong_offset;

  if (fft_flag == RFFT) {
    int src_offset = L_sub * pow_2_m * part + b * n * 2;
    int data_size_bytes = pow_2_m * sizeof(DT);
    int total_data_size_bytes = L_deal * data_size_bytes;
    int distance_bytes = int((char *)x_out1_i - (char *)x_out1_r);
    if (part < part_num / 2 || part_num == 1) {
      __memcpy_async(x_out1_r, matmul_re_mul_re_addr + src_offset,
                     total_data_size_bytes, GDRAM2NRAM, distance_bytes,
                     n * sizeof(DT), 1);
    } else {
      // According to conjugate symmetry, only the first L/2+1 set of data is
      // calculated by "mluOpBatchMatmulBcast", and the second half of the data
      // eed to be calculated according to the coordinate mapping. This is the
      // reson why memcpy's src_str appears negative below.
      int ind_fwd = part * L_sub + L % 2;
      int src_offset = b * n * 2 + (L - ind_fwd + L % 2) * pow_2_m;
      __memcpy_async(x_out1_r,                            // dst addr
                     matmul_re_mul_re_addr + src_offset,  // src addr
                     data_size_bytes,                     // size
                     GDRAM2NRAM,                          // direction
                     data_size_bytes,                     // dst_stride o0
                     L_deal - 1,                          // dst_segnum o1
                     distance_bytes,                      // dst_stride o1
                     1,                                   // dst_segnum o2
                     -data_size_bytes,                    // src_stride i0
                     L_deal - 1,                          // src_segnum i1
                     n * sizeof(DT),                      // src_stride i1
                     1);                                  // src_segnum i2
    }
  } else if (fft_flag == IRFFT) {
    int total_data_size_bytes = L_deal * pow_2_m * sizeof(DT);
    DT *x[4] = {x_out1_r, x_out1_i, y_in_r, y_in_i};
    for (int addr_i = 0; addr_i < 4; addr_i++) {
      int complex_in = addr_i / 2;
      int complex_w = addr_i % 2;
      __memcpy_async(x[addr_i],
                     matmul_re_mul_re_addr +
                         complex_in * batch * 2 * L * pow_2_m +
                         b * 2 * L * pow_2_m + complex_w * L * pow_2_m +
                         part * L_sub * pow_2_m,
                     total_data_size_bytes, GDRAM2NRAM);
    }
  } else if (fft_flag == FFT_IFFT) {
    wz_rr += pingpong_offset;
    int src_offset = b * 2 * n * 2 + part * L_sub * pow_2_m * 2;
    int total_data_size_bytes = L_deal * pow_2_m * 2 * sizeof(DT);
    __memcpy_async(y_in_r, matmul_re_mul_re_addr + src_offset,
                   total_data_size_bytes, GDRAM2NRAM);
    __memcpy_async(wz_rr, matmul_re_mul_re_addr + src_offset + n * 2,
                   total_data_size_bytes, GDRAM2NRAM);
  }
}

template <typename DT, typename YT>
__mlu_func__ void preProcessRFFT(YT *y_in_r, YT *y_in_i, YT *x_out1_r,
                                 YT *x_out1_i, YT *wz_ir, const int &L_sub,
                                 const int &part_num, const int &pow_2_m,
                                 const int &part) {
  if (sizeof(DT) == sizeof(half)) {
    if (part >= part_num / 2 && part_num > 1) {
      // According to conjugate symmetry, it need to multiply the second half of
      // the imag part by -1.
      __bang_mul_scalar((DT *)wz_ir, (DT *)wz_ir, -1.0, pow_2_m * L_sub);
    }
    // Transpose L_sub to the lowest dimension for easy vector operations.
    __bang_transpose((DT *)x_out1_r, (DT *)y_in_i, L_sub, pow_2_m);
    __bang_transpose((DT *)x_out1_i, (DT *)wz_ir, L_sub, pow_2_m);
    // Convert to float, prepare for bitwidth promition calculation.
    __bang_half2float((float *)y_in_r, (half *)x_out1_r, L_sub * pow_2_m);
    __bang_half2float((float *)y_in_i, (half *)x_out1_i, L_sub * pow_2_m);
  } else {
    if (part >= part_num / 2 && part_num > 1) {
      // According to conjugate symmetry, it need to multiply the second half of
      // the imag part by -1.
      __bang_mul_scalar(x_out1_i, x_out1_i, -1.0, pow_2_m * L_sub);
    }
    // Transpose L_sub to the lowest dimension for easy vector operations.
    __bang_transpose(y_in_r, x_out1_r, L_sub, pow_2_m);
    __bang_transpose(y_in_i, x_out1_i, L_sub, pow_2_m);
  }
}

template <typename DT, typename YT>
__mlu_func__ void preProcessFFT_IFFT(YT *y_in_r, YT *y_in_i, YT *z_in_r,
                                     YT *x_out1_r, YT *x_out1_i, YT *wz_rr,
                                     YT *wz_ri, YT *wz_ir, const int &L_sub,
                                     const int &pow_2_m) {
  if (sizeof(DT) == sizeof(half)) {
    // Transpose L_sub to the lowest dimension for easy vector operations.
    __bang_transpose((DT *)y_in_r, (DT *)y_in_i, L_sub * pow_2_m, 2);
    __bang_transpose((DT *)wz_rr, (DT *)wz_ir, L_sub * pow_2_m, 2);
    // Compute the real part: src_in(real * real) - src_in(imag * imag).
    __bang_sub((DT *)y_in_r, (DT *)y_in_r, (DT *)wz_ri, L_sub * pow_2_m);
    // Compute the imag part: src_in(real * imag) - src_in(imag * real).
    __bang_add((DT *)wz_rr, (DT *)wz_rr, (DT *)z_in_r, L_sub * pow_2_m);
    // Transpose L_sub to the lowest dimension for easy vector operations.
    __bang_transpose((DT *)y_in_i, (DT *)y_in_r, L_sub, pow_2_m);
    __bang_transpose((DT *)wz_ir, (DT *)wz_rr, L_sub, pow_2_m);
    // Convert to float, prepare for bitwidth promition calculation.
    __bang_half2float((float *)y_in_r, (half *)y_in_i, L_sub * pow_2_m);
    __bang_half2float((float *)y_in_i, (half *)wz_ir, L_sub * pow_2_m);
  } else {
    // Transpose the read and imag parts to the highest dimension for easy
    // vector operations.
    __bang_transpose(x_out1_r, y_in_r, L_sub * pow_2_m, 2);
    __bang_transpose(y_in_r, wz_rr, L_sub * pow_2_m, 2);
    // Compute the real part: src_in(real * real) - src_in(imag * imag).
    __bang_sub(x_out1_r, x_out1_r, y_in_i, L_sub * pow_2_m);
    // Compute the imag part: src_in(real * imag) - src_in(imag * real).
    __bang_add(x_out1_i, x_out1_i, y_in_r, L_sub * pow_2_m);
    // Transpose L_sub to the lowest dimension for easy vector operations.
    __bang_transpose(y_in_r, x_out1_r, L_sub, pow_2_m);
    __bang_transpose(y_in_i, x_out1_i, L_sub, pow_2_m);
  }
}

template <typename DT, typename YT>
__mlu_func__ void preProcessIRFFT(YT *y_in_r, YT *y_in_i, YT *z_in_r,
                                  YT *z_in_i, YT *x_out1_r, YT *x_out1_i,
                                  YT *x_out2_r, YT *x_out2_i, YT *wz_ir,
                                  const int &L_sub, const int &pow_2_m) {
  if (sizeof(DT) == sizeof(half)) {
    // Compute the real part: src_in(real * real) - src_in(imag * imag).
    __bang_sub((DT *)x_out2_r, (DT *)x_out2_r, (DT *)z_in_i, L_sub * pow_2_m);
    // Compute the imag part: src_in(real * imag) - src_in(imag * real).
    __bang_add((DT *)x_out2_i, (DT *)x_out2_i, (DT *)z_in_r, L_sub * pow_2_m);
    // Transpose L_sub to the lowest dimension for easy vector operations.
    __bang_transpose((DT *)z_in_r, (DT *)x_out2_r, L_sub, pow_2_m);
    __bang_transpose((DT *)wz_ir, (DT *)x_out2_i, L_sub, pow_2_m);
    // Convert to float, prepare for bitwidth promition calculation.
    __bang_half2float((float *)y_in_r, (half *)z_in_r, L_sub * pow_2_m);
    __bang_half2float((float *)y_in_i, (half *)wz_ir, L_sub * pow_2_m);
  } else {
    // Compute the real part: src_in(real * real) - src_in(imag * imag).
    __bang_sub(x_out1_r, x_out1_r, y_in_i, L_sub * pow_2_m);
    // Compute the imag part: src_in(real * imag) - src_in(imag * real).
    __bang_add(x_out1_i, x_out1_i, y_in_r, L_sub * pow_2_m);
    // Transpose L_sub to the lowest dimension for easy vector operations.
    __bang_transpose(y_in_r, x_out1_r, L_sub, pow_2_m);
    __bang_transpose(y_in_i, x_out1_i, L_sub, pow_2_m);
  }
}

// Perform preprocessing for "compute()" function, including merging of real and
// imag parts, transposition and data types conversion, etc.
template <typename DT, typename YT>
__mlu_func__ void preProcess(YT *y_in_r, YT *y_in_i, YT *z_in_r, YT *z_in_i,
                             YT *x_out1_r, YT *x_out1_i, YT *x_out2_r,
                             YT *x_out2_i, YT *wz_rr, YT *wz_ri, YT *wz_ir,
                             const int &fft_flag, const int &L_sub,
                             const int &part_num, const int &pow_2_m,
                             const int &part) {
  if (fft_flag == RFFT) {
    preProcessRFFT<DT, float>((float *)y_in_r, (float *)y_in_i,
                              (float *)x_out1_r, (float *)x_out1_i,
                              (float *)wz_ir, L_sub, part_num, pow_2_m, part);
  } else if (fft_flag == FFT_IFFT) {
    preProcessFFT_IFFT<DT, float>(
        (float *)y_in_r, (float *)y_in_i, (float *)z_in_r, (float *)x_out1_r,
        (float *)x_out1_i, (float *)wz_rr, (float *)wz_ri, (float *)wz_ir,
        L_sub, pow_2_m);
  } else if (fft_flag == IRFFT) {
    preProcessIRFFT<DT, float>(
        (float *)y_in_r, (float *)y_in_i, (float *)z_in_r, (float *)z_in_i,
        (float *)x_out1_r, (float *)x_out1_i, (float *)x_out2_r,
        (float *)x_out2_i, (float *)wz_ir, L_sub, pow_2_m);
  }
}

template <typename DT, typename YT>
__mlu_func__ void computeOneLayer(YT *y_in_r, YT *y_in_i, YT *z_in_r,
                                  YT *z_in_i, YT *x_out1_r, YT *x_out1_i,
                                  YT *w_r, YT *w_i, YT *wz_rr, YT *wz_ri,
                                  YT *wz_ir, YT *wz_ii, const int &fft_flag,
                                  const int &L_sub, const int &part,
                                  const int &pow_2_m_half, const int &layer_num,
                                  int ln, int ln_pow2) {
  int basic_size = L_sub * ln_pow2;
  int group_size = basic_size * 2;
  int basic_group_num = pow_2_m_half / ln_pow2;
  int long_size_bytes = basic_size * basic_group_num;
  // Compute w * z_in: real * reaL, real * imag, imag * reaL, imag * imag.
  __bang_cycle_mul(wz_rr, z_in_r, w_r, long_size_bytes, basic_size);
  __bang_cycle_mul(wz_ri, z_in_i, w_r, long_size_bytes, basic_size);
  __bang_cycle_mul(wz_ir, z_in_r, w_i, long_size_bytes, basic_size);
  __bang_cycle_mul(wz_ii, z_in_i, w_i, long_size_bytes, basic_size);
  // Combine real and imag parts: real = real * real - imag * imag, imag = real
  // * imag + imag * real.
  __bang_sub(wz_rr, wz_rr, wz_ii, long_size_bytes);
  __bang_add(wz_ri, wz_ri, wz_ir, long_size_bytes);

  for (int bgn = 0; bgn < basic_group_num; bgn++) {
    int bgn_offset = basic_size * bgn;
    YT *y_r = y_in_r + bgn_offset;
    YT *y_i = y_in_i + bgn_offset;
    YT *x_r = x_out1_r + group_size * bgn;
    YT *x_i = x_out1_i + group_size * bgn;
    YT *wz_rr_tmp = wz_rr + bgn_offset;
    YT *wz_ri_tmp = wz_ri + bgn_offset;
    // Compute x_out1 = y_in + w * z_in.
    __bang_add(x_r, y_r, wz_rr_tmp, basic_size);
    __bang_add(x_i, y_i, wz_ri_tmp, basic_size);
    if (fft_flag == RFFT) {
      if (ln != layer_num - 1) {
        // Compute x_out2 = y_in - w * z_in.
        __bang_sub(x_r + basic_size, y_r, wz_rr_tmp, basic_size);
        __bang_sub(x_i + basic_size, y_i, wz_ri_tmp, basic_size);
      } else if (part == 0) {
        // According to conjugate symmetrym the last layer does not need to
        // calculate the second half part, except the point (n/2 + 1).
        *((YT *)x_r + basic_size) = *((YT *)y_r) - *((YT *)wz_rr_tmp);
        *((YT *)x_i + basic_size) = *((YT *)y_i) - *((YT *)wz_ri_tmp);
      }
    } else {
      // Compute x_out2 = y_in - w * z_in.
      __bang_sub(x_r + basic_size, y_r, wz_rr_tmp, basic_size);
      __bang_sub(x_i + basic_size, y_i, wz_ri_tmp, basic_size);
    }
  }
}

// Accoding to the merging rules of Stockham algorithm, calculate layer by
// layer. An examples is as follows:
//
// layer0   |layer1      |layer2            |layer3
// ---------|------------|------------------|-------------------------
// {0}      |{0, 4}      |{0, 4, 2, 6}      |{0, 4, 2, 6, 1, 5, 3, 7}
// {1}      |            |                  |
// {2}      |{1, 5}      |                  |
// {3}      |            |                  |
// {4}      |{2, 6}      |{1, 5, 2, 6}      |
// {5}      |            |                  |
// {6}      |{3, 7}      |                  |
// {7}      |            |                  |
//
// Each {*} represets a sequence of of complex numbers of length l. Each time
// the first half and the second half are merged, such as {0} and {4}, {0, 4}
// and {1, 6}. The first half is y_in, the second half is z_in, and the output
// is x_out*(the first half is x_out1, the second half is x_out2). The
// calculation formula(Butterfly Transform) is:
//     x_out1 = y_in + w * z_in
//     x_out2 = y_in - w * z_in
// w is calculted as follows: w_k = exp(-i * k * (2 * pi / N) * flag), k
// represents the k_th point, i represents real and imag part, N represents the
// total number of points, flag represents FFT type, 1 for RFFT and FFT, -1 for
// IRFFT and IFFT.
template <typename DT, typename YT>
__mlu_func__ void compute(YT *y_in_r, YT *y_in_i, YT *z_in_r, YT *z_in_i,
                          YT *x_out1_r, YT *x_out1_i, YT *x_out2_r,
                          YT *x_out2_i, YT *w_r, YT *w_i, YT *wz_rr, YT *wz_ri,
                          YT *wz_ir, YT *wz_ii, YT *seq_addr,
                          const int &fft_flag, const int &direction,
                          const int &n, const int &L, const int &L_sub,
                          const int &part_num, const int &pow_2_m,
                          const int &pow_2_m_half, const int &layer_num,
                          const int &op_size_align_via_L_dt, float scale,
                          const float scale_factor, const int &batch_x_part,
                          const int &batch, int ping_pong) {
#if L_FIRST
  int part = batch_x_part % part_num;
#else
  int part = batch_x_part / batch;
#endif
  if (sizeof(DT) == sizeof(half)) {
    // Because float type is acually used, the number of points is half of half
    // type.
    ping_pong = ping_pong / 2;
  }
  int pingpong_offset = batch_x_part % 2 == 0 ? 0 : ping_pong;
  y_in_r += pingpong_offset;
  y_in_i += pingpong_offset;
  z_in_r += pingpong_offset;
  z_in_i += pingpong_offset;
  x_out1_r += pingpong_offset;
  x_out1_i += pingpong_offset;
  x_out2_r += pingpong_offset;
  x_out2_i += pingpong_offset;
  w_r += pingpong_offset;
  w_i += pingpong_offset;
  wz_rr += pingpong_offset;
  wz_ri += pingpong_offset;
  wz_ir += pingpong_offset;
  wz_ii += pingpong_offset;
  preProcess<DT, float>((float *)y_in_r, (float *)y_in_i, (float *)z_in_r,
                        (float *)z_in_i, (float *)x_out1_r, (float *)x_out1_i,
                        (float *)x_out2_r, (float *)x_out2_i, (float *)wz_rr,
                        (float *)wz_ri, (float *)wz_ir, fft_flag, L_sub,
                        part_num, pow_2_m, part);

  // Calculate layer by layer as shown in the example.
  for (int ln = 0; ln < layer_num; ln++) {
    int ln_pow2 = powf(2, ln);
    // Generate w vector.
    genWSc1_opt<YT>(w_r, w_i, wz_ii, seq_addr, L, L_sub, part, ln_pow2, scale,
                    n);
    computeOneLayer<DT, float>(
        (float *)y_in_r, (float *)y_in_i, (float *)z_in_r, (float *)z_in_i,
        (float *)x_out1_r, (float *)x_out1_i, (float *)w_r, (float *)w_i,
        (float *)wz_rr, (float *)wz_ri, (float *)wz_ir, (float *)wz_ii,
        fft_flag, L_sub, part, pow_2_m_half, layer_num, ln, ln_pow2);

    // In order to avoid the data movement, the addr of input and output are
    // exchanged here.
    YT *tmp_y_r = y_in_r;
    YT *tmp_y_i = y_in_i;
    YT *tmp_z_r = z_in_r;
    YT *tmp_z_i = z_in_i;
    y_in_r = x_out1_r;
    y_in_i = x_out1_i;
    z_in_r = x_out2_r;
    z_in_i = x_out2_i;
    x_out1_r = tmp_y_r;
    x_out1_i = tmp_y_i;
    x_out2_r = tmp_z_r;
    x_out2_i = tmp_z_i;
  }

  if (fft_flag != IRFFT) {
    // Iranspose to the output save data format: the real and imag parts are at
    // the lowest dimention: [c, 2^M * L_sub] -> [2^M * L_sub, c]
    __bang_transpose(x_out1_r, y_in_r, pow_2_m * 2, L_sub);
    __bang_transpose(y_in_r, x_out1_r, L_sub * 2, pow_2_m);
  }
  if (scale_factor != 1.0) {
    __bang_mul_scalar(y_in_r, y_in_r, scale_factor, L_sub * 2 * pow_2_m);
  }
  if (sizeof(DT) == sizeof(half)) {
    __mluop_float2half((half *)y_in_r, (float *)y_in_r, L_sub * 2 * pow_2_m);
  }
}

// Store the calculation result to output. The difference between RFFT, IRFFT
// and FFT_IFFT can see in the description of "load()" function.
template <typename DT>
__mlu_func__ void store(DT *output, DT *y_in_r, DT *x_out1_r,
                        const int &pow_2_m, const int &pow_2_m_half,
                        const int &m, const int &L, const int &L_sub,
                        const int &part_num, const int &n, const int &out_n,
                        const int &batch_x_part, const int &batch,
                        const int &fft_flag, const int &ping_pong) {
#if L_FIRST
  int b = batch_x_part / part_num;
  int part = batch_x_part % part_num;
#else
  int part = batch_x_part / batch;
  int b = batch_x_part % batch;
#endif
  int pingpong_offset = batch_x_part % 2 == 0 ? 0 : ping_pong;
  int L_re = L % L_sub;
  int L_deal = part < (part_num - 1) ? L_sub : (L_re != 0 ? L_re : L_sub);
  int dst_offset = part * L_sub * 2;
  DT *out_nram = m % 2 == 0 ? y_in_r : x_out1_r;
  out_nram += pingpong_offset;
  if (fft_flag == RFFT) {
    int output_block = pow_2_m_half - 1;
    __memcpy_async(output + dst_offset + b * out_n * 2, out_nram,
                   L_deal * sizeof(DT) * 2, NRAM2GDRAM, L * sizeof(DT) * 2,
                   L_sub * sizeof(DT) * 2, output_block);
    if (part == 0) {
      int dst_one_point_offset = b * out_n * 2 + n;
      int src_one_point_offset = pow_2_m * L_sub;
      *(output + dst_one_point_offset) = *(out_nram + src_one_point_offset);
      *(output + dst_one_point_offset + 1) =
          *(out_nram + src_one_point_offset + 1);
    }
  } else if (fft_flag == IRFFT) {
    int dst_offset = part * L_sub;
    int output_block = pow_2_m - 1;
    __memcpy_async(output + dst_offset + b * out_n, out_nram,
                   L_deal * sizeof(DT), NRAM2GDRAM, L * sizeof(DT),
                   L_sub * sizeof(DT), output_block);
  } else if (fft_flag == FFT_IFFT) {
    int output_block = pow_2_m - 1;
    __memcpy_async(output + dst_offset + b * out_n * 2, out_nram,
                   L_deal * sizeof(DT) * 2, NRAM2GDRAM, L * sizeof(DT) * 2,
                   L_sub * sizeof(DT) * 2, output_block);
  }
}

// Generate an incremental sequence acorrding to the following rules:
//     1. the sequence length is L_sub*pow_2_m_half, means pow_2_m_half groups,
//     each group has L_sub
//        numbers.
//     2. the init_value of each group are 0, L, L*2, ..., L*pow_2_m_half.
// For FFT algorithm, a step called many times is vector operation: W * Z, where
// compute W requires two steps:
//     1. generate an incermental sequence.
//     2. perform sin && cos operation with scale on the incermental sequence.
// where, the sequence generated by step1 can be reused. Therefore, we save it
// in seq_addr.
__mlu_func__ void generateIncSequence(float *seq_addr, float *tmp_addr, int L,
                                      int L_sub, int pow_2_m_half) {
  __mluop_get_indices(PAD_UP(L_sub, NFU_ALIGN_SIZE), (float)0.0,
                      (float *)tmp_addr, nullptr, (float *)seq_addr, nullptr);
  // reduce call times of "__mluop_get_indices", which time is longer, by
  // using "for loop" and
  // "__bang_add_scalar".
  for (size_t i = 1; i < pow_2_m_half; i++) {
    int offset = i * L_sub;
    int init_value = i * L;
    __bang_add_scalar((float *)seq_addr + offset, (float *)seq_addr, init_value,
                      L_sub);
  }
}

// Onchip iterative calculation of Stockham algorithm. It is divided into three
// steps:
//    1. Load input data. RFFT, IRFFT and FFT_IFFT are processed different
//    because of different data
//       characteristics. See the "load()" function for details.
//    2. Compute data. Before the calculation, the data is put into a suitable
//    format through
//       "compute stream transpose", and then, the calculation is carried out
//       layer by layer according to the Stockham rules. Finally, through
//       "transpose", the real and imag parts that were calculated separately
//       are mixed. See the "compute()" function for details. (In order to
//       ensure the accuracy, the HALF type is calculated with a bit width
//       increase processing: HALF->FLOAT)
//    3. Store output data. See the "store()" function for details.
template <typename DT>
__mlu_func__ void computeMutiLayerOnchip(
    const AddrNode<DT> &addr, DT *matmul_re_mul_re_addr, DT *output,
    DT *seq_addr, int batch, int n, int m, int L, int fft_flag, int direction,
    int op_size_align_via_L_dt, int pow_2_m, int pow_2_m_half, int L_sub,
    const float scale_factor, int ping_pong) {
  // Generate an incremental sequence
  generateIncSequence((float *)seq_addr, (float *)addr.y_in_r, L, L_sub,
                      pow_2_m_half);
  // Calculate the fixed part of W scale.
  float scale = M_PI / L;
  scale *=
      (fft_flag == RFFT || (fft_flag == FFT_IFFT && direction != FFT_INVERSE))
          ? -1
          : 1;
  // When RFFT, using conjugate symmetry, do "BatchMatmulBcast" only on half of
  // the data, so the input n also becames half. int in_n       = fft_flag ==
  // RFFT ? int(PAD_UP(L, L_sub)/2 + 1) * pow_2_m : n;
  int in_n = fft_flag == RFFT ? int(PAD_UP(L / 2, L_sub) + 1) * pow_2_m : n;
  in_n = L <= L_sub ? n : in_n;
  // The obtain of out_n is the same as in_n, the difference is that no
  // alignment is performed.
  int out_n = fft_flag == RFFT ? n / 2 + 1 : n;
  // Input_size = batch * L * powf(2, m), NRAM can deal at least one "powf(2,
  // m)" at a time. Split "batch" and "L" between multi-core. "batch" processes
  // one at a time. Addording to the limit of NRAM size, "L" can be splitted
  // into "part_num" parts.
  int part_num = (L / L_sub) + (L % L_sub > 0 ? 1 : 0);
  // "total_num" blocks need to be processed.
  int total_num = part_num * batch;
  int repeat_num = total_num / taskDim;
  int remain_num = total_num % taskDim;
  if (repeat_num > 0 || taskId < remain_num) {
    // Each core needs to process "t_len" blocks, "remain_num" is evenly
    // assigned to the previous "remian_num" cores.
    int t_len = repeat_num + ((remain_num > 0 && taskId < remain_num) ? 1 : 0);
    // Calculate the offset of the block at each core.
    int t_start = taskId - remain_num <= 0
                      ? taskId * (repeat_num + 1)
                      : (remain_num * (repeat_num + 1) +
                         (taskId - remain_num) * repeat_num);
    int t_end = (t_start + t_len);
    MLULOG("taskId: %d, taskDim: %d\n", taskId, taskDim);
    MLULOG(
        "scale: %d, in_n: %d, out_n: %d, part_num: %d, total_num: %d, "
        "repeat_num: %d, "
        "remain_num: %d, t_len: %d, t_start: %d, t_end: %d\n",
        scale, in_n, out_n, part_num, total_num, repeat_num, remain_num, t_len,
        t_start, t_end);

    // Exectue three-stage pipeline operation(load: GDRAM2NRAM, compute, store:
    // NRAM2GDRAM) as follows: L1
    // -----------------sync
    // C1    L2
    // -----------------sync
    // S1    C2    L3
    // -----------------sync
    //       S2    C3
    // -----------------sync
    //             S3
    //             ...
    for (int t = t_start; t < t_end + 2; t++) {
      // Store output data.
      if (t >= t_start + 2) {
        store(output, addr.y_in_r, addr.x_out1_r, pow_2_m, pow_2_m_half, m, L,
              L_sub, part_num, n, out_n, t - 2, batch, fft_flag, ping_pong);
      }
      // Compute data layer by layer according to the Stockham rules.
      if (t >= t_start + 1 && t < t_end + 1) {
        compute<DT, float>(
            (float *)addr.y_in_r, (float *)addr.y_in_i, (float *)addr.z_in_r,
            (float *)addr.z_in_i, (float *)addr.x_out1_r,
            (float *)addr.x_out1_i, (float *)addr.x_out2_r,
            (float *)addr.x_out2_i, (float *)addr.w_r, (float *)addr.w_i,
            (float *)addr.wz_rr, (float *)addr.wz_ri, (float *)addr.wz_ir,
            (float *)addr.wz_ii, (float *)seq_addr, fft_flag, direction, n, L,
            L_sub, part_num, pow_2_m, pow_2_m_half, m, op_size_align_via_L_dt,
            scale, scale_factor, t - 1, batch, ping_pong);
      }
      // Load input data.
      if (t < t_end) {
        load(addr.y_in_r, addr.y_in_i, addr.z_in_r, addr.z_in_i, addr.x_out1_r,
             addr.x_out1_i, addr.x_out2_r, addr.x_out2_i, addr.wz_rr,
             addr.wz_ir, matmul_re_mul_re_addr, in_n, L, L_sub, part_num,
             pow_2_m, pow_2_m_half, t, fft_flag, batch, op_size_align_via_L_dt,
             ping_pong);
      }
      __sync();
    }
  }
}

// Divide the space size and call the onchip iterative calculation of Stockham
// algorithm.
template <typename DT>
__mlu_func__ void fftStockham(DT *matmul_re_mul_re_addr, DT *output,
                              int fft_flag, int direction, int n, int batch,
                              int L, int m, int L_sub,
                              const float scale_factor) {
  MLULOG(
      "batch: %d, n: %d, l: %d, m: %d, L_sub: %d, fft_flag: %d, direction: "
      "%d\n",
      batch, n, L, m, L_sub, fft_flag, direction);
  int pow_2_m = powf(2, m);
  // Number of L_sub processed by a src input at a time.
  int pow_2_m_half = pow_2_m / 2;
  // Double the number of inverval points in half type, because the bit width
  // lifing processing is required to ensure the accuracy.
  int half_multiplier = sizeof(DT) == sizeof(half) ? 2 : 1;
  // The length of an float input vector, such as "z_in_r" in "w_in_r * z_in_r"
  // below.
  int op_size_align_via_L_dt = pow_2_m_half * L_sub * half_multiplier;

  // NRAM Addr Info: "_r" represents the real part of the complex vector, "_i"
  // represents the imag part of the complex vector. The complex process is as
  // follows:
  //     x_out1 = y_in + w * z_in
  //     x_out2 = y_in - w * z_in
  AddrNode<DT> addr;

  // Input vector addr.
  addr.y_in_r = (DT *)nram_buffer;
  addr.z_in_r = addr.y_in_r + op_size_align_via_L_dt;
  addr.y_in_i = addr.z_in_r + op_size_align_via_L_dt;
  addr.z_in_i = addr.y_in_i + op_size_align_via_L_dt;

  // Output vector addr.
  addr.x_out1_r = addr.z_in_i + op_size_align_via_L_dt;
  addr.x_out2_r = addr.x_out1_r + op_size_align_via_L_dt;
  addr.x_out1_i = addr.x_out2_r + op_size_align_via_L_dt;
  addr.x_out2_i = addr.x_out1_i + op_size_align_via_L_dt;

  // W vector addr.
  addr.w_r = addr.x_out2_i + op_size_align_via_L_dt;
  addr.w_i = addr.w_r + op_size_align_via_L_dt;
  addr.wz_rr = addr.w_i + op_size_align_via_L_dt;
  addr.wz_ri = addr.wz_rr + op_size_align_via_L_dt;
  addr.wz_ir = addr.wz_ri + op_size_align_via_L_dt;
  addr.wz_ii = addr.wz_ir + op_size_align_via_L_dt;

  // From "addr.y_in_r" to "addr.wz_ii", each ping_pong needs 14 spaces for
  // three-stage pipeline operation.
  int ping_pong = op_size_align_via_L_dt * 14;
  // The public space stores the incremental sequence shared by ping_pong.
  DT *seq_addr = (DT *)nram_buffer + ping_pong * 2;

  computeMutiLayerOnchip(addr, matmul_re_mul_re_addr, output, seq_addr, batch,
                         n, m, L, fft_flag, direction, op_size_align_via_L_dt,
                         pow_2_m, pow_2_m_half, L_sub, scale_factor, ping_pong);
}

__mlu_global__ void MLUKernelFFTStockham(void *matmul_re_mul_re_addr,
                                         void *output, int fft_flag,
                                         int direction, int n, int batch, int L,
                                         int m, int L_sub, int dtype_size,
                                         const float scale_factor) {
  // if (__is_mpu()) return;
  switch (dtype_size) {
    default: {
      MLULOG("mluOpFFT Not Implemented.");
    }
    case (MLUOP_DTYPE_COMPLEX_FLOAT):
    case (MLUOP_DTYPE_FLOAT): {
      MLULOG("MLUOP_DTYPE_COMPLEX_FLOAT: MLUOP_DTYPE_FLOAT\n");
      fftStockham<float>((float *)matmul_re_mul_re_addr, (float *)output,
                         fft_flag, direction, n, batch, L, m, L_sub,
                         scale_factor);
    }; break;
    case (MLUOP_DTYPE_COMPLEX_HALF):
    case (MLUOP_DTYPE_HALF): {
      MLULOG("MLUOP_DTYPE_COMPLEX_HALF: MLUOP_DTYPE_HALF\n");
      fftStockham<half>((half *)matmul_re_mul_re_addr, (half *)output, fft_flag,
                        direction, n, batch, L, m, L_sub, scale_factor);
    }; break;
  }
}

mluOpStatus_t MLUOP_WIN_API
kernelFFTStockham(cnrtDim3_t k_dim, cnrtFunctionType_t k_type,
                  cnrtQueue_t queue, mluOpFFTPlan_t fft_plan, int direction,
                  const float scale_factor, FFTFlag flag) {
  VLOG(5) << "Launch Kernel MLUKernelFFTStockham<<Union" << k_type / CORE_DIM
          << ", " << k_dim.x << ", " << k_dim.y << ", " << k_dim.z << ">>>";
  KERNEL_CHECK((MLUKernelFFTStockham<<<k_dim, k_type, queue>>>(
      fft_plan->matmul_addrs.matmul_re_mul_re_addr,
      fft_plan->matmul_addrs.output_contiguous_addr, flag,
      direction,  // direction, -1 means invalid(only FFT_IFFT use).
      fft_plan->n[0], fft_plan->batch, fft_plan->L, fft_plan->m,
      fft_plan->L_sub, fft_plan->output_dtype, scale_factor)));
  return MLUOP_STATUS_SUCCESS;
}

template <typename DT>
__mlu_func__ void fft_swap_ptr(DT **X, DT **Y) {
  DT *Z = *X;
  *X = *Y;
  *Y = Z;
}

// __nram__ uint8_t* _A[4096];

template <typename DT>
__mlu_func__ void computeRadix3ButterflyFirststage_v1(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, int section_num, int butterfly_num, int in_stride,
    int dir) {
  // outplace(nram)

  // DT * scratch_in_r0 = nram_in_r;
  // DT * scratch_in_r1 = &nram_in_r[3];
  // DT * scratch_in_r2 = &nram_in_r[6];

  // DT * scratch_in_i0 = nram_in_i;
  // DT * scratch_in_i1 = &nram_in_i[3];
  // DT * scratch_in_i2 = &nram_in_i[6];

  const int sign = (dir == FFT_FORWARD) ? 1 : -1;
  const DT tw3_1i = sign * TW3_1I_F;

  DT *Fin_r[3] = {nram_in_r, &nram_in_r[butterfly_num],
                  &nram_in_r[butterfly_num * 2]};
  DT *Fin_i[3] = {nram_in_i, &nram_in_i[butterfly_num],
                  &nram_in_i[butterfly_num * 2]};
  DT *scratch_r[4] = {nram_scratch, &nram_scratch[butterfly_num],
                      &nram_scratch[butterfly_num * 2],
                      &nram_scratch[butterfly_num * 3]};
  DT *scratch_i[4] = {
      &nram_scratch[butterfly_num * 4], &nram_scratch[butterfly_num * 5],
      &nram_scratch[butterfly_num * 6], &nram_scratch[butterfly_num * 7]};

  DT *Fout_r[3] = {&nram_scratch[butterfly_num * 8],
                   &nram_scratch[butterfly_num * 9],
                   &nram_scratch[butterfly_num * 10]};
  DT *Fout_i[3] = {&nram_scratch[butterfly_num * 11],
                   &nram_scratch[butterfly_num * 12],
                   &nram_scratch[butterfly_num * 13]};

  DT *_A_r = &nram_scratch[butterfly_num * 14];
  DT *_A_i = &nram_scratch[butterfly_num * 15];
  DT *_B_r = &nram_scratch[butterfly_num * 16];
  DT *_B_i = &nram_scratch[butterfly_num * 17];
  // CPX_ADD
  __bang_add(scratch_r[0], Fin_r[1], Fin_r[2], butterfly_num);
  __bang_add(scratch_i[0], Fin_i[1], Fin_i[2], butterfly_num);

  // CPX_SUB
  __bang_sub(scratch_r[1], Fin_r[1], Fin_r[2], butterfly_num);
  __bang_sub(scratch_i[1], Fin_i[1], Fin_i[2], butterfly_num);

  // CPX_ADD
  __bang_add(Fout_r[0], scratch_r[0], Fin_r[0], butterfly_num);
  __bang_add(Fout_i[0], scratch_i[0], Fin_i[0], butterfly_num);

  // for (int j = 0; j < 15; j++) {
  //   MLULOG("Fin_r r: %f.\n", Fin_r[0][j]);
  // }

  // for (int j = 0; j < 15; j++) {
  //   MLULOG("Fin_i i: %f.\n", Fin_i[0][j]);
  // }

  // for (int j = 0; j < 15; j++) {
  //   MLULOG("Fin_r r: %f.\n", Fin_r[1][j]);
  // }

  // for (int j = 0; j < 15; j++) {
  //   MLULOG("Fin_i i: %f.\n", Fin_i[1][j]);
  // }

  // CPX_MLA_OUTPLACE
  __bang_fusion(FUSION_FMA, _A_r, scratch_r[0], TW3_1R_F, Fin_r[0],
                butterfly_num, butterfly_num);
  __bang_fusion(FUSION_FMA, _A_i, scratch_i[0], TW3_1R_F, Fin_i[0],
                butterfly_num, butterfly_num);
  // for (int j = 0; j < 15; j++) {
  //   MLULOG("_A_r r: %f.\n", _A_r[j]);
  // }

  // for (int j = 0; j < 15; j++) {
  //   MLULOG("_A_i i: %f.\n", _A_i[j]);
  // }
  // CPX_MUL_S
  __bang_mul_scalar(_B_r, scratch_r[1], tw3_1i, butterfly_num);
  __bang_mul_scalar(_B_i, scratch_i[1], tw3_1i, butterfly_num);
  // for (int j = 0; j < 15; j++) {
  //   MLULOG("_B_r r: %f.\n", _B_r[j]);
  // }

  // for (int j = 0; j < 15; j++) {
  //   MLULOG("_B_i i: %f.\n", _B_i[j]);
  // }
  // OPENFFT_CPX_ODD_OUT
  __bang_sub(Fout_r[1], _A_r, _B_i, butterfly_num);
  __bang_add(Fout_i[1], _A_i, _B_r, butterfly_num);
  __bang_add(Fout_r[2], _A_r, _B_i, butterfly_num);
  __bang_sub(Fout_i[2], _A_i, _B_r, butterfly_num);

  // for (int j = 0; j < 15; j++) {
  //   MLULOG("Fout_r r: %f.\n", Fout_r[j]);
  // }

  // for (int j = 0; j < 15; j++) {
  //   MLULOG("Fout_i i: %f.\n", Fout_i[j]);
  // }

  // output
  __bang_transpose(nram_out_r, Fout_r[0], 3, butterfly_num);
  __bang_transpose(nram_out_i, Fout_i[0], 3, butterfly_num);

  // MLULOG("butterfly_num---------: %d.\n", butterfly_num);
  // for (int j = 0; j < 3*butterfly_num; j++) {
  //   MLULOG("Fout[%d]: (%f, %f)\n", j, nram_out_r[j], nram_out_i[j]);
  // }

  // __memcpy(scratch_in_r0, nram_out_r, butterfly_num* sizeof(DT))

  // __nram__ FFT_CPX_T<DT>* _A[butterfly_num];
  // __nram__ FFT_CPX_T<DT>* _B[butterfly_num];

  // for (; butterfly_num > 0; --butterfly_num)
  // {

  // scratch_in[0] = Fin[0];
  // scratch_in[1] = Fin[in_stride];
  // scratch_in[2] = Fin[2 * in_stride];

  // _CRADIX(R3_KERNEL)(scratch_out, scratch_in, tw3_1i);

  // Fout[0] = scratch_out[0];
  // Fout[1] = scratch_out[1];
  // Fout[2] = scratch_out[2];

  // Fin += offset;
  // Fout += 3;

  // input += in_stride;
  // output += 3;
  // }
}

template <typename DT>
__mlu_func__ void computeRadix3ButterflyFirststage(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, int section_num, int butterfly_num, int in_stride,
    int dir) {
  // outplace(nram)

  const int sign = (dir == FFT_FORWARD) ? 1 : -1;
  const DT tw3_1i = sign * TW3_1I_F;

  FFT_CPX_T<DT> scratch[4];
  int nram_scratch_offset = 0;
  for (int i = 0; i < 4; i++) {
    scratch[i].r = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
    scratch[i].i = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
  }

  FFT_CPX_T<DT> Fin[3];
  int nram_in_offset = 0;
  for (int i = 0; i < 3; i++) {
    Fin[i].r = &nram_in_r[nram_in_offset];
    Fin[i].i = &nram_in_i[nram_in_offset];
    nram_in_offset += butterfly_num;
  }

  FFT_CPX_T<DT> Fout[3];
  // int nram_out_offset = nram_scratch_offset;

  // seperate the space for: Fout[i].r  Fout[i].i
  for (int i = 0; i < 3; i++) {
    Fout[i].r = &nram_scratch[nram_scratch_offset];
    Fout[i].i = &nram_scratch[nram_scratch_offset + butterfly_num * 3];
    nram_scratch_offset += butterfly_num;
  }
  nram_scratch_offset += (butterfly_num * 3);

  // __sync();
  FFT_CPX_T<DT> _A = {&nram_scratch[nram_scratch_offset],
                      &nram_scratch[nram_scratch_offset + butterfly_num]};
  FFT_CPX_T<DT> _B = {&nram_scratch[nram_scratch_offset + butterfly_num * 2],
                      &nram_scratch[nram_scratch_offset + butterfly_num * 3]};

  // FFT_CPX_T<DT> in_0 = {&nram_scratch[nram_scratch_offset + butterfly_num *
  // 4],
  //                       &nram_scratch[nram_scratch_offset + butterfly_num *
  //                       5]};
  nram_scratch_offset += (butterfly_num * 6);
  // FFT_CPX_T<DT> in_0 = Fin[0];
  //   MLU_CPX_ADD(scratch[0], Fin[1], Fin[2], butterfly_num);
  //   MLU_CPX_SUB(scratch[1], Fin[1], Fin[2], butterfly_num);

  //   MLU_CPX_ADD(Fout[0], scratch[0], in_0, butterfly_num);

  //   MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW3_1R_F, butterfly_num);
  //   MLU_CPX_MUL_S(_B, scratch[1],  tw3_1i, butterfly_num);
  //   MLU_CPX_ODD_OUT(Fout[1], Fout[2], _A, _B, butterfly_num);

  MLU_CPX_ADD(scratch[0], Fin[1], Fin[2], butterfly_num);
  MLU_CPX_SUB(scratch[1], Fin[1], Fin[2], butterfly_num);

  MLU_CPX_ADD(Fout[0], scratch[0], Fin[0], butterfly_num);

  MLU_CPX_MLA_OUTPLACE(_A, Fin[0], scratch[0], TW3_1R_F, butterfly_num);
  MLU_CPX_MUL_S(_B, scratch[1], tw3_1i, butterfly_num);
  MLU_CPX_ODD_OUT(Fout[1], Fout[2], _A, _B, butterfly_num);

  // output
  // __bang_transpose(nram_out_r, Fout_r[0], 3, butterfly_num);
  // __bang_transpose(nram_out_i, Fout_i[0], 3, butterfly_num);
  __bang_transpose(nram_out_r, Fout[0].r, 3, butterfly_num);
  __bang_transpose(nram_out_i, Fout[0].i, 3, butterfly_num);
}

template <typename DT>
__mlu_func__ void computeRadix9ButterflyFirststage(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, int section_num, int butterfly_num, int in_stride,
    int dir) {
  // outplace(nram)

  const int sign = (dir == FFT_FORWARD) ? 1 : -1;
  const DT tw9_1i = sign * TW9_1I_F;
  const DT tw9_2i = sign * TW9_2I_F;
  const DT tw9_3i = sign * TW9_3I_F;
  const DT tw9_4i = sign * TW9_4I_F;

  FFT_CPX_T<DT> scratch[10];
  int nram_scratch_offset = 0;
  for (int i = 0; i < 10; i++) {
    scratch[i].r = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
    scratch[i].i = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
  }

  FFT_CPX_T<DT> Fin[9];
  int nram_in_offset = 0;
  for (int i = 0; i < 9; i++) {
    Fin[i].r = &nram_in_r[nram_in_offset];
    Fin[i].i = &nram_in_i[nram_in_offset];
    nram_in_offset += butterfly_num;
  }

  FFT_CPX_T<DT> Fout[9];
  // int nram_out_offset = nram_scratch_offset;

  // seperate the space for: Fout[i].r  Fout[i].i
  for (int i = 0; i < 9; i++) {
    Fout[i].r = &nram_scratch[nram_scratch_offset];
    Fout[i].i = &nram_scratch[nram_scratch_offset + butterfly_num * 9];
    nram_scratch_offset += butterfly_num;
  }
  nram_scratch_offset += (butterfly_num * 9);

  FFT_CPX_T<DT> _A = {&nram_scratch[nram_scratch_offset],
                      &nram_scratch[nram_scratch_offset + butterfly_num]};
  FFT_CPX_T<DT> _B = {&nram_scratch[nram_scratch_offset + butterfly_num * 2],
                      &nram_scratch[nram_scratch_offset + butterfly_num * 3]};

  FFT_CPX_T<DT> in_0 = {&nram_scratch[nram_scratch_offset + butterfly_num * 4],
                        &nram_scratch[nram_scratch_offset + butterfly_num * 5]};
  nram_scratch_offset += (butterfly_num * 6);

  FFT_CPX_T<DT> _A_TEMP = {&nram_scratch[nram_scratch_offset],
                           &nram_scratch[nram_scratch_offset + butterfly_num]};
  FFT_CPX_T<DT> _B_TEMP = {
      &nram_scratch[nram_scratch_offset + butterfly_num * 2],
      &nram_scratch[nram_scratch_offset + butterfly_num * 3]};

  __bang_move(in_0.r, Fin[0].r, butterfly_num * sizeof(DT));
  __bang_move(in_0.i, Fin[0].i, butterfly_num * sizeof(DT));

  MLU_CPX_ADD(scratch[0], Fin[1], Fin[8], butterfly_num);
  MLU_CPX_SUB(scratch[1], Fin[1], Fin[8], butterfly_num);
  MLU_CPX_ADD(scratch[2], Fin[2], Fin[7], butterfly_num);
  MLU_CPX_SUB(scratch[3], Fin[2], Fin[7], butterfly_num);
  MLU_CPX_ADD(scratch[4], Fin[3], Fin[6], butterfly_num);
  MLU_CPX_SUB(scratch[5], Fin[3], Fin[6], butterfly_num);
  MLU_CPX_ADD(scratch[6], Fin[4], Fin[5], butterfly_num);
  MLU_CPX_SUB(scratch[7], Fin[4], Fin[5], butterfly_num);

  MLU_CPX_ADD(scratch[9], scratch[0], scratch[2], butterfly_num);
  MLU_CPX_ADD(scratch[9], scratch[4], scratch[9], butterfly_num);
  MLU_CPX_ADD(scratch[9], scratch[6], scratch[9], butterfly_num);
  MLU_CPX_ADD(Fout[0], scratch[9], in_0, butterfly_num);

  MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_1R_F, butterfly_num);
  MLU_CPX_MUL_S(_B, scratch[1], tw9_1i, butterfly_num);
  MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_2R_F, _A_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_B, scratch[3], tw9_2i, _B_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_B, scratch[5], tw9_3i, _B_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_4R_F, _A_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_B, scratch[7], tw9_4i, _B_TEMP, butterfly_num);
  MLU_CPX_ODD_OUT(Fout[1], Fout[8], _A, _B, butterfly_num);

  MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_2R_F, butterfly_num);
  MLU_CPX_MUL_S(_B, scratch[1], tw9_2i, butterfly_num);
  MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_4R_F, _A_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_B, scratch[3], tw9_4i, _B_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_B, scratch[5], -tw9_3i, _B_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_1R_F, _A_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_B, scratch[7], -tw9_1i, _B_TEMP, butterfly_num);
  MLU_CPX_ODD_OUT(Fout[2], Fout[7], _A, _B, butterfly_num);

  MLU_CPX_ADD(_A, in_0, scratch[4], butterfly_num);
  MLU_CPX_ADD(_B, scratch[0], scratch[2], butterfly_num);
  MLU_CPX_ADD(_B, scratch[6], _B, butterfly_num);
  MLU_CPX_MLA_OUTPLACE(_A, _A, _B, TW9_3R_F, butterfly_num);
  MLU_CPX_SUB(_B, scratch[1], scratch[3], butterfly_num);
  MLU_CPX_ADD(_B, scratch[7], _B, butterfly_num);
  MLU_CPX_MUL_S(_B, _B, tw9_3i, butterfly_num);
  MLU_CPX_ODD_OUT(Fout[3], Fout[6], _A, _B, butterfly_num);

  MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_4R_F, butterfly_num);
  MLU_CPX_MUL_S(_B, scratch[1], tw9_4i, butterfly_num);
  MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_1R_F, _A_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_B, scratch[3], -tw9_1i, _B_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_B, scratch[5], tw9_3i, _B_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_2R_F, _A_TEMP, butterfly_num);
  MLU_CPX_MLA_INPLACE(_B, scratch[7], -tw9_2i, _B_TEMP, butterfly_num);
  MLU_CPX_ODD_OUT(Fout[4], Fout[5], _A, _B, butterfly_num);

  // output
  __bang_transpose(nram_out_r, Fout[0].r, 9, butterfly_num);
  __bang_transpose(nram_out_i, Fout[0].i, 9, butterfly_num);
}

// template <typename DT>
// __mlu_func__ void computeRadix9ButterflyFirststageMat(
//     DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
//     DT *nram_scratch, DT *nram_dftmtx, int section_num, int butterfly_num,
//     int in_stride, int dir) {
//   // outplace(nram)
//   const int radix = 9;
//   int nram_scratch_offset = 0;
//   int wram_scratch_offset = 0;
//   DT *wram_sratch = (DT *)wram_buffer;

//   const int align_radix = 16;
//   FFT_CPX_T<DT> in_trans_wram = {
//       &wram_sratch[wram_scratch_offset],
//       &wram_sratch[wram_scratch_offset + butterfly_num * align_radix]};

//   wram_scratch_offset += (butterfly_num * align_radix * 2);

//   FFT_CPX_T<DT> in_trans = {
//       &nram_scratch[nram_scratch_offset],
//       &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
//   FFT_CPX_T<DT> out = in_trans;
//   nram_scratch_offset += (butterfly_num * radix * 2);

//   FFT_CPX_T<DT> in_trans_align = {
//       &nram_scratch[nram_scratch_offset],
//       &nram_scratch[nram_scratch_offset + butterfly_num * align_radix]};
//   nram_scratch_offset += (butterfly_num * align_radix * 2);
//   // FFT_CPX_T<DT> out = in_trans;

//   FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};

//   // for(int i=0; i<9; i++){
//   //   for(int j=0; j<9; j++){
//   //     MLULOG("nram_dftmtx[%d][%d]: (%f, %f) ", i,j, nram_dftmtx[i*9+j],
//   //     nram_dftmtx[i*9+j+81]);
//   //   }
//   //   MLULOG("\n");
//   // }

//   DT *RR = &nram_scratch[nram_scratch_offset];
//   DT *RI = &nram_scratch[nram_scratch_offset +
//                          butterfly_num * align_radix];
//   DT *IR = &nram_scratch[nram_scratch_offset +
//                          butterfly_num * align_radix * 2];
//   DT *II = &nram_scratch[nram_scratch_offset +
//                          butterfly_num * align_radix * 3];

//   nram_scratch_offset += (butterfly_num * 4 * align_radix);

//   __bang_transpose(in_trans.r, nram_in_r, radix, butterfly_num);
//   __bang_transpose(in_trans.i, nram_in_i, radix, butterfly_num);
//   // __bang_xor(in_trans_align.r, in_trans_align)
//   __bang_write_zero(in_trans_align.r, align_radix * butterfly_num * 2);

//   __memcpy(in_trans_align.r, in_trans.r, radix * sizeof(DT), NRAM2NRAM,
//            align_radix * sizeof(DT), radix * sizeof(DT), butterfly_num * 2 -
//            1);

//   // __memcpy(in_trans_wram.r, in_trans.r, radix * butterfly_num * sizeof(DT)
//   *
//   // 2, NRAM2WRAM);

//   __memcpy(in_trans_wram.r, in_trans_align.r,
//            align_radix * butterfly_num * sizeof(DT), NRAM2WRAM);
//   __memcpy(in_trans_wram.i, in_trans_align.i,
//            align_radix * butterfly_num * sizeof(DT), NRAM2WRAM);
//   // __bang_transpose(in_trans_wram.i, in_trans.i, radix, butterfly_num);

//   __bang_matmul((float *)RR, (float *)dftmtx.r, (float *)in_trans_wram.r,
//   radix,
//                 align_radix, butterfly_num);
//   __bang_matmul((float *)RI, (float *)dftmtx.r, (float *)in_trans_wram.i,
//   radix,
//                 align_radix, butterfly_num);
//   __bang_matmul((float *)IR, (float *)dftmtx.i, (float *)in_trans_wram.r,
//   radix,
//                 align_radix, butterfly_num);
//   __bang_matmul((float *)II, (float *)dftmtx.i, (float *)in_trans_wram.i,
//   radix,
//                 align_radix, butterfly_num);

//   __bang_sub(out.r, RR, II, butterfly_num * radix);
//   __bang_add(out.i, RI, IR, butterfly_num * radix);

//   //  for(int i=0; i<9; i++){
//   //   for(int j=0; j<9; j++){
//   //     MLULOG("out[%d][%d]: (%f, %f) ", i,j, out.r[i*9+j],
//   //     out.i[i*9+j+butterfly_num*radix]);
//   //   }
//   //   MLULOG("\n");
//   // }

//   // output
//   __bang_transpose(nram_out_r, out.r, radix, butterfly_num);
//   __bang_transpose(nram_out_i, out.i, radix, butterfly_num);
// }

// matmul sample
template <typename DT>
__mlu_func__ void computeRadix9ButterflyFirststageMat_v1(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, int section_num, int butterfly_num,
    int in_stride, int dir) {
  // outplace(nram)
  const int radix = 9;
  const int align_radix = 16;
  const int align_N = 64;
  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> dftmtx_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_radix * align_N]};

  wram_scratch_offset += (align_radix * align_radix * 2);

  FFT_CPX_T<DT> dftmtx_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_radix]};
  nram_scratch_offset += (align_N * align_radix * 2);

  FFT_CPX_T<DT> dftmtx_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_radix]};
  nram_scratch_offset += (align_N * align_radix * 2);

  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * align_radix]};
  // FFT_CPX_T<DT> out = in_align;
  nram_scratch_offset += (butterfly_num * align_radix * 2);

  // overlap
  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
  FFT_CPX_T<DT> out = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * align_N]};

  nram_scratch_offset += (butterfly_num * align_N * 2);

  // FFT_CPX_T<DT> out = in_trans;

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};

  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + butterfly_num * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + butterfly_num * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + butterfly_num * align_N * 3];

  nram_scratch_offset += (butterfly_num * 4 * align_N);

  __bang_transpose(in_trans.r, nram_in_r, radix, butterfly_num);
  __bang_transpose(in_trans.i, nram_in_i, radix, butterfly_num);

  __bang_pad(dftmtx_align.r, dftmtx.r, 1, radix, radix, 0, align_N - radix, 0,
             align_radix - radix);
  __bang_pad(dftmtx_align.i, dftmtx.i, 1, radix, radix, 0, align_N - radix, 0,
             align_radix - radix);

  if (butterfly_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, butterfly_num, radix, 0, 0, 0,
               align_radix - radix);
    __bang_pad(in_align.i, in_trans.i, 1, butterfly_num, radix, 0, 0, 0,
               align_radix - radix);

  } else {
    __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_radix - radix, 0,
               0);
    __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_radix - radix, 0,
               0);
  }

  __bang_reshape_filter(dftmtx_align2.r, dftmtx_align.r, align_N, 1, 1,
                        align_radix);
  __bang_reshape_filter(dftmtx_align2.i, dftmtx_align.i, align_N, 1, 1,
                        align_radix);

  __memcpy(dftmtx_wram.r, dftmtx_align2.r, align_N * align_radix * sizeof(DT),
           NRAM2WRAM);
  __memcpy(dftmtx_wram.i, dftmtx_align2.i, align_N * align_radix * sizeof(DT),
           NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)in_align.r, (float *)dftmtx_wram.r,
                butterfly_num, align_radix, align_N);
  __bang_matmul((float *)RI, (float *)in_align.r, (float *)dftmtx_wram.i,
                butterfly_num, align_radix, align_N);
  __bang_matmul((float *)IR, (float *)in_align.i, (float *)dftmtx_wram.r,
                butterfly_num, align_radix, align_N);
  __bang_matmul((float *)II, (float *)in_align.i, (float *)dftmtx_wram.i,
                butterfly_num, align_radix, align_N);

  __bang_sub(out.r, RR, II, butterfly_num * align_N);
  __bang_add(out.i, RI, IR, butterfly_num * align_N);

  __memcpy(nram_out_r, out.r, radix * sizeof(DT), NRAM2NRAM, radix * sizeof(DT),
           align_N * sizeof(DT), butterfly_num - 1);
  __memcpy(nram_out_i, out.i, radix * sizeof(DT), NRAM2NRAM, radix * sizeof(DT),
           align_N * sizeof(DT), butterfly_num - 1);
}

// matmul sample
template <typename DT>
__mlu_func__ void computeRadix9ButterflyFirststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, int section_num, int butterfly_num,
    int in_stride, int dir) {
  // outplace(nram)

  // origin: M = radix, K = radix, N =butterfly_num
  // pad_up:
  const int radix = 9;
  const int align_M = radix;  // no align
  const int align_K = 16;
  const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> dftmtx_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_K]};
  nram_scratch_offset += (align_M * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
  nram_scratch_offset += (butterfly_num * radix * 2);

  // FFT_CPX_T<DT> out = in_trans;

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};

  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  __bang_transpose(in_trans.r, nram_in_r, radix, butterfly_num);
  __bang_transpose(in_trans.i, nram_in_i, radix, butterfly_num);

  // right pad radix -> align_K
  __bang_pad(dftmtx_align.r, dftmtx.r, 1, radix, radix, 0, 0, 0,
             align_K - radix);
  __bang_pad(dftmtx_align.i, dftmtx.i, 1, radix, radix, 0, 0, 0,
             align_K - radix);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (butterfly_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, align_K - radix);
    __bang_pad(in_align.i, in_trans.i, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, align_K - radix);

  } else {
    __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0, 0);
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx_align.r, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx_align.r, (float *)in_wram.i,
                align_M, align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx_align.i, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx_align.i, (float *)in_wram.i,
                align_M, align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  __bang_transpose(out_trans.r, out.r, align_M, align_N);
  __bang_transpose(out_trans.i, out.i, align_M, align_N);

  __memcpy(nram_out_r, out_trans.r, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
  __memcpy(nram_out_i, out_trans.i, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
}

// matmul sample
template <typename DT>
__mlu_func__ void computeRadix27ButterflyFirststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, int section_num, int butterfly_num,
    int in_stride, int dir) {
  // outplace(nram)

  // origin: M = radix, K = radix, N =butterfly_num
  // pad_up:
  const int radix = 27;
  const int align_M = radix;  // no align
  const int align_K = 32;
  const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> dftmtx_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_K]};
  nram_scratch_offset += (align_M * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
  nram_scratch_offset += (butterfly_num * radix * 2);

  // FFT_CPX_T<DT> out = in_trans;

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};

  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  __bang_transpose(in_trans.r, nram_in_r, radix, butterfly_num);
  __bang_transpose(in_trans.i, nram_in_i, radix, butterfly_num);

  // right pad radix -> align_K
  __bang_pad(dftmtx_align.r, dftmtx.r, 1, radix, radix, 0, 0, 0,
             align_K - radix);
  __bang_pad(dftmtx_align.i, dftmtx.i, 1, radix, radix, 0, 0, 0,
             align_K - radix);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (butterfly_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, align_K - radix);
    __bang_pad(in_align.i, in_trans.i, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, align_K - radix);

  } else {
    __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0, 0);
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx_align.r, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx_align.r, (float *)in_wram.i,
                align_M, align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx_align.i, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx_align.i, (float *)in_wram.i,
                align_M, align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  __bang_transpose(out_trans.r, out.r, align_M, align_N);
  __bang_transpose(out_trans.i, out.i, align_M, align_N);

  __memcpy(nram_out_r, out_trans.r, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
  __memcpy(nram_out_i, out_trans.i, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
}

// matmul sample
template <typename DT>
__mlu_func__ void computeRadix16ButterflyFirststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, int section_num, int butterfly_num,
    int in_stride, int dir) {
  // outplace(nram)

  // origin: M = radix, K = radix, N =butterfly_num
  // pad_up:
  const int radix = 16;
  const int align_M = radix;  // no align
  const int align_K = 16;
  const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
  nram_scratch_offset += (butterfly_num * radix * 2);

  // FFT_CPX_T<DT> out = in_trans;

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};

  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  __bang_transpose(in_trans.r, nram_in_r, radix, butterfly_num);
  __bang_transpose(in_trans.i, nram_in_i, radix, butterfly_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (butterfly_num != 1 && align_N != butterfly_num) {
    __bang_pad(in_align.r, in_trans.r, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, 0);

  } else {
    in_align = in_trans;
    // __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
    // 0);
    // __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
    // 0);
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx.r, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx.r, (float *)in_wram.i, align_M,
                align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx.i, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx.i, (float *)in_wram.i, align_M,
                align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  __bang_transpose(out_trans.r, out.r, align_M, align_N);
  __bang_transpose(out_trans.i, out.i, align_M, align_N);

  __memcpy(nram_out_r, out_trans.r, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
  __memcpy(nram_out_i, out_trans.i, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
}

// matmul sample
template <typename DT>
__mlu_func__ void computeRadix8ButterflyFirststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, int section_num, int butterfly_num,
    int in_stride, int dir) {
  // outplace(nram)

  // origin: M = radix, K = radix, N =butterfly_num
  // pad_up:
  const int radix = 8;
  const int align_M = radix;  // no align
  const int align_K = 16;
  const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> dftmtx_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_K]};
  nram_scratch_offset += (align_M * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
  nram_scratch_offset += (butterfly_num * radix * 2);

  // FFT_CPX_T<DT> out = in_trans;

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};

  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  __bang_transpose(in_trans.r, nram_in_r, radix, butterfly_num);
  __bang_transpose(in_trans.i, nram_in_i, radix, butterfly_num);

  // right pad radix -> align_K
  __bang_pad(dftmtx_align.r, dftmtx.r, 1, radix, radix, 0, 0, 0,
             align_K - radix);
  __bang_pad(dftmtx_align.i, dftmtx.i, 1, radix, radix, 0, 0, 0,
             align_K - radix);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (butterfly_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, align_K - radix);
    __bang_pad(in_align.i, in_trans.i, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, align_K - radix);

  } else {
    __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0, 0);
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx_align.r, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx_align.r, (float *)in_wram.i,
                align_M, align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx_align.i, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx_align.i, (float *)in_wram.i,
                align_M, align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  __bang_transpose(out_trans.r, out.r, align_M, align_N);
  __bang_transpose(out_trans.i, out.i, align_M, align_N);

  __memcpy(nram_out_r, out_trans.r, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
  __memcpy(nram_out_i, out_trans.i, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
}

template <typename DT>
__mlu_func__ void computeRadix32ButterflyFirststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, int section_num, int butterfly_num,
    int in_stride, int dir) {
  // outplace(nram)

  // origin: M = radix, K = radix, N =butterfly_num
  // pad_up:
  const int radix = 32;
  const int align_M = radix;  // no align
  const int align_K = 32;
  const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
  nram_scratch_offset += (butterfly_num * radix * 2);

  // FFT_CPX_T<DT> out = in_trans;

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};

  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  __bang_transpose(in_trans.r, nram_in_r, radix, butterfly_num);
  __bang_transpose(in_trans.i, nram_in_i, radix, butterfly_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (butterfly_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, 0);

  } else {
    in_align = in_trans;
    // __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
    // 0);
    // __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
    // 0);
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx.r, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx.r, (float *)in_wram.i, align_M,
                align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx.i, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx.i, (float *)in_wram.i, align_M,
                align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  __bang_transpose(out_trans.r, out.r, align_M, align_N);
  __bang_transpose(out_trans.i, out.i, align_M, align_N);

  __memcpy(nram_out_r, out_trans.r, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
  __memcpy(nram_out_i, out_trans.i, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
}

template <typename DT>
__mlu_func__ void computeRadix64ButterflyFirststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, int section_num, int butterfly_num,
    int in_stride, int dir) {
  // outplace(nram)

  // origin: M = radix, K = radix, N =butterfly_num
  // pad_up:
  const int radix = 64;
  const int align_M = radix;  // no align
  const int align_K = 64;
  const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
  nram_scratch_offset += (butterfly_num * radix * 2);

  // FFT_CPX_T<DT> out = in_trans;

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};

  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  __bang_transpose(in_trans.r, nram_in_r, radix, butterfly_num);
  __bang_transpose(in_trans.i, nram_in_i, radix, butterfly_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (butterfly_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, butterfly_num, radix, 0,
               align_N - butterfly_num, 0, 0);

  } else {
    in_align = in_trans;
    // __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
    // 0);
    // __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
    // 0);
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx.r, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx.r, (float *)in_wram.i, align_M,
                align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx.i, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx.i, (float *)in_wram.i, align_M,
                align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  __bang_transpose(out_trans.r, out.r, align_M, align_N);
  __bang_transpose(out_trans.i, out.i, align_M, align_N);

  __memcpy(nram_out_r, out_trans.r, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
  __memcpy(nram_out_i, out_trans.i, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
}

template <typename DT>
__mlu_func__ void computeRadix27ButterflyOtherstagesMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  const int para_num = butterfly_num * section_num;
  const int radix = 27;
  const int align_M = radix;  // no align
  const int align_K = 32;
  const int align_N = 64 * ((para_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> dftmtx_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_K]};
  nram_scratch_offset += (align_M * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + para_num * radix]};
  nram_scratch_offset += (para_num * radix * 2);

  // const int out_stride = butterfly_num;
  // int Fin_stride = 0,
  int Fout_stride = 0;

  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};
  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  // DT *RR = &nram_scratch[nram_scratch_offset];  // (9-1)*butterfly_num
  // DT *RI = &nram_scratch[nram_scratch_offset +
  //                        butterfly_num * (radix - 1)];  //
  //                        (9-1)*butterfly_num
  // DT *IR = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  2];  //
  //                                                  (9-1)*butterfly_num
  // DT *II = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  3];  //
  //                                                  (9-1)*butterfly_num
  // nram_scratch_offset += (butterfly_num * 4 * (radix - 1));

  // FFT_CPX_T<DT> scratch_in;
  FFT_CPX_T<DT> Fin[radix];
  // FFT_CPX_T<DT> Fout[radix];

  // __memcpy(scratch_tw_r, sram_tw, butterfly_num * (3 - 1) * 2, SRAM2NRAM);

  // right pad radix -> align_K
  __bang_pad(dftmtx_align.r, dftmtx.r, 1, radix, radix, 0, 0, 0,
             align_K - radix);
  __bang_pad(dftmtx_align.i, dftmtx.i, 1, radix, radix, 0, 0, 0,
             align_K - radix);

  // scratch_in = FFT_CPX_T<DT>{nram_in_r, nram_in_i};

  int nram_in_offset = 0;
  for (int i = 0; i < radix; i++) {
    Fin[i].r = &nram_in_r[nram_in_offset];
    Fin[i].i = &nram_in_i[nram_in_offset];
    nram_in_offset += para_num;
  }

  // // rotate
  // MLU_CPX_MUL(Fin[1], Fin[1], scratch_tw, RR, II, RI, IR,
  //             butterfly_num * (radix - 1));

  for (int i = 1; i < radix; i++) {
    __bang_cycle_mul(&RR[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&RI[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&IR[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&II[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
  }

  __bang_sub(Fin[1].r, RR, II, para_num * (radix - 1));
  __bang_add(Fin[1].i, RI, IR, para_num * (radix - 1));

  __bang_transpose(in_trans.r, Fin[0].r, radix, para_num);
  __bang_transpose(in_trans.i, Fin[0].i, radix, para_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (para_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
               align_N - para_num, 0, align_K - radix);
    __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
               align_N - para_num, 0, align_K - radix);

  } else {
    __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0, 0);
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx_align.r, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx_align.r, (float *)in_wram.i,
                align_M, align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx_align.i, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx_align.i, (float *)in_wram.i,
                align_M, align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  // FFT_CPX_T<DT> out_tmp = {nram_in_r, nram_in_i};

  // __memcpy(out_tmp.r, out.r, para_num * sizeof(DT),
  //           NRAM2NRAM, para_num * sizeof(DT), align_N * sizeof(DT),
  //           radix - 1);
  // __memcpy(out_tmp.i, out.i, para_num * sizeof(DT),
  //           NRAM2NRAM, para_num * sizeof(DT), align_N * sizeof(DT),
  //           radix - 1);

  for (int sec_count = 0; sec_count < section_num; ++sec_count) {
    // [radix, section_num, butterfly_num] -> [section_num, radix,
    // butterfly_num]

    __memcpy(&nram_out_r[Fout_stride], &out.r[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);
    __memcpy(&nram_out_i[Fout_stride], &out.i[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);

    // Fin_stride += butterfly_num;
    Fout_stride += radix * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix8ButterflyOtherstagesMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  const int para_num = butterfly_num * section_num;
  const int radix = 8;
  const int align_M = radix;  // no align
  const int align_K = 16;
  const int align_N = 64 * ((para_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> dftmtx_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_K]};
  nram_scratch_offset += (align_M * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + para_num * radix]};
  nram_scratch_offset += (para_num * radix * 2);

  // const int out_stride = butterfly_num;
  // int Fin_stride = 0,
  int Fout_stride = 0;

  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};
  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  // DT *RR = &nram_scratch[nram_scratch_offset];  // (9-1)*butterfly_num
  // DT *RI = &nram_scratch[nram_scratch_offset +
  //                        butterfly_num * (radix - 1)];  //
  //                        (9-1)*butterfly_num
  // DT *IR = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  2];  //
  //                                                  (9-1)*butterfly_num
  // DT *II = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  3];  //
  //                                                  (9-1)*butterfly_num
  // nram_scratch_offset += (butterfly_num * 4 * (radix - 1));

  // FFT_CPX_T<DT> scratch_in;
  FFT_CPX_T<DT> Fin[radix];
  // FFT_CPX_T<DT> Fout[radix];

  // __memcpy(scratch_tw_r, sram_tw, butterfly_num * (3 - 1) * 2, SRAM2NRAM);

  // right pad radix -> align_K
  __bang_pad(dftmtx_align.r, dftmtx.r, 1, radix, radix, 0, 0, 0,
             align_K - radix);
  __bang_pad(dftmtx_align.i, dftmtx.i, 1, radix, radix, 0, 0, 0,
             align_K - radix);

  // scratch_in = FFT_CPX_T<DT>{nram_in_r, nram_in_i};

  int nram_in_offset = 0;
  for (int i = 0; i < radix; i++) {
    Fin[i].r = &nram_in_r[nram_in_offset];
    Fin[i].i = &nram_in_i[nram_in_offset];
    nram_in_offset += para_num;
  }

  // // rotate
  // MLU_CPX_MUL(Fin[1], Fin[1], scratch_tw, RR, II, RI, IR,
  //             butterfly_num * (radix - 1));

  for (int i = 1; i < radix; i++) {
    __bang_cycle_mul(&RR[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&RI[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&IR[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&II[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
  }

  __bang_sub(Fin[1].r, RR, II, para_num * (radix - 1));
  __bang_add(Fin[1].i, RI, IR, para_num * (radix - 1));

  __bang_transpose(in_trans.r, Fin[0].r, radix, para_num);
  __bang_transpose(in_trans.i, Fin[0].i, radix, para_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (para_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
               align_N - para_num, 0, align_K - radix);
    __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
               align_N - para_num, 0, align_K - radix);

  } else {
    __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0, 0);
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx_align.r, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx_align.r, (float *)in_wram.i,
                align_M, align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx_align.i, (float *)in_wram.r,
                align_M, align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx_align.i, (float *)in_wram.i,
                align_M, align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  // FFT_CPX_T<DT> out_tmp = {nram_in_r, nram_in_i};

  // __memcpy(out_tmp.r, out.r, para_num * sizeof(DT),
  //           NRAM2NRAM, para_num * sizeof(DT), align_N * sizeof(DT),
  //           radix - 1);
  // __memcpy(out_tmp.i, out.i, para_num * sizeof(DT),
  //           NRAM2NRAM, para_num * sizeof(DT), align_N * sizeof(DT),
  //           radix - 1);

  for (int sec_count = 0; sec_count < section_num; ++sec_count) {
    // [radix, section_num, butterfly_num] -> [section_num, radix,
    // butterfly_num]

    __memcpy(&nram_out_r[Fout_stride], &out.r[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);
    __memcpy(&nram_out_i[Fout_stride], &out.i[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);

    // Fin_stride += butterfly_num;
    Fout_stride += radix * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix16ButterflyOtherstagesMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  const int para_num = butterfly_num * section_num;
  const int radix = 16;
  const int align_M = radix;  // no align
  const int align_K = 16;
  const int align_N = 64 * ((para_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + para_num * radix]};
  nram_scratch_offset += (para_num * radix * 2);

  // const int out_stride = butterfly_num;
  // int Fin_stride = 0,
  int Fout_stride = 0;

  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};
  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  // DT *RR = &nram_scratch[nram_scratch_offset];  // (9-1)*butterfly_num
  // DT *RI = &nram_scratch[nram_scratch_offset +
  //                        butterfly_num * (radix - 1)];  //
  //                        (9-1)*butterfly_num
  // DT *IR = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  2];  //
  //                                                  (9-1)*butterfly_num
  // DT *II = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  3];  //
  //                                                  (9-1)*butterfly_num
  // nram_scratch_offset += (butterfly_num * 4 * (radix - 1));

  // FFT_CPX_T<DT> scratch_in;
  FFT_CPX_T<DT> Fin[radix];
  // FFT_CPX_T<DT> Fout[radix];

  // __memcpy(scratch_tw_r, sram_tw, butterfly_num * (3 - 1) * 2, SRAM2NRAM);

  // right pad radix -> align_K

  // scratch_in = FFT_CPX_T<DT>{nram_in_r, nram_in_i};

  int nram_in_offset = 0;
  for (int i = 0; i < radix; i++) {
    Fin[i].r = &nram_in_r[nram_in_offset];
    Fin[i].i = &nram_in_i[nram_in_offset];
    nram_in_offset += para_num;
  }

  // // rotate
  // MLU_CPX_MUL(Fin[1], Fin[1], scratch_tw, RR, II, RI, IR,
  //             butterfly_num * (radix - 1));

  for (int i = 1; i < radix; i++) {
    __bang_cycle_mul(&RR[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&RI[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&IR[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&II[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
  }

  __bang_sub(Fin[1].r, RR, II, para_num * (radix - 1));
  __bang_add(Fin[1].i, RI, IR, para_num * (radix - 1));

  __bang_transpose(in_trans.r, Fin[0].r, radix, para_num);
  __bang_transpose(in_trans.i, Fin[0].i, radix, para_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (para_num != 1 && align_N != para_num) {
    __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
               align_N - para_num, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
               align_N - para_num, 0, 0);

  } else {
    // __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
    // 0);
    // __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
    // 0);
    in_align = in_trans;
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx.r, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx.r, (float *)in_wram.i, align_M,
                align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx.i, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx.i, (float *)in_wram.i, align_M,
                align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  // FFT_CPX_T<DT> out_tmp = {nram_in_r, nram_in_i};

  // __memcpy(out_tmp.r, out.r, para_num * sizeof(DT),
  //           NRAM2NRAM, para_num * sizeof(DT), align_N * sizeof(DT),
  //           radix - 1);
  // __memcpy(out_tmp.i, out.i, para_num * sizeof(DT),
  //           NRAM2NRAM, para_num * sizeof(DT), align_N * sizeof(DT),
  //           radix - 1);

  for (int sec_count = 0; sec_count < section_num; ++sec_count) {
    // [radix, section_num, butterfly_num] -> [section_num, radix,
    // butterfly_num]

    __memcpy(&nram_out_r[Fout_stride], &out.r[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);
    __memcpy(&nram_out_i[Fout_stride], &out.i[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);

    // Fin_stride += butterfly_num;
    Fout_stride += radix * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix32ButterflyOtherstagesMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  const int para_num = butterfly_num * section_num;
  const int radix = 32;
  const int align_M = radix;  // no align
  const int align_K = 32;
  const int align_N = 64 * ((para_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + para_num * radix]};
  nram_scratch_offset += (para_num * radix * 2);

  // const int out_stride = butterfly_num;
  // int Fin_stride = 0,
  int Fout_stride = 0;

  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};
  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  FFT_CPX_T<DT> Fin[radix];

  int nram_in_offset = 0;
  for (int i = 0; i < radix; i++) {
    Fin[i].r = &nram_in_r[nram_in_offset];
    Fin[i].i = &nram_in_i[nram_in_offset];
    nram_in_offset += para_num;
  }

  for (int i = 1; i < radix; i++) {
    __bang_cycle_mul(&RR[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&RI[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&IR[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&II[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
  }

  __bang_sub(Fin[1].r, RR, II, para_num * (radix - 1));
  __bang_add(Fin[1].i, RI, IR, para_num * (radix - 1));

  __bang_transpose(in_trans.r, Fin[0].r, radix, para_num);
  __bang_transpose(in_trans.i, Fin[0].i, radix, para_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (para_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
               align_N - para_num, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
               align_N - para_num, 0, 0);

  } else {
    in_align = in_trans;
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx.r, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx.r, (float *)in_wram.i, align_M,
                align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx.i, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx.i, (float *)in_wram.i, align_M,
                align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  for (int sec_count = 0; sec_count < section_num; ++sec_count) {
    // [radix, section_num, butterfly_num] -> [section_num, radix,
    // butterfly_num]

    __memcpy(&nram_out_r[Fout_stride], &out.r[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);
    __memcpy(&nram_out_i[Fout_stride], &out.i[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);

    // Fin_stride += butterfly_num;
    Fout_stride += radix * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix64ButterflyOtherstagesMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  const int para_num = butterfly_num * section_num;
  const int radix = 64;
  const int align_M = radix;  // no align
  const int align_K = 64;
  const int align_N = 64 * ((para_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + para_num * radix]};
  nram_scratch_offset += (para_num * radix * 2);

  // const int out_stride = butterfly_num;
  // int Fin_stride = 0,
  int Fout_stride = 0;

  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};
  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  FFT_CPX_T<DT> Fin[radix];

  int nram_in_offset = 0;
  for (int i = 0; i < radix; i++) {
    Fin[i].r = &nram_in_r[nram_in_offset];
    Fin[i].i = &nram_in_i[nram_in_offset];
    nram_in_offset += para_num;
  }

  for (int i = 1; i < radix; i++) {
    __bang_cycle_mul(&RR[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&RI[(i - 1) * para_num], Fin[i].r,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&IR[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&II[(i - 1) * para_num], Fin[i].i,
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
  }

  __bang_sub(Fin[1].r, RR, II, para_num * (radix - 1));
  __bang_add(Fin[1].i, RI, IR, para_num * (radix - 1));

  __bang_transpose(in_trans.r, Fin[0].r, radix, para_num);
  __bang_transpose(in_trans.i, Fin[0].i, radix, para_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  if (para_num != 1) {
    __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
               align_N - para_num, 0, 0);
    __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
               align_N - para_num, 0, 0);

  } else {
    in_align = in_trans;
  }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx.r, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)RI, (float *)dftmtx.r, (float *)in_wram.i, align_M,
                align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx.i, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx.i, (float *)in_wram.i, align_M,
                align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  for (int sec_count = 0; sec_count < section_num; ++sec_count) {
    // [radix, section_num, butterfly_num] -> [section_num, radix,
    // butterfly_num]

    __memcpy(&nram_out_r[Fout_stride], &out.r[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);
    __memcpy(&nram_out_i[Fout_stride], &out.i[sec_count * butterfly_num],
             butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num * sizeof(DT),
             align_N * sizeof(DT), radix - 1);

    // Fin_stride += butterfly_num;
    Fout_stride += radix * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix27ButterflyOtherstagesMat_v1(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  const int radix = 27;
  const int align_M = radix;  // no align
  const int align_K = 32;
  const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> dftmtx_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_K]};
  nram_scratch_offset += (align_M * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
  nram_scratch_offset += (butterfly_num * radix * 2);

  // const int out_stride = butterfly_num;
  int Fin_stride = 0, Fout_stride = 0;
  int sec_count;

  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};
  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  // DT *RR = &nram_scratch[nram_scratch_offset];  // (9-1)*butterfly_num
  // DT *RI = &nram_scratch[nram_scratch_offset +
  //                        butterfly_num * (radix - 1)];  //
  //                        (9-1)*butterfly_num
  // DT *IR = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  2];  //
  //                                                  (9-1)*butterfly_num
  // DT *II = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  3];  //
  //                                                  (9-1)*butterfly_num
  // nram_scratch_offset += (butterfly_num * 4 * (radix - 1));

  FFT_CPX_T<DT> scratch_in;
  FFT_CPX_T<DT> Fin[radix];
  // FFT_CPX_T<DT> Fout[radix];

  // __memcpy(scratch_tw_r, sram_tw, butterfly_num * (3 - 1) * 2, SRAM2NRAM);

  // right pad radix -> align_K
  __bang_pad(dftmtx_align.r, dftmtx.r, 1, radix, radix, 0, 0, 0,
             align_K - radix);
  __bang_pad(dftmtx_align.i, dftmtx.i, 1, radix, radix, 0, 0, 0,
             align_K - radix);

  for (sec_count = 0; sec_count < section_num; ++sec_count) {
    // for (int i = 0; i < radix; i++) {
    //   Fout[i].r = &nram_out_r[Fout_stride + butterfly_num * i];
    //   Fout[i].i = &nram_out_i[Fout_stride + butterfly_num * i];
    // }

    if (section_num == 1) {
      scratch_in = FFT_CPX_T<DT>{nram_in_r, nram_in_i};
    } else {
      // nram_scratch
      scratch_in.r = &nram_scratch[nram_scratch_offset];
      scratch_in.i = &nram_scratch[nram_scratch_offset + radix * butterfly_num];
      __memcpy(scratch_in.r, nram_in_r + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               radix - 1);
      __memcpy(scratch_in.i, nram_in_i + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               radix - 1);
    }

    int nram_in_offset = 0;
    for (int i = 0; i < radix; i++) {
      Fin[i].r = &scratch_in.r[nram_in_offset];
      Fin[i].i = &scratch_in.i[nram_in_offset];
      nram_in_offset += butterfly_num;
    }

    // rotate
    MLU_CPX_MUL(Fin[1], Fin[1], scratch_tw, RR, II, RI, IR,
                butterfly_num * (radix - 1));

    // __bang_cycle_mul(float *dst, const float *src, const float *seg, unsigned
    // int src_elem_count, unsigned int seg_elem_count);

    __bang_transpose(in_trans.r, Fin[0].r, radix, butterfly_num);
    __bang_transpose(in_trans.i, Fin[0].i, radix, butterfly_num);

    // right pad radix -> align_K
    // bottom pad butterfly_num -> align_N

    if (butterfly_num != 1) {
      __bang_pad(in_align.r, in_trans.r, 1, butterfly_num, radix, 0,
                 align_N - butterfly_num, 0, align_K - radix);
      __bang_pad(in_align.i, in_trans.i, 1, butterfly_num, radix, 0,
                 align_N - butterfly_num, 0, align_K - radix);

    } else {
      __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0, 0);
      __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0, 0);
    }

    __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
    __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

    __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
    __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

    __bang_matmul((float *)RR, (float *)dftmtx_align.r, (float *)in_wram.r,
                  align_M, align_K, align_N);
    __bang_matmul((float *)RI, (float *)dftmtx_align.r, (float *)in_wram.i,
                  align_M, align_K, align_N);
    __bang_matmul((float *)IR, (float *)dftmtx_align.i, (float *)in_wram.r,
                  align_M, align_K, align_N);
    __bang_matmul((float *)II, (float *)dftmtx_align.i, (float *)in_wram.i,
                  align_M, align_K, align_N);

    __bang_sub(out.r, RR, II, align_M * align_N);
    __bang_add(out.i, RI, IR, align_M * align_N);

    __memcpy(&nram_out_r[Fout_stride], out.r, butterfly_num * sizeof(DT),
             NRAM2NRAM, butterfly_num * sizeof(DT), align_N * sizeof(DT),
             radix - 1);
    __memcpy(&nram_out_i[Fout_stride], out.i, butterfly_num * sizeof(DT),
             NRAM2NRAM, butterfly_num * sizeof(DT), align_N * sizeof(DT),
             radix - 1);

    Fin_stride += butterfly_num;
    Fout_stride += radix * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix9ButterflyOtherstages_v1(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_tw, int section_num, int butterfly_num,
    int in_stride, int dir) {
  const int radix = 9;
  const int sign = (dir == FFT_FORWARD) ? 1 : -1;
  const DT tw9_1i = sign * TW9_1I_F;
  const DT tw9_2i = sign * TW9_2I_F;
  const DT tw9_3i = sign * TW9_3I_F;
  const DT tw9_4i = sign * TW9_4I_F;

  // const int out_stride = butterfly_num;
  int Fin_stride = 0, Fout_stride = 0;
  int sec_count;

  FFT_CPX_T<DT> scratch[10];
  int nram_scratch_offset = 0;
  for (int i = 0; i < 10; i++) {
    scratch[i].r = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
    scratch[i].i = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
  }

  FFT_CPX_T<DT> _A = {&nram_scratch[nram_scratch_offset],
                      &nram_scratch[nram_scratch_offset + butterfly_num]};
  FFT_CPX_T<DT> _B = {&nram_scratch[nram_scratch_offset + butterfly_num * 2],
                      &nram_scratch[nram_scratch_offset + butterfly_num * 3]};

  FFT_CPX_T<DT> in_0 = {&nram_scratch[nram_scratch_offset + butterfly_num * 4],
                        &nram_scratch[nram_scratch_offset + butterfly_num * 5]};
  nram_scratch_offset += (butterfly_num * 6);

  FFT_CPX_T<DT> _A_TEMP = {&nram_scratch[nram_scratch_offset],
                           &nram_scratch[nram_scratch_offset + butterfly_num]};
  FFT_CPX_T<DT> _B_TEMP = {
      &nram_scratch[nram_scratch_offset + butterfly_num * 2],
      &nram_scratch[nram_scratch_offset + butterfly_num * 3]};
  nram_scratch_offset += (butterfly_num * 4);
  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  DT *RR = &nram_scratch[nram_scratch_offset];  // (9-1)*butterfly_num
  DT *RI = &nram_scratch[nram_scratch_offset +
                         butterfly_num * (radix - 1)];  // (9-1)*butterfly_num
  DT *IR = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
                                                   2];  // (9-1)*butterfly_num
  DT *II = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
                                                   3];  // (9-1)*butterfly_num
  nram_scratch_offset += (butterfly_num * 4 * (radix - 1));

  FFT_CPX_T<DT> scratch_in;
  FFT_CPX_T<DT> Fin[radix];
  FFT_CPX_T<DT> Fout[radix];

  // __memcpy(scratch_tw_r, sram_tw, butterfly_num * (3 - 1) * 2, SRAM2NRAM);

  for (sec_count = 0; sec_count < section_num; ++sec_count) {
    for (int i = 0; i < radix; i++) {
      Fout[i].r = &nram_out_r[Fout_stride + butterfly_num * i];
      Fout[i].i = &nram_out_i[Fout_stride + butterfly_num * i];
    }

    if (section_num == 1) {
      scratch_in = FFT_CPX_T<DT>{nram_in_r, nram_in_i};
    } else {
      // nram_scratch
      scratch_in.r = &nram_scratch[nram_scratch_offset];
      scratch_in.i = &nram_scratch[nram_scratch_offset + 9 * butterfly_num];
      __memcpy(scratch_in.r, nram_in_r + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               radix - 1);
      __memcpy(scratch_in.i, nram_in_i + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               radix - 1);
    }

    int nram_in_offset = 0;
    for (int i = 0; i < radix; i++) {
      Fin[i].r = &scratch_in.r[nram_in_offset];
      Fin[i].i = &scratch_in.i[nram_in_offset];
      nram_in_offset += butterfly_num;
    }

    // rotate
    MLU_CPX_MUL(Fin[1], Fin[1], scratch_tw, RR, II, RI, IR,
                butterfly_num * (radix - 1));

    // butterfly compute

    __bang_move(in_0.r, Fin[0].r, butterfly_num * sizeof(DT));
    __bang_move(in_0.i, Fin[0].i, butterfly_num * sizeof(DT));

    MLU_CPX_ADD(scratch[0], Fin[1], Fin[8], butterfly_num);
    MLU_CPX_SUB(scratch[1], Fin[1], Fin[8], butterfly_num);
    MLU_CPX_ADD(scratch[2], Fin[2], Fin[7], butterfly_num);
    MLU_CPX_SUB(scratch[3], Fin[2], Fin[7], butterfly_num);
    MLU_CPX_ADD(scratch[4], Fin[3], Fin[6], butterfly_num);
    MLU_CPX_SUB(scratch[5], Fin[3], Fin[6], butterfly_num);
    MLU_CPX_ADD(scratch[6], Fin[4], Fin[5], butterfly_num);
    MLU_CPX_SUB(scratch[7], Fin[4], Fin[5], butterfly_num);

    MLU_CPX_ADD(scratch[9], scratch[0], scratch[2], butterfly_num);
    MLU_CPX_ADD(scratch[9], scratch[4], scratch[9], butterfly_num);
    MLU_CPX_ADD(scratch[9], scratch[6], scratch[9], butterfly_num);
    MLU_CPX_ADD(Fout[0], scratch[9], in_0, butterfly_num);

    MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_1R_F, butterfly_num);
    MLU_CPX_MUL_S(_B, scratch[1], tw9_1i, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_2R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[3], tw9_2i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[5], tw9_3i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_4R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[7], tw9_4i, _B_TEMP, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[1], Fout[8], _A, _B, butterfly_num);

    MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_2R_F, butterfly_num);
    MLU_CPX_MUL_S(_B, scratch[1], tw9_2i, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_4R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[3], tw9_4i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[5], -tw9_3i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_1R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[7], -tw9_1i, _B_TEMP, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[2], Fout[7], _A, _B, butterfly_num);

    MLU_CPX_ADD(_A, in_0, scratch[4], butterfly_num);
    MLU_CPX_ADD(_B, scratch[0], scratch[2], butterfly_num);
    MLU_CPX_ADD(_B, scratch[6], _B, butterfly_num);
    MLU_CPX_MLA_OUTPLACE(_A, _A, _B, TW9_3R_F, butterfly_num);
    MLU_CPX_SUB(_B, scratch[1], scratch[3], butterfly_num);
    MLU_CPX_ADD(_B, scratch[7], _B, butterfly_num);
    MLU_CPX_MUL_S(_B, _B, tw9_3i, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[3], Fout[6], _A, _B, butterfly_num);

    MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_4R_F, butterfly_num);
    MLU_CPX_MUL_S(_B, scratch[1], tw9_4i, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_1R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[3], -tw9_1i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[5], tw9_3i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_2R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[7], -tw9_2i, _B_TEMP, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[4], Fout[5], _A, _B, butterfly_num);

    Fin_stride += butterfly_num;
    Fout_stride += radix * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix9ButterflyOtherstages(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_tw, int section_num, int butterfly_num,
    int in_stride, int dir) {
  const int radix = 9;
  const int sign = (dir == FFT_FORWARD) ? 1 : -1;
  const DT tw9_1i = sign * TW9_1I_F;
  const DT tw9_2i = sign * TW9_2I_F;
  const DT tw9_3i = sign * TW9_3I_F;
  const DT tw9_4i = sign * TW9_4I_F;

  // const int out_stride = butterfly_num;
  int Fin_stride = 0, Fout_stride = 0;
  int sec_count;

  FFT_CPX_T<DT> scratch[10];
  int nram_scratch_offset = 0;
  for (int i = 0; i < 10; i++) {
    scratch[i].r = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
    scratch[i].i = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
  }

  FFT_CPX_T<DT> _A = {&nram_scratch[nram_scratch_offset],
                      &nram_scratch[nram_scratch_offset + butterfly_num]};
  FFT_CPX_T<DT> _B = {&nram_scratch[nram_scratch_offset + butterfly_num * 2],
                      &nram_scratch[nram_scratch_offset + butterfly_num * 3]};

  FFT_CPX_T<DT> in_0 = {&nram_scratch[nram_scratch_offset + butterfly_num * 4],
                        &nram_scratch[nram_scratch_offset + butterfly_num * 5]};
  nram_scratch_offset += (butterfly_num * 6);

  FFT_CPX_T<DT> _A_TEMP = {&nram_scratch[nram_scratch_offset],
                           &nram_scratch[nram_scratch_offset + butterfly_num]};
  FFT_CPX_T<DT> _B_TEMP = {
      &nram_scratch[nram_scratch_offset + butterfly_num * 2],
      &nram_scratch[nram_scratch_offset + butterfly_num * 3]};
  nram_scratch_offset += (butterfly_num * 4);
  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  DT *RR = &nram_scratch[nram_scratch_offset];  // (9-1)*butterfly_num
  DT *RI = &nram_scratch[nram_scratch_offset +
                         butterfly_num * (radix - 1)];  // (9-1)*butterfly_num
  DT *IR = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
                                                   2];  // (9-1)*butterfly_num
  DT *II = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
                                                   3];  // (9-1)*butterfly_num
  nram_scratch_offset += (butterfly_num * 4 * (radix - 1));

  FFT_CPX_T<DT> scratch_in;
  FFT_CPX_T<DT> Fin[radix];
  FFT_CPX_T<DT> Fout[radix];

  // __memcpy(scratch_tw_r, sram_tw, butterfly_num * (3 - 1) * 2, SRAM2NRAM);

  for (sec_count = 0; sec_count < section_num; ++sec_count) {
    for (int i = 0; i < radix; i++) {
      Fout[i].r = &nram_out_r[Fout_stride + butterfly_num * i];
      Fout[i].i = &nram_out_i[Fout_stride + butterfly_num * i];
    }

    if (section_num == 1) {
      scratch_in = FFT_CPX_T<DT>{nram_in_r, nram_in_i};
    } else {
      // nram_scratch
      scratch_in.r = &nram_scratch[nram_scratch_offset];
      scratch_in.i = &nram_scratch[nram_scratch_offset + radix * butterfly_num];
      __memcpy(scratch_in.r, nram_in_r + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               radix - 1);
      __memcpy(scratch_in.i, nram_in_i + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               radix - 1);
    }

    int nram_in_offset = 0;
    for (int i = 0; i < radix; i++) {
      Fin[i].r = &scratch_in.r[nram_in_offset];
      Fin[i].i = &scratch_in.i[nram_in_offset];
      nram_in_offset += butterfly_num;
    }

    // rotate
    MLU_CPX_MUL(Fin[1], Fin[1], scratch_tw, RR, II, RI, IR,
                butterfly_num * (radix - 1));

    // butterfly compute

    __bang_move(in_0.r, Fin[0].r, butterfly_num * sizeof(DT));
    __bang_move(in_0.i, Fin[0].i, butterfly_num * sizeof(DT));

    MLU_CPX_ADD(scratch[0], Fin[1], Fin[8], butterfly_num);
    MLU_CPX_SUB(scratch[1], Fin[1], Fin[8], butterfly_num);
    MLU_CPX_ADD(scratch[2], Fin[2], Fin[7], butterfly_num);
    MLU_CPX_SUB(scratch[3], Fin[2], Fin[7], butterfly_num);
    MLU_CPX_ADD(scratch[4], Fin[3], Fin[6], butterfly_num);
    MLU_CPX_SUB(scratch[5], Fin[3], Fin[6], butterfly_num);
    MLU_CPX_ADD(scratch[6], Fin[4], Fin[5], butterfly_num);
    MLU_CPX_SUB(scratch[7], Fin[4], Fin[5], butterfly_num);

    MLU_CPX_ADD(scratch[9], scratch[0], scratch[2], butterfly_num);
    MLU_CPX_ADD(scratch[9], scratch[4], scratch[9], butterfly_num);
    MLU_CPX_ADD(scratch[9], scratch[6], scratch[9], butterfly_num);
    MLU_CPX_ADD(Fout[0], scratch[9], in_0, butterfly_num);

    MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_1R_F, butterfly_num);
    MLU_CPX_MUL_S(_B, scratch[1], tw9_1i, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_2R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[3], tw9_2i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[5], tw9_3i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_4R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[7], tw9_4i, _B_TEMP, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[1], Fout[8], _A, _B, butterfly_num);

    MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_2R_F, butterfly_num);
    MLU_CPX_MUL_S(_B, scratch[1], tw9_2i, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_4R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[3], tw9_4i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[5], -tw9_3i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_1R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[7], -tw9_1i, _B_TEMP, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[2], Fout[7], _A, _B, butterfly_num);

    MLU_CPX_ADD(_A, in_0, scratch[4], butterfly_num);
    MLU_CPX_ADD(_B, scratch[0], scratch[2], butterfly_num);
    MLU_CPX_ADD(_B, scratch[6], _B, butterfly_num);
    MLU_CPX_MLA_OUTPLACE(_A, _A, _B, TW9_3R_F, butterfly_num);
    MLU_CPX_SUB(_B, scratch[1], scratch[3], butterfly_num);
    MLU_CPX_ADD(_B, scratch[7], _B, butterfly_num);
    MLU_CPX_MUL_S(_B, _B, tw9_3i, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[3], Fout[6], _A, _B, butterfly_num);

    MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_4R_F, butterfly_num);
    MLU_CPX_MUL_S(_B, scratch[1], tw9_4i, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_1R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[3], -tw9_1i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[5], tw9_3i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_2R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[7], -tw9_2i, _B_TEMP, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[4], Fout[5], _A, _B, butterfly_num);

    Fin_stride += butterfly_num;
    Fout_stride += radix * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix9ButterflyOtherstages_v3(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_tw, int section_num, int butterfly_num,
    int in_stride, int dir) {
  const int radix = 9;
  const int sign = (dir == FFT_FORWARD) ? 1 : -1;
  const DT tw9_1i = sign * TW9_1I_F;
  const DT tw9_2i = sign * TW9_2I_F;
  const DT tw9_3i = sign * TW9_3I_F;
  const DT tw9_4i = sign * TW9_4I_F;

  // const int out_stride = butterfly_num;
  int Fin_stride = 0, Fout_stride = 0;
  int sec_count;

  FFT_CPX_T<DT> scratch[10];
  int nram_scratch_offset = 0;
  for (int i = 0; i < 10; i++) {
    scratch[i].r = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
    scratch[i].i = &nram_scratch[nram_scratch_offset];
    nram_scratch_offset += butterfly_num;
  }

  FFT_CPX_T<DT> _A = {&nram_scratch[nram_scratch_offset],
                      &nram_scratch[nram_scratch_offset + butterfly_num]};
  FFT_CPX_T<DT> _B = {&nram_scratch[nram_scratch_offset + butterfly_num * 2],
                      &nram_scratch[nram_scratch_offset + butterfly_num * 3]};

  FFT_CPX_T<DT> in_0 = {&nram_scratch[nram_scratch_offset + butterfly_num * 4],
                        &nram_scratch[nram_scratch_offset + butterfly_num * 5]};
  nram_scratch_offset += (butterfly_num * 6);

  FFT_CPX_T<DT> _A_TEMP = {&nram_scratch[nram_scratch_offset],
                           &nram_scratch[nram_scratch_offset + butterfly_num]};
  FFT_CPX_T<DT> _B_TEMP = {
      &nram_scratch[nram_scratch_offset + butterfly_num * 2],
      &nram_scratch[nram_scratch_offset + butterfly_num * 3]};
  nram_scratch_offset += (butterfly_num * 4);
  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  DT *RR = &nram_scratch[nram_scratch_offset];  // (9-1)*butterfly_num
  DT *RI = &nram_scratch[nram_scratch_offset +
                         butterfly_num * (radix - 1)];  // (9-1)*butterfly_num
  DT *IR = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
                                                   2];  // (9-1)*butterfly_num
  DT *II = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
                                                   3];  // (9-1)*butterfly_num
  nram_scratch_offset += (butterfly_num * 4 * (radix - 1));

  FFT_CPX_T<DT> scratch_in;
  FFT_CPX_T<DT> Fin[radix];
  FFT_CPX_T<DT> Fout[radix];

  // __memcpy(scratch_tw_r, sram_tw, butterfly_num * (3 - 1) * 2, SRAM2NRAM);

  // parallel num: butterfly_num * section_num * large_radix_para_num
  for (sec_count = 0; sec_count < section_num; ++sec_count) {
    for (int i = 0; i < radix; i++) {
      Fout[i].r = &nram_out_r[Fout_stride + butterfly_num * i];
      Fout[i].i = &nram_out_i[Fout_stride + butterfly_num * i];
    }

    if (section_num == 1) {
      scratch_in = FFT_CPX_T<DT>{nram_in_r, nram_in_i};
    } else {
      // nram_scratch
      scratch_in.r = &nram_scratch[nram_scratch_offset];
      scratch_in.i = &nram_scratch[nram_scratch_offset + radix * butterfly_num];
      __memcpy(scratch_in.r, nram_in_r + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               radix - 1);
      __memcpy(scratch_in.i, nram_in_i + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               radix - 1);
    }

    int nram_in_offset = 0;
    for (int i = 0; i < radix; i++) {
      Fin[i].r = &scratch_in.r[nram_in_offset];
      Fin[i].i = &scratch_in.i[nram_in_offset];
      nram_in_offset += butterfly_num;
    }

    // rotate
    MLU_CPX_MUL(Fin[1], Fin[1], scratch_tw, RR, II, RI, IR,
                butterfly_num * (radix - 1));

    // butterfly compute

    __bang_move(in_0.r, Fin[0].r, butterfly_num * sizeof(DT));
    __bang_move(in_0.i, Fin[0].i, butterfly_num * sizeof(DT));

    MLU_CPX_ADD(scratch[0], Fin[1], Fin[8], butterfly_num);
    MLU_CPX_SUB(scratch[1], Fin[1], Fin[8], butterfly_num);
    MLU_CPX_ADD(scratch[2], Fin[2], Fin[7], butterfly_num);
    MLU_CPX_SUB(scratch[3], Fin[2], Fin[7], butterfly_num);
    MLU_CPX_ADD(scratch[4], Fin[3], Fin[6], butterfly_num);
    MLU_CPX_SUB(scratch[5], Fin[3], Fin[6], butterfly_num);
    MLU_CPX_ADD(scratch[6], Fin[4], Fin[5], butterfly_num);
    MLU_CPX_SUB(scratch[7], Fin[4], Fin[5], butterfly_num);

    MLU_CPX_ADD(scratch[9], scratch[0], scratch[2], butterfly_num);
    MLU_CPX_ADD(scratch[9], scratch[4], scratch[9], butterfly_num);
    MLU_CPX_ADD(scratch[9], scratch[6], scratch[9], butterfly_num);
    MLU_CPX_ADD(Fout[0], scratch[9], in_0, butterfly_num);

    MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_1R_F, butterfly_num);
    MLU_CPX_MUL_S(_B, scratch[1], tw9_1i, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_2R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[3], tw9_2i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[5], tw9_3i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_4R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[7], tw9_4i, _B_TEMP, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[1], Fout[8], _A, _B, butterfly_num);

    MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_2R_F, butterfly_num);
    MLU_CPX_MUL_S(_B, scratch[1], tw9_2i, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_4R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[3], tw9_4i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[5], -tw9_3i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_1R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[7], -tw9_1i, _B_TEMP, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[2], Fout[7], _A, _B, butterfly_num);

    MLU_CPX_ADD(_A, in_0, scratch[4], butterfly_num);
    MLU_CPX_ADD(_B, scratch[0], scratch[2], butterfly_num);
    MLU_CPX_ADD(_B, scratch[6], _B, butterfly_num);
    MLU_CPX_MLA_OUTPLACE(_A, _A, _B, TW9_3R_F, butterfly_num);
    MLU_CPX_SUB(_B, scratch[1], scratch[3], butterfly_num);
    MLU_CPX_ADD(_B, scratch[7], _B, butterfly_num);
    MLU_CPX_MUL_S(_B, _B, tw9_3i, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[3], Fout[6], _A, _B, butterfly_num);

    MLU_CPX_MLA_OUTPLACE(_A, in_0, scratch[0], TW9_4R_F, butterfly_num);
    MLU_CPX_MUL_S(_B, scratch[1], tw9_4i, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[2], TW9_1R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[3], -tw9_1i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[4], TW9_3R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[5], tw9_3i, _B_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_A, scratch[6], TW9_2R_F, _A_TEMP, butterfly_num);
    MLU_CPX_MLA_INPLACE(_B, scratch[7], -tw9_2i, _B_TEMP, butterfly_num);
    MLU_CPX_ODD_OUT(Fout[4], Fout[5], _A, _B, butterfly_num);

    Fin_stride += butterfly_num;
    Fout_stride += radix * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix9ButterflyLaststage(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_tw, int section_num, int butterfly_num,
    int in_stride, int dir) {
  computeRadix9ButterflyOtherstages(nram_out_r, nram_out_i, nram_in_r,
                                    nram_in_i, nram_scratch, nram_tw,
                                    section_num, butterfly_num, in_stride, dir);
}

template <typename DT>
__mlu_func__ void computeRadix27ButterflyLaststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  computeRadix27ButterflyOtherstagesMat(
      nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch, nram_dftmtx,
      nram_tw, section_num, butterfly_num, in_stride, dir);
}
template <typename DT>
__mlu_func__ void computeRadix8ButterflyLaststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  computeRadix8ButterflyOtherstagesMat(
      nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch, nram_dftmtx,
      nram_tw, section_num, butterfly_num, in_stride, dir);
}

template <typename DT>
__mlu_func__ void computeRadix16ButterflyLaststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  computeRadix16ButterflyOtherstagesMat(
      nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch, nram_dftmtx,
      nram_tw, section_num, butterfly_num, in_stride, dir);
}

template <typename DT>
__mlu_func__ void computeRadix32ButterflyLaststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  computeRadix32ButterflyOtherstagesMat(
      nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch, nram_dftmtx,
      nram_tw, section_num, butterfly_num, in_stride, dir);
}

template <typename DT>
__mlu_func__ void computeRadix64ButterflyLaststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int in_stride, int dir) {
  computeRadix64ButterflyOtherstagesMat(
      nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch, nram_dftmtx,
      nram_tw, section_num, butterfly_num, in_stride, dir);
}

template <typename DT>
__mlu_func__ void computeGenericButterflyFirststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, int section_num, int butterfly_num,
    int in_stride, int dir, int radix) {
  // outplace(nram)

  // origin: M = radix, K = radix, N =butterfly_num
  // pad_up:
  const int para_num = butterfly_num;
  // const int radix = 16;
  const int align_M = radix;  // no align
  const int K_num = 64 / sizeof(DT);
  const int align_K = K_num * ((radix + K_num - 1) / K_num);
  const int align_N = 64 * ((para_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;
  DT *wram_sratch = (DT *)wram_buffer;

  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};

  wram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  // FFT_CPX_T<DT> in_trans = {
  //     &nram_scratch[nram_scratch_offset],
  //     &nram_scratch[nram_scratch_offset + butterfly_num * radix]};
  // nram_scratch_offset += (butterfly_num * radix * 2);

  FFT_CPX_T<DT> in_trans = {nram_out_r, nram_out_i};
  // FFT_CPX_T<DT> out = in_trans;

  // FFT_CPX_T<DT> dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};
  // return;
  FFT_CPX_T<DT> dftmtx;

  // FFT_CPX_T<DT> dftmtx_align = {
  //     &nram_scratch[nram_scratch_offset],
  //     &nram_scratch[nram_scratch_offset + align_M * align_K]};
  // nram_scratch_offset += (align_M * align_K * 2);
  if (align_K != radix) {
    dftmtx = {&nram_scratch[nram_scratch_offset],
              &nram_scratch[nram_scratch_offset + align_M * align_K]};
    nram_scratch_offset += (align_M * align_K * 2);
    __bang_pad(dftmtx.r, nram_dftmtx, 1, radix, radix, 0, 0, 0,
               align_K - radix);
    __bang_pad(dftmtx.i, &nram_dftmtx[radix * radix], 1, radix, radix, 0, 0, 0,
               align_K - radix);

  } else {
    dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};
  }

  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);
  MLULOG("nram_scratch_offset: %d bytes.\n", nram_scratch_offset * sizeof(DT));
  MLULOG("nram offset in generic: %d bytes.\n",
         (size_t)&nram_scratch[nram_scratch_offset] - (size_t)nram_buffer);
  __bang_transpose(in_trans.r, nram_in_r, radix, butterfly_num);
  __bang_transpose(in_trans.i, nram_in_i, radix, butterfly_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  // if (butterfly_num != 1 && align_N != butterfly_num) {
  //   __bang_pad(in_align.r, in_trans.r, 1, butterfly_num, radix, 0,
  //              align_N - butterfly_num, 0, 0);
  //   __bang_pad(in_align.i, in_trans.i, 1, butterfly_num, radix, 0,
  //              align_N - butterfly_num, 0, 0);

  // } else {
  //   in_align = in_trans;
  //   // __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
  //   // 0);
  //   // __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
  //   // 0);
  // }
  // const int para_num = butterfly_num;
  // if (para_num != 1 && (align_K != radix || align_N != para_num)) {
  //   __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
  //              align_N - para_num, 0, align_K - radix);
  //   __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
  //              align_N - para_num, 0, align_K - radix);

  // } else if (para_num == 1 && align_K != radix) {
  //   __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
  //   0);
  //   __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
  //   0);
  // } else {
  //   // __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
  //   // 0);
  //   // __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
  //   // 0);
  //   in_align = in_trans;
  // }

  if (align_K == radix) {
    if (para_num != 1 && align_N != para_num) {
      __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
                 align_N - para_num, 0, 0);
      __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
                 align_N - para_num, 0, 0);

    } else {
      in_align = in_trans;
      // __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
      // 0);
      // __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
      // 0);
    }
  } else {
    if (para_num != 1) {
      __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
                 align_N - para_num, 0, align_K - radix);
      __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
                 align_N - para_num, 0, align_K - radix);

    } else {
      __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0, 0);
      __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0, 0);
    }
  }


  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);


  __bang_matmul((float *)RR, (float *)dftmtx.r, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx.i, (float *)in_wram.i, align_M,
                align_K, align_N);

  __bang_sub(out.r, RR, II, align_M * align_N);
  __bang_transpose(out_trans.r, out.r, align_M, align_N);
  __memcpy(nram_out_r, out_trans.r, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);

  __bang_matmul((float *)RI, (float *)dftmtx.r, (float *)in_wram.i, align_M,
                align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx.i, (float *)in_wram.r, align_M,
                align_K, align_N);

  __bang_add(out.i, RI, IR, align_M * align_N);
  __bang_transpose(out_trans.i, out.i, align_M, align_N);
  __memcpy(nram_out_i, out_trans.i, radix * butterfly_num * sizeof(DT),
           NRAM2NRAM);
}

template <typename DT>
__mlu_func__ void computeGenericButterflyOtherstagesMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int para_large_butterfly, int in_stride, int dir,
    int radix) {
  // return;
  const int para_num = butterfly_num * section_num * para_large_butterfly;
  // const int para_num_small = butterfly_num * section_num;
  const int align_M = radix;  // no align
  const int K_num = 64 / sizeof(DT);
  const int align_K = K_num * ((radix + K_num - 1) / K_num);
  const int align_N = 64 * ((para_num + 64 - 1) / 64);
  // const int align_N = 64 * ((butterfly_num + 64 - 1) / 64);

  FFT_CPX_T<DT> scratch_tw = {nram_tw, &nram_tw[butterfly_num * (radix - 1)]};

  int nram_scratch_offset = 0;
  int wram_scratch_offset = 0;

  // overlap
  FFT_CPX_T<DT> in_align2 = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  FFT_CPX_T<DT> out = {&nram_scratch[nram_scratch_offset],
                       &nram_scratch[nram_scratch_offset + align_M * align_N]};

  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> Fin = in_align2;
  // {
  //   int src_stride0 = butterfly_num * section_num * sizeof(DT);
  //   int src_segnum1 = radix - 1;
  //   int src_stride1 = radix * butterfly_num * section_num * sizeof(DT);
  //   int src_segnum2 = para_large_butterfly - 1;

  //   int dst_stride0 = para_num * sizeof(DT);
  //   int dst_segnum1 = src_segnum1;
  //   int dst_stride1 = src_stride0;
  //   int dst_segnum2 = src_segnum2;

  //   __memcpy(Fin.r, nram_in_r, sizeof(DT) * butterfly_num * section_num,
  //            NRAM2NRAM, dst_stride0, dst_segnum1, dst_stride1, dst_segnum2,
  //            src_stride0, src_segnum1, src_stride1, src_segnum2);
  //   __memcpy(Fin.i, nram_in_i, sizeof(DT) * butterfly_num * section_num,
  //            NRAM2NRAM, dst_stride0, dst_segnum1, dst_stride1, dst_segnum2,
  //            src_stride0, src_segnum1, src_stride1, src_segnum2);
  // }

  // [para_large_butterfly, large_radix]
  // [para_large_butterfly, radix, section_num, butterfly_num]

  // [radix, para_large_butterfly, section_num, butterfly_num]
  // [radix, para_num]

  // butterfly: [radix, radix] * [radix, para_num]
  TRANSPOSE_XYZ2YXZ_PAIR(Fin.r, Fin.i, nram_in_r, nram_in_i,
                         para_large_butterfly, radix,
                         butterfly_num * section_num, DT)

  DT *wram_sratch = (DT *)wram_buffer;
  FFT_CPX_T<DT> in_wram = {
      &wram_sratch[wram_scratch_offset],
      &wram_sratch[wram_scratch_offset + align_N * align_K]};
  wram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_align = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + align_N * align_K]};
  nram_scratch_offset += (align_N * align_K * 2);

  FFT_CPX_T<DT> in_trans = {
      &nram_scratch[nram_scratch_offset],
      &nram_scratch[nram_scratch_offset + para_num * radix]};
  nram_scratch_offset += (para_num * radix * 2);

  // const int out_stride = butterfly_num;
  // int Fin_stride = 0,
  // int Fout_stride = 0;

  FFT_CPX_T<DT> dftmtx;

  // FFT_CPX_T<DT> dftmtx_align = {
  //     &nram_scratch[nram_scratch_offset],
  //     &nram_scratch[nram_scratch_offset + align_M * align_K]};
  // nram_scratch_offset += (align_M * align_K * 2);
  if (align_K != radix) {
    dftmtx = {&nram_scratch[nram_scratch_offset],
              &nram_scratch[nram_scratch_offset + align_M * align_K]};
    nram_scratch_offset += (align_M * align_K * 2);
    __bang_pad(dftmtx.r, nram_dftmtx, 1, radix, radix, 0, 0, 0,
               align_K - radix);
    __bang_pad(dftmtx.i, &nram_dftmtx[radix * radix], 1, radix, radix, 0, 0, 0,
               align_K - radix);

  } else {
    dftmtx = {nram_dftmtx, &nram_dftmtx[radix * radix]};
  }

  DT *RR = &nram_scratch[nram_scratch_offset];
  DT *RI = &nram_scratch[nram_scratch_offset + align_K * align_N];
  DT *IR = &nram_scratch[nram_scratch_offset + align_K * align_N * 2];
  DT *II = &nram_scratch[nram_scratch_offset + align_K * align_N * 3];

  nram_scratch_offset += (align_K * 4 * align_N);

  // DT *RR = &nram_scratch[nram_scratch_offset];  // (9-1)*butterfly_num
  // DT *RI = &nram_scratch[nram_scratch_offset +
  //                        butterfly_num * (radix - 1)];  //
  //                        (9-1)*butterfly_num
  // DT *IR = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  2];  //
  //                                                  (9-1)*butterfly_num
  // DT *II = &nram_scratch[nram_scratch_offset + butterfly_num * (radix - 1) *
  //                                                  3];  //
  //                                                  (9-1)*butterfly_num
  // nram_scratch_offset += (butterfly_num * 4 * (radix - 1));

  // FFT_CPX_T<DT> scratch_in;
  // FFT_CPX_T<DT> Fin[radix];
  // FFT_CPX_T<DT> Fout[radix];

  // __memcpy(scratch_tw_r, sram_tw, butterfly_num * (3 - 1) * 2, SRAM2NRAM);

  // right pad radix -> align_K

  // scratch_in = FFT_CPX_T<DT>{nram_in_r, nram_in_i};

  // int nram_in_offset = 0;
  // for (int i = 0; i < radix; i++) {
  //   Fin[i].r = &nram_in_r[nram_in_offset];
  //   Fin[i].i = &nram_in_i[nram_in_offset];
  //   nram_in_offset += para_num;
  // }

  // FFT_CPX_T<DT> Fin = {nram_in_r, nram_in_i};
  // overlap

  // __bang_transpose(Fin.r, nram_in_r, para_large_butterfly, radix *
  // butterfly_num
  // * section_num);
  // __bang_transpose(Fin.i, nram_in_i, radix, para_num);

  // [para_large_butterfly, radix, butterfly_num * section_num]
  //  -> [radix, para_large_butterfly, butterfly_num * section_num]

  int nram_in_offset = para_num;

  for (int i = 1; i < radix; i++, nram_in_offset += para_num) {
    // __memcpy(&Fin.r[nram_in_offset],
    //          &nram_in_r[butterfly_num * section_num * i],
    //          butterfly_num * section_num * sizeof(DT), NRAM2NRAM,
    //          butterfly_num * section_num * sizeof(DT),
    //          butterfly_num * section_num * radix * sizeof(DT),
    //          para_large_butterfly - 1);

    // __memcpy(&Fin.i[nram_in_offset],
    //          &nram_in_i[butterfly_num * section_num * i],
    //          butterfly_num * section_num * sizeof(DT), NRAM2NRAM,
    //          butterfly_num * section_num * sizeof(DT),
    //          butterfly_num * section_num * radix * sizeof(DT),
    //          para_large_butterfly - 1);

    // 0: [para_large_butterfly, section_num, butterfly_num] * twiddles[butterfly_num * 0]
    // 1: [para_large_butterfly, section_num, butterfly_num] * twiddles[butterfly_num * 1]

    // if (i > 0) {
    __bang_cycle_mul(&RR[(i - 1) * para_num], &Fin.r[nram_in_offset],
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&RI[(i - 1) * para_num], &Fin.r[nram_in_offset],
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&IR[(i - 1) * para_num], &Fin.i[nram_in_offset],
                     &scratch_tw.r[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    __bang_cycle_mul(&II[(i - 1) * para_num], &Fin.i[nram_in_offset],
                     &scratch_tw.i[(i - 1) * butterfly_num], para_num,
                     butterfly_num);
    // }
  }

  __bang_sub(&Fin.r[para_num], RR, II, para_num * (radix - 1));
  __bang_transpose(in_trans.r, Fin.r, radix, para_num);

  __bang_add(&Fin.i[para_num], RI, IR, para_num * (radix - 1));
  __bang_transpose(in_trans.i, Fin.i, radix, para_num);

  // right pad radix -> align_K
  // bottom pad butterfly_num -> align_N

  // if (para_num != 1 && align_N != para_num) {
  //   __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
  //              align_N - para_num, 0, align_K - radix);
  //   __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
  //              align_N - para_num, 0, align_K - radix);

  // } else if (para_num == 1 && align_K != radix) {
  //   __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
  //   0);
  //   __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
  //   0);
  // } else {
  //   // __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
  //   // 0);
  //   // __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
  //   // 0);
  //   in_align = in_trans;
  // }

  if (align_K == radix) {
    if (para_num != 1 && align_N != para_num) {
      __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
                 align_N - para_num, 0, 0);
      __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
                 align_N - para_num, 0, 0);

    } else {
      in_align = in_trans;
      // __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
      // 0);
      // __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
      // 0);
    }
  } else {
    if (para_num != 1) {
      __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
                 align_N - para_num, 0, align_K - radix);
      __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
                 align_N - para_num, 0, align_K - radix);

    } else {
      __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0, 0);
      __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0, 0);
    }
  }

  // if (para_num != 1) {
  //   __bang_pad(in_align.r, in_trans.r, 1, para_num, radix, 0,
  //              align_N - para_num, 0, align_K - radix);
  //   __bang_pad(in_align.i, in_trans.i, 1, para_num, radix, 0,
  //              align_N - para_num, 0, align_K - radix);

  // } else {
  //   __bang_pad(in_align.r, in_trans.r, 1, radix, 1, 0, align_K - radix, 0,
  //   0);
  //   __bang_pad(in_align.i, in_trans.i, 1, radix, 1, 0, align_K - radix, 0,
  //   0);
  // }

  __bang_reshape_filter(in_align2.r, in_align.r, align_N, 1, 1, align_K);
  __bang_reshape_filter(in_align2.i, in_align.i, align_N, 1, 1, align_K);

  __memcpy(in_wram.r, in_align2.r, align_N * align_K * sizeof(DT), NRAM2WRAM);
  __memcpy(in_wram.i, in_align2.i, align_N * align_K * sizeof(DT), NRAM2WRAM);

  __bang_matmul((float *)RR, (float *)dftmtx.r, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_matmul((float *)II, (float *)dftmtx.i, (float *)in_wram.i, align_M,
                align_K, align_N);
  __bang_sub(out.r, RR, II, align_M * align_N);

  __bang_matmul((float *)RI, (float *)dftmtx.r, (float *)in_wram.i, align_M,
                align_K, align_N);
  __bang_matmul((float *)IR, (float *)dftmtx.i, (float *)in_wram.r, align_M,
                align_K, align_N);
  __bang_add(out.i, RI, IR, align_M * align_N);

  // FFT_CPX_T<DT> out_tmp = {nram_in_r, nram_in_i};

  // __memcpy(out_tmp.r, out.r, para_num * sizeof(DT),
  //           NRAM2NRAM, para_num * sizeof(DT), align_N * sizeof(DT),
  //           radix - 1);
  // __memcpy(out_tmp.i, out.i, para_num * sizeof(DT),
  //           NRAM2NRAM, para_num * sizeof(DT), align_N * sizeof(DT),
  //           radix - 1);
  // if(0)
  // for (int para_large_id = 0; para_large_id < para_large_butterfly;
  //      ++para_large_id) {
  //   for (int sec_count = 0; sec_count < section_num; ++sec_count) {
  //     // [radix, section_num, butterfly_num] -> [section_num, radix,
  //     // butterfly_num]

  //     // __memcpy(&nram_out_r[Fout_stride], &out.r[sec_count *
  //     butterfly_num],
  //     //          butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num *
  //     //          sizeof(DT), align_N * sizeof(DT), radix - 1);
  //     // __memcpy(&nram_out_i[Fout_stride], &out.i[sec_count *
  //     butterfly_num],
  //     //          butterfly_num * sizeof(DT), NRAM2NRAM, butterfly_num *
  //     //          sizeof(DT), align_N * sizeof(DT), radix - 1);

  //     // [radix, para_large_butterfly, section_num, butterfly_num] ->
  //     // [para_large_butterfly, section_num, radix, butterfly_num]

  //     __memcpy(&nram_out_r[Fout_stride],
  //              &out.r[sec_count * butterfly_num +
  //                     para_large_id * butterfly_num * section_num],
  //              butterfly_num * sizeof(DT), NRAM2NRAM,
  //              butterfly_num * sizeof(DT), align_N * sizeof(DT), radix - 1);
  //     __memcpy(&nram_out_i[Fout_stride],
  //              &out.i[sec_count * butterfly_num +
  //                     para_large_id * butterfly_num * section_num],
  //              butterfly_num * sizeof(DT), NRAM2NRAM,
  //              butterfly_num * sizeof(DT), align_N * sizeof(DT), radix - 1);

  //     // Fin_stride += butterfly_num;
  //     Fout_stride += radix * butterfly_num;
  //   }
  // }

  // [small_section_num, para_ldst_num, radix] -> [para_ldst_num,
  // small_section_num, radix]
  {
    int src_stride0 = butterfly_num * sizeof(DT);
    int src_segnum1 = para_large_butterfly * section_num - 1;
    int src_stride1 = align_N * sizeof(DT);
    int src_segnum2 = radix - 1;

    int dst_stride0 = radix * butterfly_num * sizeof(DT);
    int dst_segnum1 = para_large_butterfly * section_num - 1;
    int dst_stride1 = butterfly_num * sizeof(DT);
    int dst_segnum2 = radix - 1;

    __memcpy(nram_out_r, out.r, sizeof(DT) * butterfly_num, NRAM2NRAM,
             dst_stride0, dst_segnum1, dst_stride1, dst_segnum2, src_stride0,
             src_segnum1, src_stride1, src_segnum2);
    __memcpy(nram_out_i, out.i, sizeof(DT) * butterfly_num, NRAM2NRAM,
             dst_stride0, dst_segnum1, dst_stride1, dst_segnum2, src_stride0,
             src_segnum1, src_stride1, src_segnum2);
  }
}

template <typename DT>
__mlu_func__ void computeGenericButterflyLaststageMat(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_dftmtx, DT *nram_tw, int section_num,
    int butterfly_num, int para_large_butterfly, int in_stride, int dir,
    int radix) {
  computeGenericButterflyOtherstagesMat(
      nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch, nram_dftmtx,
      nram_tw, section_num, butterfly_num, para_large_butterfly, in_stride, dir,
      radix);
}
// template <typename DT>
// __mlu_func__ void load_input_gdram2nram()

template <typename DT>
__mlu_func__ void computeRadix3ButterflyOtherstages(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_tw, int section_num, int butterfly_num,
    int in_stride, int dir) {
  // MLULOG("computeRadix3ButterflyOtherstages\n");
  const int sign = (dir == FFT_FORWARD) ? 1 : -1;
  const DT tw3_1i = sign * TW3_1I_F;

  // const int out_stride = butterfly_num;
  int Fin_stride = 0, Fout_stride = 0;
  int sec_count;

  DT *scratch_r[4] = {nram_scratch, &nram_scratch[butterfly_num],
                      &nram_scratch[butterfly_num * 2],
                      &nram_scratch[butterfly_num * 3]};
  DT *scratch_i[4] = {
      &nram_scratch[butterfly_num * 4], &nram_scratch[butterfly_num * 5],
      &nram_scratch[butterfly_num * 6], &nram_scratch[butterfly_num * 7]};

  DT *_A_r = &nram_scratch[butterfly_num * 8];
  DT *_A_i = &nram_scratch[butterfly_num * 9];
  DT *_B_r = &nram_scratch[butterfly_num * 10];
  DT *_B_i = &nram_scratch[butterfly_num * 11];

  // DT *scratch_tw_r = &nram_scratch[butterfly_num * 12]; //
  // (3-1)*butterfly_num DT *scratch_tw_i = &nram_scratch[butterfly_num * 14];
  DT *scratch_tw_r = nram_tw;  // (3-1)*butterfly_num
  DT *scratch_tw_i = &nram_tw[butterfly_num * (3 - 1)];

  DT *CPX_MUL_RR = &nram_scratch[butterfly_num * 16];  // (3-1)*butterfly_num
  DT *CPX_MUL_RI = &nram_scratch[butterfly_num * 18];  // (3-1)*butterfly_num
  DT *CPX_MUL_IR = &nram_scratch[butterfly_num * 20];  // (3-1)*butterfly_num
  DT *CPX_MUL_II = &nram_scratch[butterfly_num * 22];  // (3-1)*butterfly_num

  DT *scratch_in_r;
  DT *scratch_in_i;

  // __memcpy(scratch_tw_r, sram_tw, butterfly_num * (3 - 1) * 2, SRAM2NRAM);

  for (sec_count = 0; sec_count < section_num; ++sec_count) {
    DT *Fout_r[3] = {&nram_out_r[Fout_stride],
                     &nram_out_r[butterfly_num + Fout_stride],
                     &nram_out_r[butterfly_num * 2 + Fout_stride]};
    DT *Fout_i[3] = {&nram_out_i[Fout_stride],
                     &nram_out_i[butterfly_num + Fout_stride],
                     &nram_out_i[butterfly_num * 2 + Fout_stride]};

    if (section_num == 1) {
      scratch_in_r = nram_in_r;
      scratch_in_i = nram_in_i;
      // scratch_in_r = nram_in_r + Fin_stride;
      // scratch_in_i = nram_in_i + Fin_stride;
    } else {
      // nram_scratch
      scratch_in_r = &nram_scratch[butterfly_num * 24];
      scratch_in_i = &nram_scratch[butterfly_num * (24 + 3)];
      __memcpy(scratch_in_r, nram_in_r + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               3 - 1);
      __memcpy(scratch_in_i, nram_in_i + Fin_stride, sizeof(DT) * butterfly_num,
               NRAM2NRAM, sizeof(DT) * butterfly_num, in_stride * sizeof(DT),
               3 - 1);
    }

    // CNAME(r3_other_stages_kernel)
    // (Fout + Fout_stride, Fin + Fin_stride, twiddles, tw3_1i, in_stride,
    // out_stride, butterfly_num);
    DT *Fin_r[3] = {scratch_in_r, &scratch_in_r[butterfly_num],
                    &scratch_in_r[butterfly_num * 2]};
    DT *Fin_i[3] = {scratch_in_i, &scratch_in_i[butterfly_num],
                    &scratch_in_i[butterfly_num * 2]};

    // rotate
    __bang_mul(CPX_MUL_RR, Fin_r[1], scratch_tw_r, butterfly_num * (3 - 1));
    __bang_mul(CPX_MUL_II, Fin_i[1], scratch_tw_i, butterfly_num * (3 - 1));
    __bang_mul(CPX_MUL_RI, Fin_r[1], scratch_tw_i, butterfly_num * (3 - 1));
    __bang_mul(CPX_MUL_IR, Fin_i[1], scratch_tw_r, butterfly_num * (3 - 1));

    __bang_sub(Fin_r[1], CPX_MUL_RR, CPX_MUL_II, butterfly_num * (3 - 1));
    __bang_add(Fin_i[1], CPX_MUL_RI, CPX_MUL_IR, butterfly_num * (3 - 1));

    // butterfly compute

    // CPX_ADD
    __bang_add(scratch_r[0], Fin_r[1], Fin_r[2], butterfly_num);
    __bang_add(scratch_i[0], Fin_i[1], Fin_i[2], butterfly_num);

    // CPX_SUB
    __bang_sub(scratch_r[1], Fin_r[1], Fin_r[2], butterfly_num);
    __bang_sub(scratch_i[1], Fin_i[1], Fin_i[2], butterfly_num);

    // CPX_ADD
    __bang_add(Fout_r[0], scratch_r[0], Fin_r[0], butterfly_num);
    __bang_add(Fout_i[0], scratch_i[0], Fin_i[0], butterfly_num);

    // CPX_MLA_OUTPLACE
    __bang_fusion(FUSION_FMA, _A_r, scratch_r[0], TW3_1R_F, Fin_r[0],
                  butterfly_num, butterfly_num);
    __bang_fusion(FUSION_FMA, _A_i, scratch_i[0], TW3_1R_F, Fin_i[0],
                  butterfly_num, butterfly_num);

    // CPX_MUL_S
    __bang_mul_scalar(_B_r, scratch_r[1], tw3_1i, butterfly_num);
    __bang_mul_scalar(_B_i, scratch_i[1], tw3_1i, butterfly_num);

    // OPENFFT_CPX_ODD_OUT
    __bang_sub(Fout_r[1], _A_r, _B_i, butterfly_num);
    __bang_add(Fout_i[1], _A_i, _B_r, butterfly_num);
    __bang_add(Fout_r[2], _A_r, _B_i, butterfly_num);
    __bang_sub(Fout_i[2], _A_i, _B_r, butterfly_num);

    Fin_stride += butterfly_num;
    Fout_stride += 3 * butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeRadix3ButterflyLaststage(
    DT *nram_out_r, DT *nram_out_i, DT *nram_in_r, DT *nram_in_i,
    DT *nram_scratch, DT *nram_tw, int section_num, int butterfly_num,
    int in_stride, int dir) {
  computeRadix3ButterflyOtherstages(nram_out_r, nram_out_i, nram_in_r,
                                    nram_in_i, nram_scratch, nram_tw,
                                    section_num, butterfly_num, in_stride, dir);
}

#define MAX_BUTTERFLY_ON_CHIP 32
#define NRAM_BUFFER_SIZE 1024

template <typename DT>
__mlu_func__ void computeLargeButterflyFirststage(
    DT *output, DT *input, int large_in_stride, int section_num,
    const DT *twiddles, const DT *dft_matrix, void *nram_buf,
    const int *small_factors, int dir, int nfft, int last_stage) {
  // constant
  // const int max_para_ldst_num = 3;
  const dft_table_entry *dft_table = (const dft_table_entry *)dft_matrix;
  // test
  // for(int i =0; i<3; i++){
  //   MLULOG("entry: %d, dft_table.radix: %d, dft_table.offset: %d.\n",
  //    i, dft_table[i].radix, dft_table[i].offset);
  // }
  // network info
  int radix, small_in_stride, small_stage_count, large_radix,
      _small_stage_count;
  int small_section_num, small_butterfly_num, value_mul;
  int tw_offset;
  // int max_radix = small_factors[4];
  _small_stage_count = small_factors[0];
  large_radix = small_factors[1];
  tw_offset = small_factors[2];

  // for (int i=2; i<= _small_stage_count; i++) {

  //   max_radix = max(small_factors[i*4], max_radix);

  // }

  // load compute store
  // (0)                              load 0 ping sync()
  // (1)              compute 0 ping  load 1 pong sync()
  // (2) store 0      compute 1 pong  load 2 ping sync()
  // (3) store 1      compute 2   load 3  sync()

  // compute last-large-stage (nram_out_r,nram_out_i) [2, large_radix]->
  // transpose -> [large_radix, 2]

  // complex array -> real array, imag array -> complex array
  // first-large-stage complex -> real array, imag array
  // other-large-stage none
  // last-large-stage real array, imag array -> complex
  // const int max_para_ldst_num = (6561 + large_radix - 1) / large_radix;
  // const int max_para_ldst_num = (2048 + large_radix - 1) / large_radix;
  // const int max_para_ldst_num = (1024 + large_radix - 1) / large_radix;
  // const int max_para_ldst_num = 6;
  const int max_para_ldst_num = (4096 + large_radix - 1) / large_radix;
  // const int max_para_ldst_num = 1;
  const DT *small_twiddles = twiddles + tw_offset * 2;  // complex

  // TODO(zrg): save nram space.
  // assign nram space
  int nram_buf_offset = 0;
  // sizeof(DT) * 2 * large_radix * max_para_ldst_num * 2
  DT *nram_in_r = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num;

  DT *nram_in_i = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num;

  DT *nram_out_r = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num;

  DT *nram_out_i = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num;

  // parallel load/store space
  // sizeof(DT) * 2 * large_radix * max_para_ldst_num * 4
  DT *nram_para_load_ping = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  DT *nram_para_load_pong = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  DT *nram_para_store_ping = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  DT *nram_para_store_pong = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  // transpose space: [radix, 2 * parrallel] -> [parrallel * 2, radix]
  // DT *nram_transpose_load = (DT *)nram_buf + nram_buf_offset;
  // nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  // FFT_CPX_T<DT> nram_transpose_temp;
  // temp out-space before transpose
  // if last-stage:
  //                compute_id 0 r
  //                compute_id 0 i
  //                compute_id 1 r
  //                compute_id 1 i
  // else:
  //                compute_id 0 r
  //                compute_id 1 i
  //                compute_id 0 r
  //                compute_id 1 i

  DT *_nram_tw = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * 2;  // complex

  // nram_buf_offset += max_radix * max_radix * 2;  // complex

  // DT *nram_dftmtx_ping = (DT *)nram_buf + nram_buf_offset;
  // nram_buf_offset += max_radix * max_radix * 2;  // complex

  // DT *nram_dftmtx_pong = (DT *)nram_buf + nram_buf_offset;
  // nram_buf_offset += max_radix * max_radix * 2;  // complex
  // nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  // DT *nram_transpose_store = (DT *)nram_buf + nram_buf_offset;
  // nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  // load dftmtx sample
  int ld_dft_radix = -1;
  const int max_radix = 64;
  DT *nram_dftmtx = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += max_radix * max_radix * 2;  // complex

  // const int ld_dft_radix = 16;
  // DT *nram_dftmtx8 = (DT *)nram_buf + nram_buf_offset;
  // nram_buf_offset += 8 * 8 * 2;  // complex

  // for (int entry = 0;; entry++) {
  //   if (dft_table[entry].radix == 8) {
  //     __memcpy_async(nram_dftmtx8, &dft_matrix[dft_table[entry].offset * 2],
  //                    sizeof(DT) * 2 * 8 * 8, GDRAM2NRAM);
  //     break;
  //   }
  //   if (dft_table[entry].radix == -1) {
  //     break;
  //   }
  // }

  // nram space used:
  // sizeof(DT) * 2 * large_radix * (max_para_ldst_num * 6 + 1) + sizeof(DT) * 2
  // * (max_radix * max_radix)
  // + sizeof(DT) * 2 * large_radix * max_para_ldst_num * 4
  DT *nram_scratch = (DT *)nram_buf + nram_buf_offset;

  MLULOG("nram used: %d bytes.\n",
         (int)((size_t)nram_scratch - (size_t)nram_buffer));
  // DT *nram_transpose_temp = nram_scratch;
  // overlap nram_scratch
  // nram_transpose_temp = {
  //     (DT *)nram_scratch,
  //     (DT *)nram_scratch + large_radix * ((int)last_stage) +
  //         large_radix * (1 - (int)last_stage) * max_para_ldst_num};
  // nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  __memcpy_async(_nram_tw, small_twiddles, large_radix * sizeof(DT) * 2,
                 SRAM2NRAM);

  // ceil
  int repeat_num = (section_num + max_para_ldst_num - 1) / max_para_ldst_num;

  for (int repeat_id = 0; repeat_id < repeat_num + 2; ++repeat_id) {
    // pipeline: load-stage
    if (repeat_id < repeat_num) {
      // MLULOG("pipeline: load-stage.\n");
      int i = max_para_ldst_num * repeat_id;
      DT *nram_para_load =
          (repeat_id % 2 == 0) ? nram_para_load_ping : nram_para_load_pong;

      // DT *nram_dftmtx =
      //     (repeat_id % 2 == 0) ? nram_dftmtx_ping : nram_dftmtx_pong;
      int para_load_num = (max_para_ldst_num > (section_num - i))
                              ? (section_num - i)
                              : max_para_ldst_num;
      if (section_num == 1) {
        __memcpy_async(nram_para_load, input, sizeof(DT) * 2 * large_radix,
                       GDRAM2NRAM);
      } else {
        // gather load
        // 2d memcpy
        // 0 1 2 3 4 ... 1023
        // GDRAM -> NRAM
        // 8bytes radix-1024
        // 64bytes

        __memcpy_async(nram_para_load, input + i * 2,
                       sizeof(DT) * 2 * para_load_num, GDRAM2NRAM,
                       sizeof(DT) * 2 * para_load_num,
                       large_in_stride * sizeof(DT) * 2, large_radix - 1);
      }
    }

    // pipeline: store-stage
    if (repeat_id >= 2) {
      // MLULOG("pipeline: store-stage.\n");
      int i = max_para_ldst_num * (repeat_id - 2);

      int para_store_num = (max_para_ldst_num > (section_num - i))
                               ? (section_num - i)
                               : max_para_ldst_num;

      DT *nram_para_store =
          (repeat_id % 2 == 0) ? nram_para_store_ping : nram_para_store_pong;

      if (last_stage) {
        if (section_num == 1) {
          __memcpy_async(output, nram_para_store, sizeof(DT) * 2 * large_radix,
                         NRAM2GDRAM);
        } else {
          // scatter-store
          __memcpy_async(output + i * large_radix * 2, nram_para_store,
                         sizeof(DT) * 2 * para_store_num * large_radix,
                         NRAM2GDRAM);
        }
      } else {
        // real
        __memcpy_async(output + i * large_radix, nram_para_store,
                       para_store_num * large_radix * sizeof(DT), NRAM2GDRAM);
        // imag
        __memcpy_async(output + i * large_radix + nfft,
                       nram_para_store + max_para_ldst_num * large_radix,
                       para_store_num * large_radix * sizeof(DT), NRAM2GDRAM);
      }
    }

    // pipeline: compute-stage
    
    if (repeat_id >= 1 && repeat_id < repeat_num + 1) {
      int i = max_para_ldst_num * (repeat_id - 1);

      DT *nram_para_load =
          (repeat_id % 2 != 0) ? nram_para_load_ping : nram_para_load_pong;
      DT *nram_para_store =
          (repeat_id % 2 != 0) ? nram_para_store_ping : nram_para_store_pong;

      int para_ldst_num = (max_para_ldst_num > (section_num - i))
                              ? (section_num - i)
                              : max_para_ldst_num;
      // // [large_radix, para_ldst_num, 2] -> [para_ldst_num, 2, large_radix]
      // __bang_transpose(nram_transpose_load, nram_para_load, large_radix,
      //                  2 * para_ldst_num);

      // [large_radix, para_ldst_num, 2] -> [2, para_ldst_num, large_radix]
      // overlap nram_out_r
      // DT *nram_transpose_load = nram_out_r;
      // __bang_transpose(nram_transpose_load, nram_para_load,
      //                  large_radix * para_ldst_num, 2);
      // // [large_radix, para_ldst_num] -> [para_ldst_num, large_radix]
      // __bang_transpose(nram_in_r, nram_transpose_load, large_radix,
      //                  para_ldst_num);
      // __bang_transpose(nram_in_i,
      //                  nram_transpose_load + large_radix * para_ldst_num,
      //                  large_radix, para_ldst_num);

      // DT *nram_transpose_load = nram_in_r;
      __bang_transpose(nram_in_r, nram_para_load, large_radix * para_ldst_num,
                       2);
      // [large_radix, para_ldst_num] -> [para_ldst_num, large_radix]
      // __bang_transpose(nram_in_r, nram_transpose_load, large_radix,
      //                  para_ldst_num);
      // __bang_transpose(nram_in_i,
      //                  nram_transpose_load + large_radix * para_ldst_num,
      //                  large_radix, para_ldst_num);

      for (int compute_id = 0; compute_id < para_ldst_num;
           compute_id += para_ldst_num) {
        // load real & imag

        radix = small_factors[4];
        small_section_num = small_factors[5];
        small_in_stride = small_factors[7];
        small_stage_count = _small_stage_count;

        // __memcpy(nram_in_r,
        //         nram_transpose_load + compute_id * large_radix * 2,
        //          large_radix * sizeof(DT) * 2, NRAM2NRAM);

        // first stage

        if (ld_dft_radix != radix) {
          ld_dft_radix = radix;
          for (int entry = 0;; entry++) {
            if (dft_table[entry].radix == ld_dft_radix) {
              __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                       sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix, SRAM2NRAM);
              break;
            }

            if (dft_table[entry].radix == -1) {
              break;
            }
          }
        }

        switch (radix) {
          // case 3:
          //   computeRadix3ButterflyFirststage(
          //       nram_out_r, nram_out_i, nram_transpose_load,
          //       nram_transpose_load + large_radix * para_ldst_num,
          //       nram_scratch, small_section_num * para_ldst_num,
          //       small_section_num * para_ldst_num, 1, dir);
          //   break;
          // case 9:
          //   MLULOG("computeRadix9ButterflyFirststage.\n");
          //   // computeRadix9ButterflyFirststage(
          //   //     nram_out_r, nram_out_i,
          //   //     nram_transpose_load + compute_id * large_radix * 2,
          //   //     nram_transpose_load + (compute_id * 2 + 1) * large_radix,
          //   //     nram_scratch, small_section_num, small_section_num, 1,
          //   dir);

          //   computeRadix9ButterflyFirststageMat(
          //       nram_out_r, nram_out_i, nram_transpose_load,
          //       nram_transpose_load + large_radix * para_ldst_num,
          //       nram_scratch, nram_dftmtx, small_section_num * para_ldst_num,
          //       small_section_num * para_ldst_num, 1, dir);
          //   break;
          // case 27:
          //   MLULOG("computeRadix27ButterflyFirststage.\n");

          //   computeRadix27ButterflyFirststageMat(
          //       nram_out_r, nram_out_i, nram_transpose_load,
          //       nram_transpose_load + large_radix * para_ldst_num,
          //       nram_scratch, nram_dftmtx, small_section_num * para_ldst_num,
          //       small_section_num * para_ldst_num, 1, dir);
          //   break;
          // case 8:
          //   computeRadix8ButterflyFirststageMat(
          //       nram_out_r, nram_out_i, nram_transpose_load,
          //       nram_transpose_load + large_radix * para_ldst_num,
          //       nram_scratch, nram_dftmtx, small_section_num * para_ldst_num,
          //       small_section_num * para_ldst_num, 1, dir);
          //   break;
          default:
            MLULOG("computeGenericButterflyFirststageMat: %d.\n", radix);
            computeGenericButterflyFirststageMat(
                nram_out_r, nram_out_i, nram_in_r,
                nram_in_r + large_radix * para_ldst_num, nram_scratch,
                nram_dftmtx, small_section_num * para_ldst_num,
                small_section_num * para_ldst_num, 1, dir, radix);
            break;
        }
        //           for (int j = 0; j < large_radix; j++) {
        //   MLULOG("output i: (%f, %f).\n", nram_out_r[j], nram_out_i[j]);
        // }

        // [radix, small_section_num, para_ldst_num] ->
        // [small_section_num, para_ldst_num, radix] -> [para_ldst_num,
        // small_section_num, radix]

        // __memcpy(nram_out_r + para_ldst_id * small_section_num * radix,
        //          nram_in_r + para_ldst_id * radix, sizeof(DT) * radix,
        //          NRAM2NRAM, sizeof(DT) * radix,
        //          para_ldst_num * radix * sizeof(DT), small_section_num - 1);

        // __memcpy(nram_out_i + para_ldst_id * small_section_num * radix,
        //          nram_in_i + para_ldst_id * radix, sizeof(DT) * radix,
        //          NRAM2NRAM, sizeof(DT) * radix,
        //          para_ldst_num * radix * sizeof(DT), small_section_num - 1);
        // __sync_move();

        small_stage_count--;
        if (small_stage_count == 0) {
          // nram to gdram

          if (last_stage) {
            //  [2, para_ldst_num, large_radix] -> [para_ldst_num, large_radix,
            //  2]
            // DT* nram_transpose_store = nram_in_r;

            __bang_transpose(nram_para_store, nram_out_r, 2,
                             max_para_ldst_num * large_radix);

          } else {
            //  [2, para_ldst_num, large_radix] -> [2, para_ldst_num,
            //  large_radix]
            // TODO(zrg): redundant move
            __memcpy(nram_para_store, nram_out_r,
                     para_ldst_num * large_radix * sizeof(DT), NRAM2NRAM);
            __memcpy(nram_para_store + max_para_ldst_num * large_radix,
                     nram_out_i, para_ldst_num * large_radix * sizeof(DT),
                     NRAM2NRAM);
          }

          continue;
        }

        // [small_section_num, para_ldst_num, radix] -> [para_ldst_num,
        // small_section_num, radix]

        FFT_SWAP_PTR(nram_out_r, nram_in_r);
        FFT_SWAP_PTR(nram_out_i, nram_in_i);

        TRANSPOSE_XYZ2YXZ_PAIR(nram_out_r, nram_out_i, nram_in_r, nram_in_i,
                               small_section_num, para_ldst_num, radix, DT)

        value_mul = 8;
        // DT *sram_tw = (DT *)sram_buffer;
        DT *nram_tw = _nram_tw;

        for (; small_stage_count > 1; small_stage_count--) {
          FFT_SWAP_PTR(nram_out_r, nram_in_r);
          FFT_SWAP_PTR(nram_out_i, nram_in_i);

          // value_mul = (_small_stage_count - small_stage_count + 1) << 2;

          // // update parameter
          radix = small_factors[value_mul++];
          small_section_num = small_factors[value_mul++];
          small_butterfly_num = small_factors[value_mul++];
          small_in_stride = small_factors[value_mul++];
          // value_mul += 4;
          // copy GDRAM2SRAM

          // if (compute_id == 0 && repeat_id == 1 && 0) {
          //   __memcpy(nram_tw, small_twiddles,
          //            small_butterfly_num * (radix - 1) * sizeof(DT) * 2,
          //            GDRAM2NRAM);
          //   small_twiddles += small_butterfly_num * (radix - 1) * 2;
          // }

          if (ld_dft_radix != radix) {
            ld_dft_radix = radix;
            for (int entry = 0;; entry++) {
              if (dft_table[entry].radix == ld_dft_radix) {
                __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                         sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix,
                         SRAM2NRAM);
                break;
              }

              if (dft_table[entry].radix == -1) {
                break;
              }
            }
          }

          switch (radix) {
            case 2:
              // computeRadix2ButterflyOtherstages(Fout, Fin, section_num,
              // section_num, 1, dir);
              break;
            case 3:
              computeRadix3ButterflyOtherstages(
                  nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                  nram_tw, small_section_num, small_butterfly_num,
                  small_in_stride, dir);
              break;
            // case 9:
            //   computeRadix9ButterflyOtherstages(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_tw, small_section_num, small_butterfly_num,
            //       small_in_stride, dir);
            //   break;
            // case 27:
            //   computeRadix27ButterflyOtherstagesMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            // case 8:
            //   computeRadix8ButterflyOtherstagesMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            // case 16:
            //   computeRadix16ButterflyOtherstagesMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            // case 32:
            //   computeRadix32ButterflyOtherstagesMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            // case 64:
            //   computeRadix64ButterflyOtherstagesMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            default:
              // computeGenericButterflyOtherstages(Fout, buffer, twiddles,
              // radix, section_num, butterfly_num, in_stride, 0, dir);
              MLULOG("computeGenericButterflyOtherstagesMat: %d.\n", radix);
              computeGenericButterflyOtherstagesMat(
                  nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                  nram_dftmtx, nram_tw, small_section_num, small_butterfly_num,
                  para_ldst_num, small_in_stride, dir, radix);
              break;
          }

          nram_tw += small_butterfly_num * (radix - 1) * 2;
        }  // for (stage_count)

        //           for (int j = 0; j < large_radix; j++) {
        //   MLULOG("output i: (%f, %f).\n", nram_out_r[j], nram_out_i[j]);
        // }

        // MLULOG("butterfly id: %d\n", i);
        // last stage
        {
          FFT_SWAP_PTR(nram_out_r, nram_in_r);
          FFT_SWAP_PTR(nram_out_i, nram_in_i);

          // copy GDRAM2SRAM

          // update parameter
          // value_mul = _small_stage_count << 2;
          radix = small_factors[value_mul++];
          small_section_num = small_factors[value_mul++];
          small_butterfly_num = small_factors[value_mul++];
          small_in_stride = small_factors[value_mul];

          // if (compute_id == 0 && repeat_id == 1 && 0) {
          //   __memcpy(nram_tw, small_twiddles,
          //            small_butterfly_num * (radix - 1) * sizeof(DT) * 2,
          //            GDRAM2NRAM);
          // }
          if (ld_dft_radix != radix) {
            ld_dft_radix = radix;
            for (int entry = 0;; entry++) {
              if (dft_table[entry].radix == ld_dft_radix) {
                __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                         sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix,
                         SRAM2NRAM);
                break;
              }

              if (dft_table[entry].radix == -1) {
                break;
              }
            }
          }

          switch (radix) {
            case 2:
              break;
            // case 3:
            //   computeRadix3ButterflyLaststage(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_tw, small_section_num, small_butterfly_num,
            //       small_in_stride, dir);
            //   break;
            // case 9:
            //   computeRadix9ButterflyLaststage(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_tw, small_section_num, small_butterfly_num,
            //       small_in_stride, dir);
            //   break;
            // case 27:
            //   computeRadix27ButterflyLaststageMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            // case 8:
            //   computeRadix8ButterflyLaststageMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            // case 16:
            //   computeRadix16ButterflyLaststageMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            // case 32:
            //   computeRadix32ButterflyLaststageMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            // case 64:
            //   computeRadix64ButterflyLaststageMat(
            //       nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
            //       nram_dftmtx, nram_tw, small_section_num,
            //       small_butterfly_num, small_in_stride, dir);
            //   break;
            default:
              MLULOG("computeGenericButterflyLaststageMat: %d.\n", radix);
              computeGenericButterflyLaststageMat(
                  nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                  nram_dftmtx, nram_tw, small_section_num, small_butterfly_num,
                  para_ldst_num, small_in_stride, dir, radix);
              MLULOG("computeGenericButterflyLaststageMat: %d End.\n", radix);
              break;
          }

          if (last_stage) {
            //  [2, para_ldst_num, large_radix] -> [para_ldst_num, large_radix,
            //  2]
            // DT* nram_transpose_store = nram_in_r;

            __bang_transpose(nram_para_store, nram_out_r, 2,
                             max_para_ldst_num * large_radix);

          } else {
            //  [2, para_ldst_num, large_radix] -> [2, para_ldst_num,
            //  large_radix]
            // TODO(zrg): redundant move
            __memcpy(nram_para_store, nram_out_r,
                     para_ldst_num * large_radix * sizeof(DT), NRAM2NRAM);
            __memcpy(nram_para_store + max_para_ldst_num * large_radix,
                     nram_out_i, para_ldst_num * large_radix * sizeof(DT),
                     NRAM2NRAM);
          }

          // if (last_stage) {
          //   // MLULOG("last_stage. \n");

          //   // __memcpy(nram_transpose_temp + (compute_id * 2) * large_radix,
          //   //          nram_out_r, large_radix * sizeof(DT), NRAM2NRAM);
          //   // __memcpy(nram_transpose_temp
          //   // + (compute_id * 2 + 1) * large_radix,
          //   //          nram_out_i, large_radix * sizeof(DT), NRAM2NRAM);

          //   __memcpy(nram_transpose_temp + (compute_id * 2) * large_radix,
          //            nram_out_r, large_radix * sizeof(DT) * 2, NRAM2NRAM);
          //   __bang_transpose(
          //       nram_para_store + (compute_id * 2) * large_radix,
          //       nram_transpose_temp + (compute_id * 2) * large_radix, 2,
          //       large_radix);

          // } else {
          //   // MLULOG("not last_stage. \n");
          //   __memcpy(nram_para_store + compute_id * large_radix, nram_out_r,
          //            large_radix * sizeof(DT), NRAM2NRAM);
          //   __memcpy(nram_para_store +
          //                (compute_id + max_para_ldst_num) * large_radix,
          //            nram_out_i, large_radix * sizeof(DT), NRAM2NRAM);
          // }
          // MLULOG("last_stage. \n");
        }
      }
    }

    __sync();
    // __sync_io();
    // __sync_compute();
    // __sync_move();

    // __sync();
  }
}

template <typename DT>
__mlu_func__ void computeLargeButterflyOtherstages(
    DT *output, DT *input, const DT *cur_large_twiddles, const DT *_twiddles,
    const DT *dft_matrix, int large_section_num, int large_butterfly_num,
    int large_in_stride, void *nram_buf, const int *small_factors, int nfft,
    int dir, int last_stage) {
  // return;
  const dft_table_entry *dft_table = (const dft_table_entry *)dft_matrix;

  int radix, small_in_stride, small_stage_count, large_radix,
      _small_stage_count;
  int small_section_num, small_butterfly_num, value_mul;

  const int large_out_stride = large_butterfly_num;
  int tw_offset;

  _small_stage_count = small_factors[0];
  large_radix = small_factors[1];
  tw_offset = small_factors[2];

  const DT *small_twiddles = _twiddles + tw_offset * 2;  // complex
  // const DT *small_twiddles;                               // complex

  // MLULOG("small_section_num:      %d.\n\n\n", small_section_num);
  // max num for parallel  load/store
  // const int max_para_ldst_num = 2187 / large_radix;
  // const int max_para_ldst_num = (2187 + large_radix - 1) / large_radix;
  // const int max_para_ldst_num = (512 + large_radix - 1) / large_radix;
  // const int max_para_ldst_num = 20;
  const int max_para_ldst_num = (4096 + large_radix - 1) / large_radix;

  // int para_ldst_num;
  // TODO(zrg): save nram space.
  // __nram__ DT nram_space[MAX_BUTTERFLY_ON_CHIP * 2];
  // 0 1 2 3 4 5
  // 0   1   3
  // DT *nram_buf_end = (DT*)&((uint8_t*)nram_buf)[NRAM_BUFFER_SIZE];
  // FFT_CPX_T<DT> *nram_in = (FFT_CPX_T<DT> *)nram_buffer;
  // FFT_CPX_T<DT> *nram_out = &nram_in[large_radix];
  // FFT_CPX_T<DT> *nram_buf = &nram_in[large_radix * 2];
  int nram_buf_offset = 0;
  DT *nram_in_r = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num;

  DT *nram_in_i = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num;

  DT *nram_out_r = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num;

  DT *nram_out_i = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * max_para_ldst_num;

  // parallel load/store space
  FFT_CPX_T<DT> nram_para_load_in_ping = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * max_para_ldst_num};
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  FFT_CPX_T<DT> nram_para_load_in_pong = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * max_para_ldst_num};
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  FFT_CPX_T<DT> nram_para_load_tw_ping = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * max_para_ldst_num};
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  FFT_CPX_T<DT> nram_para_load_tw_pong = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * max_para_ldst_num};
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  FFT_CPX_T<DT> nram_para_store_ping = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * max_para_ldst_num};
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  FFT_CPX_T<DT> nram_para_store_pong = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * max_para_ldst_num};
  nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  // overlap nram_in
  FFT_CPX_T<DT> nram_transpose_temp;
  // temp out-space before transpose
  // if last-stage:
  //                compute_id 0 r
  //                compute_id 0 i
  //                compute_id 1 r
  //                compute_id 1 i
  // else:
  //                compute_id 0 r
  //                compute_id 1 i
  //                compute_id 0 r
  //                compute_id 1 i
  nram_transpose_temp = {
      (DT *)nram_in_r,
      (DT *)nram_in_r + large_radix * ((int)last_stage) +
          large_radix * (1 - (int)last_stage) * max_para_ldst_num};
  // nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  DT *_nram_tw = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * 2;  // complex

  // transpose space: [radix, 2 * parrallel] -> [parrallel * 2, radix]
  // FFT_CPX_T<DT> nram_transpose_load = {
  //     (DT *)nram_buf + nram_buf_offset,
  //     (DT *)nram_buf + nram_buf_offset + large_radix * max_para_ldst_num};
  // nram_buf_offset += large_radix * max_para_ldst_num * 2;  // complex

  // load dftmtx sample
  // const int ld_dft_radix = 16;
  // DT *nram_dftmtx = (DT *)nram_buf + nram_buf_offset;
  // nram_buf_offset += ld_dft_radix * ld_dft_radix * 2;  // complex

  // for (int entry = 0;; entry++) {
  //   if (dft_table[entry].radix == ld_dft_radix) {
  //     __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
  //              sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix, GDRAM2NRAM);
  //     break;
  //   }

  //   if (dft_table[entry].radix == -1) {
  //     break;
  //   }
  // }
  int ld_dft_radix = -1;
  const int max_radix = 64;
  DT *nram_dftmtx = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += max_radix * max_radix * 2;  // complex

  DT *nram_scratch = (DT *)nram_buf + nram_buf_offset;

  // temp overlap with "nram_scratch"
  DT *CPX_MUL_RR = nram_scratch;
  DT *CPX_MUL_RI = &CPX_MUL_RR[large_radix * max_para_ldst_num];
  DT *CPX_MUL_IR = &CPX_MUL_RI[large_radix * max_para_ldst_num];
  DT *CPX_MUL_II = &CPX_MUL_IR[large_radix * max_para_ldst_num];

  nram_buf_offset += large_radix * max_para_ldst_num * 4;  // complex

  // size: (large_radix - 1) * max_para_ldst_num
  // DT *scratch_tw_r = &CPX_MUL_II[large_radix * max_para_ldst_num];
  // DT *scratch_tw_i = &scratch_tw_r[(large_radix - 1) * max_para_ldst_num];

  // if (nram_buf == NULL) {
  //   MLULOG("nram_buf: NULL.\n");
  // }
  // if (input == NULL) {
  //   MLULOG("input: NULL.\n");
  // }
  // if (output == NULL) {
  //   MLULOG("output: NULL.\n");
  // }

  // if (cur_large_twiddles == NULL) {
  //   MLULOG("twiddles: NULL.\n");
  // }

  // __nram__ DT *nram_buf = &nram_space[MAX_BUTTERFLY_ON_CHIP*2];

  // DT *odd_extra_buffer = buffer + nfft*2; // for in_place temp buffer

  // const int para_num = 1;

  int Fin_stride = 0, Fout_stride = 0;
  int sec_count;
  int repeat_num =
      (large_butterfly_num + max_para_ldst_num - 1) / max_para_ldst_num;
  for (sec_count = 0; sec_count < large_section_num; ++sec_count) {
    for (int repeat_id = 0; repeat_id < repeat_num + 2; ++repeat_id) {
      // small_twiddles = _small_twiddles;

      // pipeline: load-stage

      if (repeat_id < repeat_num) {
        // MLULOG("pipeline: load-stage.\n");
        int i = max_para_ldst_num * repeat_id;
        FFT_CPX_T<DT> nram_para_load_in = (repeat_id % 2 == 0)
                                              ? nram_para_load_in_ping
                                              : nram_para_load_in_pong;

        FFT_CPX_T<DT> nram_para_load_tw = (repeat_id % 2 == 0)
                                              ? nram_para_load_tw_ping
                                              : nram_para_load_tw_pong;

        int para_load_num = (max_para_ldst_num > (large_butterfly_num - i))
                                ? (large_butterfly_num - i)
                                : max_para_ldst_num;
        if (para_load_num != 1) {
          __memcpy_async(nram_para_load_in.r, input + Fin_stride + i,
                         sizeof(DT) * para_load_num, GDRAM2NRAM,
                         sizeof(DT) * para_load_num,
                         large_in_stride * sizeof(DT), large_radix - 1);
          __memcpy_async(nram_para_load_in.i, input + nfft + Fin_stride + i,
                         sizeof(DT) * para_load_num, GDRAM2NRAM,
                         sizeof(DT) * para_load_num,
                         large_in_stride * sizeof(DT), large_radix - 1);
          __memcpy_async(nram_para_load_tw.r, cur_large_twiddles + i,
                         sizeof(DT) * para_load_num, SRAM2NRAM,
                         sizeof(DT) * para_load_num,
                         large_out_stride * sizeof(DT), large_radix - 2);
          __memcpy_async(
              nram_para_load_tw.i,
              cur_large_twiddles + large_butterfly_num * (large_radix - 1) + i,
              sizeof(DT) * para_load_num, SRAM2NRAM, sizeof(DT) * para_load_num,
              large_out_stride * sizeof(DT), large_radix - 2);
        }
      }

      // pipeline: store-stage
      if (repeat_id >= 2) {
        // MLULOG("pipeline: store-stage.\n");
        int i = max_para_ldst_num * (repeat_id - 2);

        int para_store_num = (max_para_ldst_num > (large_butterfly_num - i))
                                 ? (large_butterfly_num - i)
                                 : max_para_ldst_num;

        FFT_CPX_T<DT> nram_para_store =
            (repeat_id % 2 == 0) ? nram_para_store_ping : nram_para_store_pong;

        if (last_stage) {
          // __memcpy_async(
          //     output + (Fout_stride + i * large_radix) * 2,
          //     nram_para_store.r,
          //     sizeof(DT) * 2 * para_store_num * large_radix, NRAM2GDRAM);

          __memcpy_async(output + (Fout_stride + i) * 2, nram_para_store.r,
                         sizeof(DT) * 2 * para_store_num, NRAM2GDRAM,
                         large_out_stride * 2 * sizeof(DT),
                         sizeof(DT) * 2 * para_store_num, large_radix - 1);

        } else {
          // // real
          // __memcpy_async(output + Fout_stride + i * large_radix,
          //                nram_para_store.r,
          //                para_store_num * large_radix * sizeof(DT),
          //                NRAM2GDRAM);
          // // imag
          // __memcpy_async(output + Fout_stride + i * large_radix + nfft,
          //                nram_para_store.i,
          //                para_store_num * large_radix * sizeof(DT),
          //                NRAM2GDRAM);
          // real
          __memcpy_async(output + Fout_stride + i, nram_para_store.r,
                         para_store_num * sizeof(DT), NRAM2GDRAM,
                         large_out_stride * sizeof(DT),
                         sizeof(DT) * para_store_num, large_radix - 1);
          // imag
          __memcpy_async(output + Fout_stride + i + nfft, nram_para_store.i,
                         para_store_num * sizeof(DT), NRAM2GDRAM,
                         large_out_stride * sizeof(DT),
                         sizeof(DT) * para_store_num, large_radix - 1);
        }
      }
      // __sync();
      // pipeline: compute-stage
      
      if (repeat_id >= 1 && repeat_id < repeat_num + 1) {
        // MLULOG("pipeline: compute-stage.\n");
        int i = max_para_ldst_num * (repeat_id - 1);

        FFT_CPX_T<DT> nram_para_load_in = (repeat_id % 2 != 0)
                                              ? nram_para_load_in_ping
                                              : nram_para_load_in_pong;

        FFT_CPX_T<DT> nram_para_load_tw = (repeat_id % 2 != 0)
                                              ? nram_para_load_tw_ping
                                              : nram_para_load_tw_pong;

        FFT_CPX_T<DT> nram_para_store =
            (repeat_id % 2 != 0) ? nram_para_store_ping : nram_para_store_pong;

        int para_ldst_num = (max_para_ldst_num > (large_butterfly_num - i))
                                ? (large_butterfly_num - i)
                                : max_para_ldst_num;

        // __bang_transpose(nram_transpose_load, nram_para_load, large_radix,
        //                  2 * para_ldst_num);

        // rotation-large
        __bang_mul(CPX_MUL_RR, nram_para_load_in.r + para_ldst_num,
                   nram_para_load_tw.r, para_ldst_num * (large_radix - 1));
        __bang_mul(CPX_MUL_II, nram_para_load_in.i + para_ldst_num,
                   nram_para_load_tw.i, para_ldst_num * (large_radix - 1));
        __bang_mul(CPX_MUL_RI, nram_para_load_in.r + para_ldst_num,
                   nram_para_load_tw.i, para_ldst_num * (large_radix - 1));
        __bang_mul(CPX_MUL_IR, nram_para_load_in.i + para_ldst_num,
                   nram_para_load_tw.r, para_ldst_num * (large_radix - 1));

        __bang_sub(nram_para_load_in.r + para_ldst_num, CPX_MUL_RR, CPX_MUL_II,
                   para_ldst_num * (large_radix - 1));
        __bang_add(nram_para_load_in.i + para_ldst_num, CPX_MUL_RI, CPX_MUL_IR,
                   para_ldst_num * (large_radix - 1));

        // __bang_transpose(nram_transpose_load.r, nram_para_load_in.r,
        //                  large_radix, para_ldst_num);
        // __bang_transpose(nram_transpose_load.i, nram_para_load_in.i,
        //                  large_radix, para_ldst_num);

        // for (int compute_id = 0; compute_id < para_ldst_num; compute_id++) {
        for (int compute_id = 0; compute_id < para_ldst_num;
             compute_id += para_ldst_num) {
          // load real & imag

          radix = small_factors[4];
          small_section_num = small_factors[5];
          small_in_stride = small_factors[7];
          small_stage_count = _small_stage_count;

          // __memcpy(nram_in_r,
          //         nram_transpose_load + compute_id * large_radix * 2,
          //          large_radix * sizeof(DT) * 2, NRAM2NRAM);

          // first stage
          // if(0)
          if (ld_dft_radix != radix) {
            ld_dft_radix = radix;
            for (int entry = 0;; entry++) {
              if (dft_table[entry].radix == ld_dft_radix) {
                __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                         sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix,
                         SRAM2NRAM);
                break;
              }

              if (dft_table[entry].radix == -1) {
                break;
              }
            }
          }

          switch (radix) {
            // case 2:
            //   break;
            // case 3:
            //   computeRadix3ButterflyFirststage(
            //       nram_out_r, nram_out_i,
            //       nram_para_load_in.r + compute_id * large_radix,
            //       nram_para_load_in.i + compute_id * large_radix,
            //       nram_scratch, small_section_num * para_ldst_num,
            //       small_section_num * para_ldst_num, 1, dir);
            //   break;
            // case 9:
            //   MLULOG("computeRadix9ButterflyFirststage.\n");
            //   computeRadix9ButterflyFirststageMat(
            //       nram_out_r, nram_out_i,
            //       nram_para_load_in.r + compute_id * large_radix,
            //       nram_para_load_in.i + compute_id * large_radix,
            //       nram_scratch, nram_dftmtx, small_section_num *
            //       para_ldst_num, small_section_num * para_ldst_num, 1, dir);
            //   break;
            // case 27:
            //   MLULOG("computeRadix9ButterflyFirststage.\n");
            //   computeRadix27ButterflyFirststageMat(
            //       nram_out_r, nram_out_i,
            //       nram_para_load_in.r + compute_id * large_radix,
            //       nram_para_load_in.i + compute_id * large_radix,
            //       nram_scratch, nram_dftmtx, small_section_num *
            //       para_ldst_num, small_section_num * para_ldst_num, 1, dir);
            //   break;
            // // case 16:
            // //   computeRadix16ButterflyFirststageMat(
            // //       nram_out_r, nram_out_i,
            // //       nram_para_load_in.r + compute_id * large_radix,
            // //       nram_para_load_in.i + compute_id * large_radix,
            // //       nram_scratch, nram_dftmtx, small_section_num *
            // //       para_ldst_num, small_section_num * para_ldst_num, 1,
            // dir);
            // //   break;
            // case 32:
            //   computeRadix32ButterflyFirststageMat(
            //       nram_out_r, nram_out_i,
            //       nram_para_load_in.r + compute_id * large_radix,
            //       nram_para_load_in.i + compute_id * large_radix,
            //       nram_scratch, nram_dftmtx, small_section_num *
            //       para_ldst_num, small_section_num * para_ldst_num, 1, dir);
            //   break;
            // case 64:
            //   computeRadix64ButterflyFirststageMat(
            //       nram_out_r, nram_out_i,
            //       nram_para_load_in.r + compute_id * large_radix,
            //       nram_para_load_in.i + compute_id * large_radix,
            //       nram_scratch, nram_dftmtx, small_section_num *
            //       para_ldst_num, small_section_num * para_ldst_num, 1, dir);
            //   break;
            default:
              // computeGenericButterflyFirststage(Fout, buffer, twiddles,
              // radix, section_num, butterfly_num, in_stride, 0, dir);
              MLULOG("computeGenericButterflyFirststageMat: %d.\n", radix);

              // para_ldst_num = 1
              // in: [radix, butterfly_num]
              // butterfly: [radix, radix] * [radix, butterfly_num]
              // out_butterfly: [radix, butterfly_num]
              // out: [butterfly_num, radix]

              // para_ldst_num != 1
              // in: [radix, butterfly_num, para_ldst_num] == [large_radix, para_ldst_num]
              // butterfly: [radix, radix] * [radix, butterfly_num, para_ldst_num]
              // out_butterfly: [radix, butterfly_num, para_ldst_num] == [radix, butterfly_num * para_ldst_num]
              // out: [butterfly_num, para_ldst_num, radix]

              computeGenericButterflyFirststageMat(
                  nram_out_r, nram_out_i, nram_para_load_in.r,
                  nram_para_load_in.i, nram_scratch, nram_dftmtx,
                  small_section_num * para_ldst_num,
                  small_section_num * para_ldst_num, 1, dir, radix);
              break;
          }


          //           for (int j = 0; j < large_radix; j++) {
          //   MLULOG("output i: (%f, %f).\n", nram_out_r[j], nram_out_i[j]);
          // }

          // [radix, small_section_num, para_ldst_num] ->
          // [small_section_num, para_ldst_num, radix] ->
          // [para_ldst_num, small_section_num, radix] ->
          // [small_section_num, radix, para_ldst_num] ==
          // [large_radix, para_ldst_num]

          small_stage_count--;
          if (small_stage_count == 0) {
            // nram to gdram

            // if (last_stage) {
            //   //  [2, para_ldst_num, large_radix] -> [para_ldst_num,
            //   large_radix,
            //   //  2]
            //   // DT* nram_transpose_store = nram_in_r;

            //   __bang_transpose(nram_in_r, nram_out_r, 2,
            //                    max_para_ldst_num * large_radix);

            // } else {
            //   //  [2, para_ldst_num, large_radix] -> [2, para_ldst_num,
            //   //  large_radix]
            //   // TODO(zrg): redundant move
            //   __memcpy(nram_in_r, nram_out_r,
            //            para_ldst_num * large_radix * sizeof(DT), NRAM2NRAM);
            //   __memcpy(nram_in_i,
            //            nram_out_i, para_ldst_num * large_radix * sizeof(DT),
            //            NRAM2NRAM);
            // }

            // [nfft, 2] -> [2, nfft] -> [2, nfft] -> [nfft, 2]
            if (last_stage) {
              __memcpy(nram_transpose_temp.r +
                           compute_id * large_radix * (1 + (int)last_stage),
                       nram_out_r, sizeof(DT) * large_radix, NRAM2NRAM,
                       sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
                       para_ldst_num - 1);

              __memcpy(nram_transpose_temp.i +
                           compute_id * large_radix * (1 + (int)last_stage),
                       nram_out_i, sizeof(DT) * large_radix, NRAM2NRAM,
                       sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
                       para_ldst_num - 1);

              __bang_transpose(nram_para_store.r, nram_transpose_temp.r,
                               para_ldst_num * 2, large_radix);
            } else {
              __bang_transpose(nram_para_store.r, nram_out_r, para_ldst_num,
                               large_radix);
              __bang_transpose(nram_para_store.i, nram_out_i, para_ldst_num,
                               large_radix);
            }

            continue;
          }


          FFT_SWAP_PTR(nram_out_r, nram_in_r);
          FFT_SWAP_PTR(nram_out_i, nram_in_i);
          // DT* nram_transpose_store = nram_in_r;

          // for (int para_ldst_id = 0; para_ldst_id < para_ldst_num;
          //      para_ldst_id++) {
          //   __memcpy(nram_out_r + para_ldst_id * small_section_num * radix,
          //            nram_in_r + para_ldst_id * radix, sizeof(DT) * radix,
          //            NRAM2NRAM, sizeof(DT) * radix,
          //            para_ldst_num * radix * sizeof(DT), small_section_num -
          //            1);

          //   __memcpy(nram_out_i + para_ldst_id * small_section_num * radix,
          //            nram_in_i + para_ldst_id * radix, sizeof(DT) * radix,
          //            NRAM2NRAM, sizeof(DT) * radix,
          //            para_ldst_num * radix * sizeof(DT), small_section_num -
          //            1);
          // }

          // after first stage: [butterfly_num, para_ldst_num, radix]
          // other in: [para_ldst_num, butterfly_num, radix] == [para_ldst_num, large_radix]
          TRANSPOSE_XYZ2YXZ_PAIR(nram_out_r, nram_out_i, nram_in_r, nram_in_i,
                                 small_section_num, para_ldst_num, radix, DT)



          // TODO(zrg) : add not last-stage
          // if (small_stage_count == 0) {
          //   // if last-stage: stride = large_radix * 2
          //   //                compute_id 0 r
          //   //                compute_id 0 i
          //   //                compute_id 1 r
          //   //                compute_id 1 i
          //   // else: stride = large_radix
          //   //                compute_id 0 r
          //   //                compute_id 1 i
          //   //                compute_id 0 r
          //   //                compute_id 1 i

          //   // [radix, small_section_num, para_ldst_num] ->
          //   // [small_section_num, para_ldst_num, radix] ->
          //   // [para_ldst_num, small_section_num, radix] ->
          //   // [small_section_num, radix, para_ldst_num] ==
          //   // [large_radix, para_ldst_num]

          //   __memcpy(nram_transpose_temp.r +
          //                compute_id * large_radix * (1 + (int)last_stage),
          //            nram_out_r, sizeof(DT) * large_radix, NRAM2NRAM,
          //            sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
          //            para_ldst_num - 1);

          //   __memcpy(nram_transpose_temp.i +
          //                compute_id * large_radix * (1 + (int)last_stage),
          //            nram_out_i, sizeof(DT) * large_radix, NRAM2NRAM,
          //            sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
          //            para_ldst_num - 1);

          //   // __memcpy(nram_transpose_temp.r +
          //   //              compute_id * large_radix * (1 + (int)last_stage),
          //   //          nram_out_r, large_radix * sizeof(DT), NRAM2NRAM);
          //   // __memcpy(nram_transpose_temp.i +
          //   //              compute_id * large_radix * (1 + (int)last_stage),
          //   //          nram_out_i, large_radix * sizeof(DT), NRAM2NRAM);

          //   // __bang_transpose(nram_transpose_temp.r, nram_transpose_temp.r,
          //   //                  max_para_ldst_num * 2, large_radix);
          //   continue;
          // }

          // DT *sram_tw = (DT *)sram_buffer;
          DT *nram_tw = _nram_tw;
          value_mul = 8;

          for (; small_stage_count > 1; small_stage_count--) {
            FFT_SWAP_PTR(nram_out_r, nram_in_r);
            FFT_SWAP_PTR(nram_out_i, nram_in_i);

            // value_mul = (_small_stage_count - small_stage_count + 1) * 4;
            // // update parameter
            radix = small_factors[value_mul++];
            small_section_num = small_factors[value_mul++];
            small_butterfly_num = small_factors[value_mul++];
            small_in_stride = small_factors[value_mul++];
            // copy GDRAM2SRAM

            if (ld_dft_radix != radix) {
              ld_dft_radix = radix;
              for (int entry = 0;; entry++) {
                if (dft_table[entry].radix == ld_dft_radix) {
                  __memcpy(
                      nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                      sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix, SRAM2NRAM);
                  break;
                }

                if (dft_table[entry].radix == -1) {
                  break;
                }
              }
            }

            if (sec_count == 0 && compute_id == 0 && repeat_id == 1) {
              __memcpy(nram_tw, small_twiddles,
                       small_butterfly_num * (radix - 1) * sizeof(DT) * 2,
                       SRAM2NRAM);
              small_twiddles += small_butterfly_num * (radix - 1) * 2;
            }

            switch (radix) {
              // case 2:
              //   // computeRadix2ButterflyOtherstages(Fout, Fin, section_num,
              //   // section_num, 1, dir);
              //   break;
              // case 3:
              //   computeRadix3ButterflyOtherstages(
              //       nram_out_r, nram_out_i, nram_in_r, nram_in_i,
              //       nram_scratch, nram_tw, small_section_num,
              //       small_butterfly_num, small_in_stride, dir);
              //   break;
              // case 9:
              //   computeRadix9ButterflyOtherstages(
              //       nram_out_r, nram_out_i, nram_in_r, nram_in_i,
              //       nram_scratch, nram_tw, small_section_num,
              //       small_butterfly_num, small_in_stride, dir);
              //   break;
              default:
                // computeGenericButterflyOtherstages(Fout, buffer, twiddles,
                // radix, section_num, butterfly_num, in_stride, 0, dir);
                computeGenericButterflyOtherstagesMat(
                    nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                    nram_dftmtx, nram_tw, small_section_num,
                    small_butterfly_num, para_ldst_num, small_in_stride, dir,
                    radix);
                break;
            }

            nram_tw += small_butterfly_num * (radix - 1) * 2;
          }  // for (stage_count)

          //           for (int j = 0; j < large_radix; j++) {
          //   MLULOG("output i: (%f, %f).\n", nram_out_r[j], nram_out_i[j]);
          // }

          // MLULOG("butterfly id: %d\n", i);
          // last stage
          {
            FFT_SWAP_PTR(nram_out_r, nram_in_r);
            FFT_SWAP_PTR(nram_out_i, nram_in_i);
            // copy GDRAM2SRAM

            // update parameter
            radix = small_factors[value_mul++];
            small_section_num = small_factors[value_mul++];
            small_butterfly_num = small_factors[value_mul++];
            small_in_stride = small_factors[value_mul];

            if (sec_count == 0 && compute_id == 0 && repeat_id == 1) {
              __memcpy(nram_tw, small_twiddles,
                       small_butterfly_num * (radix - 1) * sizeof(DT) * 2,
                       SRAM2NRAM);
            }

            if (ld_dft_radix != radix) {
              ld_dft_radix = radix;
              for (int entry = 0;; entry++) {
                if (dft_table[entry].radix == ld_dft_radix) {
                  __memcpy(
                      nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                      sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix, SRAM2NRAM);
                  break;
                }

                if (dft_table[entry].radix == -1) {
                  break;
                }
              }
            }
            switch (radix) {
              // case 2:
              //   break;
              // case 3:
              //   computeRadix3ButterflyLaststage(
              //       nram_out_r, nram_out_i, nram_in_r, nram_in_i,
              //       nram_scratch, nram_tw, small_section_num,
              //       small_butterfly_num, small_in_stride, dir);
              //   break;
              // case 9:
              //   computeRadix9ButterflyLaststage(
              //       nram_out_r, nram_out_i, nram_in_r, nram_in_i,
              //       nram_scratch, nram_tw, small_section_num,
              //       small_butterfly_num, small_in_stride, dir);
              //   break;
              default:
                // computeGenericButterflyLaststage(Fout, buffer, twiddles,
                // radix, section_num, butterfly_num, in_stride, 0, dir);
                computeGenericButterflyLaststageMat(
                    nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                    nram_dftmtx, nram_tw, small_section_num,
                    small_butterfly_num, para_ldst_num, small_in_stride, dir,
                    radix);
                break;
            }

            // if last-stage: stride = large_radix * 2
            //                compute_id 0 r
            //                compute_id 0 i
            //                compute_id 1 r
            //                compute_id 1 i
            // else: stride = large_radix
            //                compute_id 0 r
            //                compute_id 1 i
            //                compute_id 0 r
            //                compute_id 1 i
            // __memcpy(nram_transpose_temp.r +
            //              compute_id * large_radix * (1 + (int)last_stage),
            //          nram_out_r, large_radix * sizeof(DT), NRAM2NRAM);
            // __memcpy(nram_transpose_temp.i +
            //              compute_id * large_radix * (1 + (int)last_stage),
            //          nram_out_i, large_radix * sizeof(DT), NRAM2NRAM);

            if (last_stage) {
              __memcpy(nram_transpose_temp.r +
                           compute_id * large_radix * (1 + (int)last_stage),
                       nram_out_r, sizeof(DT) * large_radix, NRAM2NRAM,
                       sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
                       para_ldst_num - 1);

              __memcpy(nram_transpose_temp.i +
                           compute_id * large_radix * (1 + (int)last_stage),
                       nram_out_i, sizeof(DT) * large_radix, NRAM2NRAM,
                       sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
                       para_ldst_num - 1);

              __bang_transpose(nram_para_store.r, nram_transpose_temp.r,
                               para_ldst_num * 2, large_radix);
            } else {
              __bang_transpose(nram_para_store.r, nram_out_r, para_ldst_num,
                               large_radix);
              __bang_transpose(nram_para_store.i, nram_out_i, para_ldst_num,
                               large_radix);
            }

            // __bang_transpose(nram_para_store, nram_transpose_temp.r,
            //                  max_para_ldst_num * 2, large_radix);
          }
        }
      }
      // __sync();

      __sync();
    }
    Fin_stride += large_butterfly_num;
    Fout_stride += large_radix * large_butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeLargeButterflyLaststage(
    DT *output, DT *input, const DT *cur_large_twiddles, const DT *_twiddles,
    const DT *dft_matrix, int large_section_num, int large_butterfly_num,
    int large_in_stride, void *nram_buf, const int *small_factors, int nfft,
    int dir) {
  computeLargeButterflyOtherstages(output, input, cur_large_twiddles, _twiddles,
                                   dft_matrix, large_section_num,
                                   large_butterfly_num, large_in_stride,
                                   nram_buf, small_factors, nfft, dir, 1);
}

template <typename DT>
__mlu_func__ void computeLargeButterflyOtherstagesColumn(
    DT *output, DT *input, const DT *cur_large_twiddles, const DT *_twiddles,
    const DT *dft_matrix, int large_section_num, int large_butterfly_num,
    int large_in_stride, void *nram_buf, const int *small_factors, int nfft,
    int dir, int last_stage, int para_batch, int nb0, int nb1) {
  // return;
  const dft_table_entry *dft_table = (const dft_table_entry *)dft_matrix;

  int radix, small_in_stride, small_stage_count, large_radix,
      _small_stage_count;
  int small_section_num, small_butterfly_num, value_mul;

  const int large_out_stride = large_butterfly_num;
  int tw_offset;

  _small_stage_count = small_factors[0];
  large_radix = small_factors[1];
  tw_offset = small_factors[2];

  const DT *small_twiddles = _twiddles + tw_offset * 2;  // complex
  // const DT *small_twiddles;                               // complex

  // MLULOG("small_section_num:      %d.\n\n\n", small_section_num);
  // max num for parallel  load/store
  // const int max_para_ldst_num = 2187 / large_radix;
  // const int max_para_ldst_num = (2187 + large_radix - 1) / large_radix;
  // const int max_para_ldst_num = (512 + large_radix - 1) / large_radix;
  // const int max_para_ldst_num = 20;
  // const int max_para_ldst_num = (4096 + large_radix - 1) / large_radix;

  // int para_ldst_num;
  // TODO(zrg): save nram space.
  // __nram__ DT nram_space[MAX_BUTTERFLY_ON_CHIP * 2];
  // 0 1 2 3 4 5
  // 0   1   3
  // DT *nram_buf_end = (DT*)&((uint8_t*)nram_buf)[NRAM_BUFFER_SIZE];
  // FFT_CPX_T<DT> *nram_in = (FFT_CPX_T<DT> *)nram_buffer;
  // FFT_CPX_T<DT> *nram_out = &nram_in[large_radix];
  // FFT_CPX_T<DT> *nram_buf = &nram_in[large_radix * 2];
  int nram_buf_offset = 0;
  DT *nram_in_r = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch;

  DT *nram_in_i = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch;

  DT *nram_out_r = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch;

  DT *nram_out_i = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch;

  // parallel load/store space
  FFT_CPX_T<DT> nram_para_load_in_ping = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * para_batch};
  nram_buf_offset += large_radix * para_batch * 2;  // complex

  FFT_CPX_T<DT> nram_para_load_in_pong = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * para_batch};
  nram_buf_offset += large_radix * para_batch * 2;  // complex

  // FFT_CPX_T<DT> nram_para_load_tw_ping = {
  //     (DT *)nram_buf + nram_buf_offset,
  //     (DT *)nram_buf + nram_buf_offset + large_radix * para_batch};
  // nram_buf_offset += large_radix * para_batch * 2;  // complex

  // FFT_CPX_T<DT> nram_para_load_tw_pong = {
  //     (DT *)nram_buf + nram_buf_offset,
  //     (DT *)nram_buf + nram_buf_offset + large_radix * para_batch};
  // nram_buf_offset += large_radix * para_batch * 2;  // complex

  FFT_CPX_T<DT> nram_para_load_tw_ping = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + (large_radix - 1)};
  nram_buf_offset += (large_radix - 1) * 2;  // complex

  FFT_CPX_T<DT> nram_para_load_tw_pong = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + (large_radix - 1)};
  nram_buf_offset += (large_radix - 1) * 2;  // complex

  FFT_CPX_T<DT> nram_para_store_ping = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * para_batch};
  nram_buf_offset += large_radix * para_batch * 2;  // complex

  FFT_CPX_T<DT> nram_para_store_pong = {
      (DT *)nram_buf + nram_buf_offset,
      (DT *)nram_buf + nram_buf_offset + large_radix * para_batch};
  nram_buf_offset += large_radix * para_batch * 2;  // complex

  // overlap nram_in
  FFT_CPX_T<DT> nram_transpose_temp;
  // temp out-space before transpose
  // if last-stage:
  //                compute_id 0 r
  //                compute_id 0 i
  //                compute_id 1 r
  //                compute_id 1 i
  // else:
  //                compute_id 0 r
  //                compute_id 1 i
  //                compute_id 0 r
  //                compute_id 1 i
  nram_transpose_temp = {(DT *)nram_in_r,
                         (DT *)nram_in_r + large_radix * ((int)last_stage) +
                             large_radix * (1 - (int)last_stage) * para_batch};
  // nram_buf_offset += large_radix * para_batch * 2;  // complex

  DT *_nram_tw = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * 2;  // complex

  // transpose space: [radix, 2 * parrallel] -> [parrallel * 2, radix]
  // FFT_CPX_T<DT> nram_transpose_load = {
  //     (DT *)nram_buf + nram_buf_offset,
  //     (DT *)nram_buf + nram_buf_offset + large_radix * para_batch};
  // nram_buf_offset += large_radix * para_batch * 2;  // complex

  // load dftmtx sample
  // const int ld_dft_radix = 16;
  // DT *nram_dftmtx = (DT *)nram_buf + nram_buf_offset;
  // nram_buf_offset += ld_dft_radix * ld_dft_radix * 2;  // complex

  // for (int entry = 0;; entry++) {
  //   if (dft_table[entry].radix == ld_dft_radix) {
  //     __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
  //              sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix, GDRAM2NRAM);
  //     break;
  //   }

  //   if (dft_table[entry].radix == -1) {
  //     break;
  //   }
  // }
  int ld_dft_radix = -1;
  const int max_radix = 64;
  DT *nram_dftmtx = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += max_radix * max_radix * 2;  // complex

  DT *nram_scratch = (DT *)nram_buf + nram_buf_offset;

  // temp overlap with "nram_scratch"
  DT *CPX_MUL_RR = nram_scratch;
  DT *CPX_MUL_RI = &CPX_MUL_RR[(large_radix - 1) * para_batch];
  DT *CPX_MUL_IR = &CPX_MUL_RI[(large_radix - 1) * para_batch];
  DT *CPX_MUL_II = &CPX_MUL_IR[(large_radix - 1) * para_batch];

  nram_buf_offset += (large_radix - 1) * para_batch * 4;  // complex

  // size: (large_radix - 1) * para_batch
  // DT *scratch_tw_r = &CPX_MUL_II[large_radix * para_batch];
  // DT *scratch_tw_i = &scratch_tw_r[(large_radix - 1) * para_batch];

  // if (nram_buf == NULL) {
  //   MLULOG("nram_buf: NULL.\n");
  // }
  // if (input == NULL) {
  //   MLULOG("input: NULL.\n");
  // }
  // if (output == NULL) {
  //   MLULOG("output: NULL.\n");
  // }

  // if (cur_large_twiddles == NULL) {
  //   MLULOG("twiddles: NULL.\n");
  // }

  // __nram__ DT *nram_buf = &nram_space[MAX_BUTTERFLY_ON_CHIP*2];

  // DT *odd_extra_buffer = buffer + nfft*2; // for in_place temp buffer

  // const int para_num = 1;

  int Fin_stride = 0, Fout_stride = 0;
  int sec_count;
  int repeat_num = large_butterfly_num;

  for (sec_count = 0; sec_count < large_section_num; ++sec_count) {
    for (int repeat_id = 0; repeat_id < repeat_num + 2; ++repeat_id) {
      // small_twiddles = _small_twiddles;

      // pipeline: load-stage

      if (repeat_id < repeat_num) {
        // MLULOG("pipeline: load-stage.\n");
        int i = repeat_id;
        FFT_CPX_T<DT> nram_para_load_in = (repeat_id % 2 == 0)
                                              ? nram_para_load_in_ping
                                              : nram_para_load_in_pong;

        FFT_CPX_T<DT> nram_para_load_tw = (repeat_id % 2 == 0)
                                              ? nram_para_load_tw_ping
                                              : nram_para_load_tw_pong;

        if (para_batch != 1 || 1) {
          __memcpy_async(
              nram_para_load_in.r, input + (Fin_stride + i) * para_batch,
              sizeof(DT) * para_batch, GDRAM2NRAM, sizeof(DT) * para_batch,
              para_batch * large_in_stride * sizeof(DT), large_radix - 1);
          __memcpy_async(
              nram_para_load_in.i,
              input + para_batch * nfft + (Fin_stride + i) * para_batch,
              sizeof(DT) * para_batch, GDRAM2NRAM, sizeof(DT) * para_batch,
              para_batch * large_in_stride * sizeof(DT), large_radix - 1);

          // __memcpy_async(nram_para_load_tw.r, cur_large_twiddles + i,
          //                sizeof(DT), SRAM2NRAM, sizeof(DT),
          //                large_out_stride * sizeof(DT), large_radix - 2);
          // __memcpy_async(
          //     nram_para_load_tw.i,
          //     cur_large_twiddles + large_butterfly_num * (large_radix - 1) +
          //     i, sizeof(DT), SRAM2NRAM, sizeof(DT), large_out_stride *
          //     sizeof(DT), large_radix - 2);
          __memcpy_async(nram_para_load_tw.r,
                         cur_large_twiddles + i * (large_radix - 1),
                         sizeof(DT) * (large_radix - 1) * 2, SRAM2NRAM);
          __memcpy_async(nram_para_load_tw.i,
                         cur_large_twiddles +
                             large_butterfly_num * (large_radix - 1) +
                             i * (large_radix - 1),
                         sizeof(DT) * (large_radix - 1), SRAM2NRAM);
        }
      }

      // pipeline: store-stage
      if (repeat_id >= 2) {
        // MLULOG("pipeline: store-stage.\n");
        int i = (repeat_id - 2);

        // int para_store_num = (max_para_ldst_num > (large_butterfly_num - i))
        //                          ? (large_butterfly_num - i)
        //                          : max_para_ldst_num;

        FFT_CPX_T<DT> nram_para_store =
            (repeat_id % 2 == 0) ? nram_para_store_ping : nram_para_store_pong;

        if (last_stage) {
          // __memcpy_async(
          //     output + (Fout_stride + i * large_radix) * 2,
          //     nram_para_store.r,
          //     sizeof(DT) * 2 * para_store_num * large_radix, NRAM2GDRAM);

          __memcpy_async(output + (Fout_stride + i) * 2 * nb1,
                         nram_para_store.r, sizeof(DT) * 2 * para_batch,
                         NRAM2GDRAM, nb1 * large_out_stride * 2 * sizeof(DT),
                         sizeof(DT) * 2 * para_batch, large_radix - 1);

        } else {
          // // real
          // __memcpy_async(output + Fout_stride + i * large_radix,
          //                nram_para_store.r,
          //                para_store_num * large_radix * sizeof(DT),
          //                NRAM2GDRAM);
          // // imag
          // __memcpy_async(output + Fout_stride + i * large_radix + nfft,
          //                nram_para_store.i,
          //                para_store_num * large_radix * sizeof(DT),
          //                NRAM2GDRAM);
          // real
          __memcpy_async(output + (Fout_stride + i) * para_batch,
                         nram_para_store.r, para_batch * sizeof(DT), NRAM2GDRAM,
                         para_batch * large_out_stride * sizeof(DT),
                         sizeof(DT) * para_batch, large_radix - 1);
          // imag
          __memcpy_async(
              output + (Fout_stride + i) * para_batch + nfft * para_batch,
              nram_para_store.i, para_batch * sizeof(DT), NRAM2GDRAM,
              para_batch * large_out_stride * sizeof(DT),
              sizeof(DT) * para_batch, large_radix - 1);
        }
      }
      // __sync();
      // pipeline: compute-stage
  
      if (repeat_id >= 1 && repeat_id < repeat_num + 1) {
        // MLULOG("pipeline: compute-stage.\n");
        // int i = (repeat_id - 1);

        FFT_CPX_T<DT> nram_para_load_in = (repeat_id % 2 != 0)
                                              ? nram_para_load_in_ping
                                              : nram_para_load_in_pong;

        FFT_CPX_T<DT> nram_para_load_tw = (repeat_id % 2 != 0)
                                              ? nram_para_load_tw_ping
                                              : nram_para_load_tw_pong;

        FFT_CPX_T<DT> nram_para_store =
            (repeat_id % 2 != 0) ? nram_para_store_ping : nram_para_store_pong;

        // __bang_transpose(nram_transpose_load, nram_para_load, large_radix,
        //                  2 * para_ldst_num);

        // rotation-large
        // __bang_mul(CPX_MUL_RR, nram_para_load_in.r + para_batch,
        //            nram_para_load_tw.r, para_batch * (large_radix - 1));
        // __bang_mul(CPX_MUL_II, nram_para_load_in.i + para_batch,
        //            nram_para_load_tw.i, para_batch * (large_radix - 1));
        // __bang_mul(CPX_MUL_RI, nram_para_load_in.r + para_batch,
        //            nram_para_load_tw.i, para_batch * (large_radix - 1));
        // __bang_mul(CPX_MUL_IR, nram_para_load_in.i + para_batch,
        //            nram_para_load_tw.r, para_batch * (large_radix - 1));

        // __bang_sub(nram_para_load_in.r + para_batch, CPX_MUL_RR, CPX_MUL_II,
        //            para_batch * (large_radix - 1));
        // __bang_add(nram_para_load_in.i + para_batch, CPX_MUL_RI, CPX_MUL_IR,
        //            para_batch * (large_radix - 1));
        if (1) {
          for (int i = 1; i < large_radix; i++) {
            // __memcpy(&Fin.r[nram_in_offset],
            //          &nram_in_r[butterfly_num * section_num * i],
            //          butterfly_num * section_num * sizeof(DT), NRAM2NRAM,
            //          butterfly_num * section_num * sizeof(DT),
            //          butterfly_num * section_num * radix * sizeof(DT),
            //          para_large_butterfly - 1);

            // __memcpy(&Fin.i[nram_in_offset],
            //          &nram_in_i[butterfly_num * section_num * i],
            //          butterfly_num * section_num * sizeof(DT), NRAM2NRAM,
            //          butterfly_num * section_num * sizeof(DT),
            //          butterfly_num * section_num * radix * sizeof(DT),
            //          para_large_butterfly - 1);

            __bang_mul_scalar(&CPX_MUL_RR[(i - 1) * para_batch],
                              nram_para_load_in.r + para_batch * i,
                              nram_para_load_tw.r[(i - 1)], para_batch);
            __bang_mul_scalar(&CPX_MUL_RI[(i - 1) * para_batch],
                              nram_para_load_in.r + para_batch * i,
                              nram_para_load_tw.i[(i - 1)], para_batch);
            __bang_mul_scalar(&CPX_MUL_IR[(i - 1) * para_batch],
                              nram_para_load_in.i + para_batch * i,
                              nram_para_load_tw.r[(i - 1)], para_batch);
            __bang_mul_scalar(&CPX_MUL_II[(i - 1) * para_batch],
                              nram_para_load_in.i + para_batch * i,
                              nram_para_load_tw.i[(i - 1)], para_batch);

            // __bang_cycle_mul(&CPX_MUL_RR[(i - 1) * para_batch],
            // nram_para_load_in.r + para_batch * i,
            //                 &nram_para_load_tw.r[(i - 1) *
            //                 large_butterfly_num], para_batch, 1);
            // __bang_cycle_mul(&CPX_MUL_RI[(i - 1) * para_batch],
            // nram_para_load_in.r + para_batch * i,
            //                 &nram_para_load_tw.i[(i - 1) *
            //                 large_butterfly_num], para_batch, 1);
            // __bang_cycle_mul(&CPX_MUL_IR[(i - 1) * para_batch],
            // nram_para_load_in.i + para_batch * i,
            //                 &nram_para_load_tw.r[(i - 1) *
            //                 large_butterfly_num], para_batch, 1);
            // __bang_cycle_mul(&CPX_MUL_II[(i - 1) * para_batch],
            // nram_para_load_in.i + para_batch * i,
            //                 &nram_para_load_tw.i[(i - 1) *
            //                 large_butterfly_num], para_batch, 1);
          }
          __bang_sub(nram_para_load_in.r + para_batch, CPX_MUL_RR, CPX_MUL_II,
                     para_batch * (large_radix - 1));
          __bang_add(nram_para_load_in.i + para_batch, CPX_MUL_RI, CPX_MUL_IR,
                     para_batch * (large_radix - 1));
        }
        // __bang_transpose(nram_transpose_load.r, nram_para_load_in.r,
        //                  large_radix, para_batch);
        // __bang_transpose(nram_transpose_load.i, nram_para_load_in.i,
        //                  large_radix, para_batch);

        // for (int compute_id = 0; compute_id < para_batch; compute_id++) {
        for (int compute_id = 0; compute_id < para_batch;
             compute_id += para_batch) {
          // load real & imag

          radix = small_factors[4];
          small_section_num = small_factors[5];
          small_in_stride = small_factors[7];
          small_stage_count = _small_stage_count;

          // __memcpy(nram_in_r,
          //         nram_transpose_load + compute_id * large_radix * 2,
          //          large_radix * sizeof(DT) * 2, NRAM2NRAM);

          // first stage
          // if(0)
          if (ld_dft_radix != radix) {
            ld_dft_radix = radix;
            for (int entry = 0;; entry++) {
              if (dft_table[entry].radix == ld_dft_radix) {
                __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                         sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix,
                         SRAM2NRAM);
                break;
              }

              if (dft_table[entry].radix == -1) {
                break;
              }
            }
          }

          switch (radix) {
            default:
              // computeGenericButterflyFirststage(Fout, buffer, twiddles,
              // radix, section_num, butterfly_num, in_stride, 0, dir);
              MLULOG("computeGenericButterflyFirststageMat: %d.\n", radix);

              computeGenericButterflyFirststageMat(
                  nram_out_r, nram_out_i, nram_para_load_in.r,
                  nram_para_load_in.i, nram_scratch, nram_dftmtx,
                  small_section_num * para_batch,
                  small_section_num * para_batch, 1, dir, radix);
              break;
          }
          //           for (int j = 0; j < large_radix; j++) {
          //   MLULOG("output i: (%f, %f).\n", nram_out_r[j], nram_out_i[j]);
          // }

          // [radix, small_section_num, para_ldst_num] ->
          // [small_section_num, para_ldst_num, radix] ->
          // [para_ldst_num, small_section_num, radix] ->
          // [small_section_num, radix, para_ldst_num] ==
          // [large_radix, para_ldst_num]

          small_stage_count--;
          if (small_stage_count == 0) {
            // FFT_SWAP_PTR(nram_out_r, nram_in_r);
            // FFT_SWAP_PTR(nram_out_i, nram_in_i);
            // __bang_transpose(nram_out_r, nram_in_r, para_batch, large_radix);
            // __bang_transpose(nram_out_i, nram_in_i, para_batch, large_radix);

            // nram to gdram

            // if (last_stage) {
            //   //  [2, para_ldst_num, large_radix] -> [para_ldst_num,
            //   large_radix,
            //   //  2]
            //   // DT* nram_transpose_store = nram_in_r;

            //   __bang_transpose(nram_in_r, nram_out_r, 2,
            //                    max_para_ldst_num * large_radix);

            // } else {
            //   //  [2, para_ldst_num, large_radix] -> [2, para_ldst_num,
            //   //  large_radix]
            //   // TODO(zrg): redundant move
            //   __memcpy(nram_in_r, nram_out_r,
            //            para_ldst_num * large_radix * sizeof(DT), NRAM2NRAM);
            //   __memcpy(nram_in_i,
            //            nram_out_i, para_ldst_num * large_radix * sizeof(DT),
            //            NRAM2NRAM);
            // }

            if (last_stage) {
              __memcpy(nram_transpose_temp.r + compute_id * large_radix * 2,
                       nram_out_r, sizeof(DT) * large_radix, NRAM2NRAM,
                       sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
                       para_batch - 1);

              __memcpy(nram_transpose_temp.i + compute_id * large_radix * 2,
                       nram_out_i, sizeof(DT) * large_radix, NRAM2NRAM,
                       sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
                       para_batch - 1);

              __bang_transpose(nram_para_store.r, nram_transpose_temp.r,
                               para_batch * 2, large_radix);
            } else {
              __bang_transpose(nram_para_store.r, nram_out_r, para_batch,
                               large_radix);
              __bang_transpose(nram_para_store.i, nram_out_i, para_batch,
                               large_radix);
            }

            continue;
          }

          FFT_SWAP_PTR(nram_out_r, nram_in_r);
          FFT_SWAP_PTR(nram_out_i, nram_in_i);
          // DT* nram_transpose_store = nram_in_r;

          // for (int para_ldst_id = 0; para_ldst_id < para_ldst_num;
          //      para_ldst_id++) {
          //   __memcpy(nram_out_r + para_ldst_id * small_section_num * radix,
          //            nram_in_r + para_ldst_id * radix, sizeof(DT) * radix,
          //            NRAM2NRAM, sizeof(DT) * radix,
          //            para_ldst_num * radix * sizeof(DT), small_section_num -
          //            1);

          //   __memcpy(nram_out_i + para_ldst_id * small_section_num * radix,
          //            nram_in_i + para_ldst_id * radix, sizeof(DT) * radix,
          //            NRAM2NRAM, sizeof(DT) * radix,
          //            para_ldst_num * radix * sizeof(DT), small_section_num -
          //            1);
          // }

          TRANSPOSE_XYZ2YXZ_PAIR(nram_out_r, nram_out_i, nram_in_r, nram_in_i,
                                 small_section_num, para_batch, radix, DT)

          // TODO(zrg) : add not last-stage
          // if (small_stage_count == 0) {
          //   // if last-stage: stride = large_radix * 2
          //   //                compute_id 0 r
          //   //                compute_id 0 i
          //   //                compute_id 1 r
          //   //                compute_id 1 i
          //   // else: stride = large_radix
          //   //                compute_id 0 r
          //   //                compute_id 1 i
          //   //                compute_id 0 r
          //   //                compute_id 1 i

          //   // [radix, small_section_num, para_ldst_num] ->
          //   // [small_section_num, para_ldst_num, radix] ->
          //   // [para_ldst_num, small_section_num, radix] ->
          //   // [small_section_num, radix, para_ldst_num] ==
          //   // [large_radix, para_ldst_num]

          //   __memcpy(nram_transpose_temp.r +
          //                compute_id * large_radix * (1 + (int)last_stage),
          //            nram_out_r, sizeof(DT) * large_radix, NRAM2NRAM,
          //            sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
          //            para_ldst_num - 1);

          //   __memcpy(nram_transpose_temp.i +
          //                compute_id * large_radix * (1 + (int)last_stage),
          //            nram_out_i, sizeof(DT) * large_radix, NRAM2NRAM,
          //            sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
          //            para_ldst_num - 1);

          //   // __memcpy(nram_transpose_temp.r +
          //   //              compute_id * large_radix * (1 + (int)last_stage),
          //   //          nram_out_r, large_radix * sizeof(DT), NRAM2NRAM);
          //   // __memcpy(nram_transpose_temp.i +
          //   //              compute_id * large_radix * (1 + (int)last_stage),
          //   //          nram_out_i, large_radix * sizeof(DT), NRAM2NRAM);

          //   // __bang_transpose(nram_transpose_temp.r, nram_transpose_temp.r,
          //   //                  max_para_ldst_num * 2, large_radix);
          //   continue;
          // }

          // DT *sram_tw = (DT *)sram_buffer;
          DT *nram_tw = _nram_tw;
          value_mul = 8;

          for (; small_stage_count > 1; small_stage_count--) {
            FFT_SWAP_PTR(nram_out_r, nram_in_r);
            FFT_SWAP_PTR(nram_out_i, nram_in_i);

            // value_mul = (_small_stage_count - small_stage_count + 1) * 4;
            // // update parameter
            radix = small_factors[value_mul++];
            small_section_num = small_factors[value_mul++];
            small_butterfly_num = small_factors[value_mul++];
            small_in_stride = small_factors[value_mul++];
            // copy GDRAM2SRAM

            if (ld_dft_radix != radix) {
              ld_dft_radix = radix;
              for (int entry = 0;; entry++) {
                if (dft_table[entry].radix == ld_dft_radix) {
                  __memcpy(
                      nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                      sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix, SRAM2NRAM);
                  break;
                }

                if (dft_table[entry].radix == -1) {
                  break;
                }
              }
            }

            if (sec_count == 0 && compute_id == 0 && repeat_id == 1) {
              __memcpy(nram_tw, small_twiddles,
                       small_butterfly_num * (radix - 1) * sizeof(DT) * 2,
                       SRAM2NRAM);
              small_twiddles += small_butterfly_num * (radix - 1) * 2;
            }

            switch (radix) {
              // case 2:
              //   // computeRadix2ButterflyOtherstages(Fout, Fin, section_num,
              //   // section_num, 1, dir);
              //   break;
              // case 3:
              //   computeRadix3ButterflyOtherstages(
              //       nram_out_r, nram_out_i, nram_in_r, nram_in_i,
              //       nram_scratch, nram_tw, small_section_num,
              //       small_butterfly_num, small_in_stride, dir);
              //   break;
              // case 9:
              //   computeRadix9ButterflyOtherstages(
              //       nram_out_r, nram_out_i, nram_in_r, nram_in_i,
              //       nram_scratch, nram_tw, small_section_num,
              //       small_butterfly_num, small_in_stride, dir);
              //   break;
              default:
                // computeGenericButterflyOtherstages(Fout, buffer, twiddles,
                // radix, section_num, butterfly_num, in_stride, 0, dir);
                computeGenericButterflyOtherstagesMat(
                    nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                    nram_dftmtx, nram_tw, small_section_num,
                    small_butterfly_num, para_batch, small_in_stride, dir,
                    radix);
                break;
            }

            nram_tw += small_butterfly_num * (radix - 1) * 2;
          }  // for (stage_count)

          //           for (int j = 0; j < large_radix; j++) {
          //   MLULOG("output i: (%f, %f).\n", nram_out_r[j], nram_out_i[j]);
          // }

          // MLULOG("butterfly id: %d\n", i);
          // last stage
          {
            FFT_SWAP_PTR(nram_out_r, nram_in_r);
            FFT_SWAP_PTR(nram_out_i, nram_in_i);
            // copy GDRAM2SRAM

            // update parameter
            radix = small_factors[value_mul++];
            small_section_num = small_factors[value_mul++];
            small_butterfly_num = small_factors[value_mul++];
            small_in_stride = small_factors[value_mul];

            if (sec_count == 0 && compute_id == 0 && repeat_id == 1) {
              __memcpy(nram_tw, small_twiddles,
                       small_butterfly_num * (radix - 1) * sizeof(DT) * 2,
                       SRAM2NRAM);
            }

            if (ld_dft_radix != radix) {
              ld_dft_radix = radix;
              for (int entry = 0;; entry++) {
                if (dft_table[entry].radix == ld_dft_radix) {
                  __memcpy(
                      nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                      sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix, SRAM2NRAM);
                  break;
                }

                if (dft_table[entry].radix == -1) {
                  break;
                }
              }
            }
            switch (radix) {
              // case 2:
              //   break;
              // case 3:
              //   computeRadix3ButterflyLaststage(
              //       nram_out_r, nram_out_i, nram_in_r, nram_in_i,
              //       nram_scratch, nram_tw, small_section_num,
              //       small_butterfly_num, small_in_stride, dir);
              //   break;
              // case 9:
              //   computeRadix9ButterflyLaststage(
              //       nram_out_r, nram_out_i, nram_in_r, nram_in_i,
              //       nram_scratch, nram_tw, small_section_num,
              //       small_butterfly_num, small_in_stride, dir);
              //   break;
              default:
                // computeGenericButterflyLaststage(Fout, buffer, twiddles,
                // radix, section_num, butterfly_num, in_stride, 0, dir);
                computeGenericButterflyLaststageMat(
                    nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                    nram_dftmtx, nram_tw, small_section_num,
                    small_butterfly_num, para_batch, small_in_stride, dir,
                    radix);
                break;
            }

            if (last_stage) {
              // [2, para_batch, large_radix] -> [large_radix, para_batch, 2]
              __memcpy(nram_transpose_temp.r + compute_id * large_radix * 2,
                       nram_out_r, sizeof(DT) * large_radix, NRAM2NRAM,
                       sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
                       para_batch - 1);

              __memcpy(nram_transpose_temp.i + compute_id * large_radix * 2,
                       nram_out_i, sizeof(DT) * large_radix, NRAM2NRAM,
                       sizeof(DT) * large_radix * 2, sizeof(DT) * large_radix,
                       para_batch - 1);

              __bang_transpose(nram_para_store.r, nram_transpose_temp.r,
                               para_batch * 2, large_radix);
            } else {
              __bang_transpose(nram_para_store.r, nram_out_r, para_batch,
                               large_radix);
              __bang_transpose(nram_para_store.i, nram_out_i, para_batch,
                               large_radix);
            }

            // __bang_transpose(nram_para_store, nram_transpose_temp.r,
            //                  max_para_ldst_num * 2, large_radix);
          }
        }
      }

      __sync();
    }
    Fin_stride += large_butterfly_num;
    Fout_stride += large_radix * large_butterfly_num;
  }
}

template <typename DT>
__mlu_func__ void computeLargeButterflyLaststageColumn(
    DT *output, DT *input, const DT *cur_large_twiddles, const DT *_twiddles,
    const DT *dft_matrix, int large_section_num, int large_butterfly_num,
    int large_in_stride, void *nram_buf, const int *small_factors, int nfft,
    int dir, int para_batch, int nb0, int nb1) {
  computeLargeButterflyOtherstagesColumn(
      output, input, cur_large_twiddles, _twiddles, dft_matrix,
      large_section_num, large_butterfly_num, large_in_stride, nram_buf,
      small_factors, nfft, dir, 1, para_batch, nb0, nb1);
}

template <typename DT>
__mlu_func__ void computeMutiStageOnchip(DT *input, DT *output, int *factors,
                                         DT *twiddles, const DT *twiddles_end,
                                         const DT *dft_matrix, DT *buffer,
                                         int batch, int fft_flag,
                                         int direction) {
  int total_num = batch;
  int repeat_num = total_num / taskDim;
  int remain_num = total_num % taskDim;
  // MLULOG("\n\n\ntaskId: %d, taskDim: %d, coreId: %d\n\n\n", taskId, taskDim,
  //        coreId);

  // if (taskId % 4 == 0) {
  //   MLULOG("\n\n\n__is_mpu\n\n\n");
  // }

  // return;
  // __sync_cluster();
  // __nram__ uint8_t *nram_buf[NRAM_BUFFER_SIZE];
  char *nram_buf = nram_buffer;

  // Each core needs to process "t_len" blocks, "remain_num" is evenly
  // assigned to the previous "remian_num" cores.
  int t_len = repeat_num + ((remain_num > 0 && taskId < remain_num) ? 1 : 0);
  // Calculate the offset of the block at each core.
  int t_start = taskId - remain_num <= 0 ? taskId * (repeat_num + 1)
                                         : (remain_num * (repeat_num + 1) +
                                            (taskId - remain_num) * repeat_num);
  int t_end = (t_start + t_len);

  MLULOG(
      "taskId: %d, repeat_num: %d, "
      "remain_num: %d, t_len: %d, t_start: %d, t_end: %d\n",
      taskId, repeat_num, remain_num, t_len, t_start, t_end);

  int radix, section_num, butterfly_num, in_stride, stage_count, value_mul,
      small_factors_offset;
  // const int is_two_stages = (stage_count == 2);  // the variable for
  // twostages
  // const int is_in_place = (input == output);

  int *small_factors;
  int last_stage;
  // __sync_io();
  __nram__ int nram_factors[FFT_MAXFACTORS];

  int sram_offset = 0;
  int *sram_factors = (int *)(sram_buffer + sram_offset);
  sram_offset += FFT_MAXFACTORS * sizeof(int);
  DT *sram_dftmtx = (DT *)(sram_buffer + sram_offset);
  sram_offset += DFT_TABLE_SIZE * sizeof(DT);
  DT *sram_twiddles = (DT *)(sram_buffer + sram_offset);
  const int twiddles_size = twiddles_end - twiddles;
  sram_offset += twiddles_size * sizeof(DT);

  // __mlu_shared__ char sram_twiddles[MAX_SRAM_SIZE - FFT_MAXFACTORS -
  // DFT_TABLE_SIZE];  // radix-1024
  //   __mlu_shared__ int sram_factors[FFT_MAXFACTORS];
  //   __mlu_shared__ DT sram_dftmtx[DFT_TABLE_SIZE];

  const int _stage_count = factors[0];
  const int nfft = factors[1];
  if (__is_ipu()) MLULOG("nfft: %d\n", nfft);

  // first stage
  radix = factors[5 + 0];
  section_num = factors[5 + 1];
  in_stride = factors[5 + 3];
  small_factors_offset = factors[5 + 4];

  small_factors = factors + small_factors_offset;

  stage_count = _stage_count;
  last_stage = (stage_count == 1);

  if (__is_mpu()) {
    __memcpy_async(sram_factors, factors, FFT_MAXFACTORS * sizeof(int),
                   GDRAM2SRAM);
    __memcpy_async(sram_twiddles, twiddles, twiddles_size * sizeof(DT),
                   GDRAM2SRAM);
    // _small_stage_count = small_factors[0];

    const dft_table_entry *dft_table_gdram =
        (const dft_table_entry *)dft_matrix;
    int dft_matrix_offset = dft_table_gdram[0].offset;

    if (dft_matrix_offset != -1) {
      // copy the table
      __memcpy(sram_dftmtx, dft_matrix, sizeof(DT) * 2 * dft_matrix_offset,
               GDRAM2SRAM);
      const dft_table_entry *dft_table = (const dft_table_entry *)sram_dftmtx;

      for (int entry = 0;; entry++) {
        if (dft_table[entry + 1].radix == -1) {
          int last_radix = dft_table[entry].radix;
          int last_offset = dft_table[entry].offset;
          __memcpy_async(
              sram_dftmtx, dft_matrix,
              sizeof(DT) * 2 * (last_radix * last_radix + last_offset),
              GDRAM2SRAM);
          break;
        }
      }
    }
    factors = sram_factors;
  }


  __sync_cluster();
  if (__is_ipu()) {
    __memcpy(nram_factors, sram_factors, FFT_MAXFACTORS * sizeof(int),
             SRAM2NRAM);
    factors = nram_factors;
    // __memcpy(nram_factors, sram_factors, FFT_MAXFACTORS * sizeof(int),
    //          SRAM2NRAM);
    // factors = sram_factors;
    twiddles = sram_twiddles;
  }

  if (__is_mpu()) {
    return;
  }

  DT *_twiddles = twiddles;
  DT *odd_extra_buffer;
  if (__is_ipu()) {
    // FFT_CPX_T<DT> *odd_extra_buffer = (FFT_CPX_T<DT> *)buffer + batch * nfft;
    // // for in_place temp buffer
    odd_extra_buffer =
        buffer + batch * (nfft << 1);  // for in_place temp buffer
    // out_place: input -> output (1 stage)
    //            input -> buffer -> output (2 stage)
    //            input -> buffer -> odd_extra_buffer -> output (3 stage)
    //            input -> buffer -> output -> buffer -> output (4 stage)
    //            input -> buffer -> output -> buffer -> odd_extra_buffer ->
    //            output (5 stage)

    // _stage_count = stage_count;

    if (_stage_count != 1) FFT_SWAP_PTR(buffer, output);

    if (repeat_num > 0 || taskId < remain_num) {
      for (int t = t_start; t < t_end; t++) {
        // MLULOG("taskId: %d, batchId: %d\n", taskId, t);
        DT *input_batch = input + t * (nfft << 1);
        DT *output_batch = output + t * (nfft << 1);

        // DT *buffer_batch = buffer + t * (nfft * 2);
        // DT *odd_extra_buffer_batch = odd_extra_buffer + t * (nfft * 2);

        // first stage

        computeLargeButterflyFirststage<DT>(
            output_batch, input_batch, in_stride, section_num, twiddles,
            sram_dftmtx, (void *)nram_buf, small_factors, direction, nfft,
            last_stage);
      }
    }
    // __sync();
  } else {
    stage_count = _stage_count;
    last_stage = (stage_count == 1);
  }
  // return;

  // sram_large_tw
  stage_count--;
  if (stage_count == 0) {
    // continue;
    return;
  }

  // if (__is_mpu()) {
  //   return;
  // }

  // sram_large_tw
  value_mul = 10;
  for (; stage_count > 1; stage_count--) {
    // fft_swap_ptr<DT>(&buffer, &output);
    // FFT_SWAP_PTR(buffer, output);
    FFT_SWAP_PTR(buffer, output);

    // if (is_two_stages && is_in_place) {
    //   if (_stage_count % 2 == 0) {
    //     if ((_stage_count - stage_count) == 2)
    //       fft_swap_ptr<DT>(&odd_extra_buffer, &output);
    //   } else {
    //     if ((_stage_count - stage_count) == 3)
    //       fft_swap_ptr<DT>(&odd_extra_buffer, &output);
    //   }
    // }

    if (stage_count == 2 && _stage_count % 2) {
      // fft_swap_ptr<DT>(&odd_extra_buffer, &output);
      FFT_SWAP_PTR(odd_extra_buffer, output);
    }

    // value_mul = (_stage_count - stage_count + 1) * 5;

    // update parameter
    radix = factors[value_mul++];
    section_num = factors[value_mul++];
    butterfly_num = factors[value_mul++];
    in_stride = factors[value_mul++];
    small_factors_offset = factors[value_mul++];

    small_factors = factors + small_factors_offset;

    if (__is_ipu()) {
      // MLULOG("other stage radix: %d \n", radix);
      if (repeat_num > 0 || taskId < remain_num) {
        for (int t = t_start; t < t_end; t++) {
          DT *output_batch = output + t * (nfft << 1);
          DT *buffer_batch = buffer + t * (nfft << 1);

          computeLargeButterflyOtherstages<DT>(
              output_batch, buffer_batch, (DT *)twiddles, _twiddles,
              sram_dftmtx, section_num, butterfly_num, in_stride,
              (void *)nram_buf, small_factors, nfft, direction, 0);

          // __sync();
        }
      }
    }
    twiddles += butterfly_num * (radix - 1) * 2;  // 2 for complex
  }                                               // for (stage_count)

  // __mlu_shared__ DT *sram_tw[2048];  // radix-1024
  // __mlu_shared__ DT *sram_tw[64];  // radix-1024
  // last stage
  {
    if ((_stage_count % 2 == 1)) {
      FFT_SWAP_PTR(odd_extra_buffer, buffer);
    }

    // fft_swap_ptr<DT>(&buffer, &output);
    FFT_SWAP_PTR(buffer, output);

    // update parameter
    radix = factors[value_mul++];
    section_num = factors[value_mul++];
    butterfly_num = factors[value_mul++];
    in_stride = factors[value_mul++];
    small_factors_offset = factors[value_mul];

    small_factors = factors + small_factors_offset;

    // DT * sram_la = sram_large_tw

    // if (__is_mpu()) {
    //   __memcpy(sram_tw, twiddles, sizeof(DT) * 2 * butterfly_num * (radix -
    //   1),
    //            GDRAM2SRAM);
    // }

    // __sync_cluster();
    // // imag
    // __memcpy(
    //     sram_large_tw + butterfly_num * (radix - 1),
    //     twiddles + + butterfly_num * (radix - 1),
    //     sizeof(DT) * para_ldst_num, GDRAM2NRAM,
    //     sizeof(DT) * para_ldst_num, large_out_stride * sizeof(DT),
    //     large_radix - 2);

    if (__is_ipu()) {
      MLULOG("last stage radix: %d, section_num: %d\n", radix, section_num);

      if (repeat_num > 0 || taskId < remain_num) {
        for (int t = t_start; t < t_end; t++) {
          DT *output_batch = output + t * (nfft << 1);
          DT *buffer_batch = buffer + t * (nfft << 1);

          computeLargeButterflyLaststage<DT>(
              output_batch, buffer_batch, (DT *)twiddles, _twiddles,
              sram_dftmtx, section_num, butterfly_num, in_stride,
              (void *)nram_buf, small_factors, nfft, direction);
        }
      }
    }
  }
}

__mlu_global__ void MLUKernelFFTButterfly(void *input, void *output,
                                          int *factors, void *twiddles,
                                          void *twiddles_end, void *dft_matrix,
                                          void *buffer, int batch, int fft_flag,
                                          int direction, int dtype_size) {
  // if (__is_mpu()) return;
  // if (coreId == 0x80) {
  //     MLULOG("\n\n\nmemcore\n\n");
  // }
  // if (taskId == 0) {
  //     MLULOG("\n\n\nmemcore\n\n");
  // }

  // MLULOG("batch: %d\n", batch);
  switch (dtype_size) {
    case (MLUOP_DTYPE_COMPLEX_FLOAT):
    case (MLUOP_DTYPE_FLOAT): {
      // MLULOG("MLUOP_DTYPE_COMPLEX_FLOAT: MLUOP_DTYPE_FLOAT\n");
      computeMutiStageOnchip<float>((float *)input, (float *)output, factors,
                                    (float *)twiddles, (float *)twiddles_end,
                                    (float *)dft_matrix, (float *)buffer, batch,
                                    fft_flag, direction);
    }; break;
    case (MLUOP_DTYPE_COMPLEX_HALF):
    case (MLUOP_DTYPE_HALF): {
      // MLULOG("MLUOP_DTYPE_COMPLEX_HALF: MLUOP_DTYPE_HALF\n");
      computeMutiStageOnchip<half>((half *)input, (half *)output, factors,
                                   (half *)twiddles, (half *)twiddles_end,
                                   (half *)dft_matrix, (half *)buffer, batch,
                                   fft_flag, direction);
    }; break;

    default: {
      MLULOG("mluOpFFT Not Implemented.");
    }
  }
}

mluOpStatus_t MLUOP_WIN_API kernelFFTButterfly(cnrtDim3_t k_dim,
                                               cnrtFunctionType_t k_type,
                                               cnrtQueue_t queue,
                                               mluOpFFTPlan_t fft_plan,
                                               int direction, FFTFlag flag) {
  VLOG(5) << "Launch Kernel kernelFFTButterfly <<Union" << k_type / CORE_DIM
          << ", " << k_dim.x << ", " << k_dim.y << ", " << k_dim.z << ">>>";
  KERNEL_CHECK((MLUKernelFFTButterfly<<<k_dim, k_type, queue>>>(
      fft_plan->mlu_addrs.input, fft_plan->mlu_addrs.output,
      fft_plan->mlu_addrs.factors, fft_plan->mlu_addrs.twiddles,
      fft_plan->mlu_addrs.twiddles_end, fft_plan->mlu_addrs.dft_matrix,
      fft_plan->mlu_addrs.buffer_buf, fft_plan->batch, flag,
      direction,  // direction, -1 means invalid(only FFT_IFFT use).
      fft_plan->output_dtype)));
  return MLUOP_STATUS_SUCCESS;
}

mluOpStatus_t MLUOP_WIN_API kernelFFTButterfly2d(cnrtDim3_t k_dim,
                                                 cnrtFunctionType_t k_type,
                                                 cnrtQueue_t queue,
                                                 mluOpFFTPlan_t fft_plan,
                                                 int direction, FFTFlag flag) {
  VLOG(5) << "Launch Kernel kernelFFTButterfly <<Union" << k_type / CORE_DIM
          << ", " << k_dim.x << ", " << k_dim.y << ", " << k_dim.z << ">>>";
  KERNEL_CHECK((MLUKernelFFTButterfly<<<k_dim, k_type, queue>>>(
      fft_plan->mlu_addrs.input, fft_plan->mlu_addrs.input,
      fft_plan->mlu_addrs.factors_2d, fft_plan->mlu_addrs.twiddles_2d,
      fft_plan->mlu_addrs.twiddles_2d_end, fft_plan->mlu_addrs.dft_matrix_2d,
      fft_plan->mlu_addrs.buffer_buf, fft_plan->batch_2d, flag,
      direction,  // direction, -1 means invalid(only FFT_IFFT use).
      fft_plan->output_dtype)));

  return MLUOP_STATUS_SUCCESS;
}

template <typename DT>
__mlu_func__ void computeLargeButterflyFirststageColumn(
    DT *output, DT *input, int large_in_stride, int section_num,
    const DT *twiddles, const DT *dft_matrix, void *nram_buf,
    const int *small_factors, int dir, int nfft, int last_stage,
    const int para_batch, const int nb0, const int nb1) {
  // constant
  // const int para_batch = 3;
  const dft_table_entry *dft_table = (const dft_table_entry *)dft_matrix;
  // test
  // for(int i =0; i<3; i++){
  //   MLULOG("entry: %d, dft_table.radix: %d, dft_table.offset: %d.\n",
  //    i, dft_table[i].radix, dft_table[i].offset);
  // }
  // network info
  int radix, small_in_stride, small_stage_count, large_radix,
      _small_stage_count;
  int small_section_num, small_butterfly_num, value_mul;
  int tw_offset;
  // int max_radix = small_factors[4];
  _small_stage_count = small_factors[0];
  large_radix = small_factors[1];
  tw_offset = small_factors[2];

  // for (int i=2; i<= _small_stage_count; i++) {

  //   max_radix = max(small_factors[i*4], max_radix);

  // }

  // load compute store
  // (0)                              load 0 ping sync()
  // (1)              compute 0 ping  load 1 pong sync()
  // (2) store 0      compute 1 pong  load 2 ping sync()
  // (3) store 1      compute 2   load 3  sync()

  // compute last-large-stage (nram_out_r,nram_out_i) [2, large_radix]->
  // transpose -> [large_radix, 2]

  // complex array -> real array, imag array -> complex array
  // first-large-stage complex -> real array, imag array
  // other-large-stage none
  // last-large-stage real array, imag array -> complex
  const DT *small_twiddles = twiddles + tw_offset * 2;  // complex

  // TODO(zrg): save nram space.
  // assign nram space
  int nram_buf_offset = 0;
  DT *nram_in_r = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch;

  DT *nram_in_i = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch;

  DT *nram_out_r = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch;

  DT *nram_out_i = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch;

  // parallel load/store space
  // sizeof(DT) * 2 * large_radix * para_batch * 4
  DT *nram_para_load_ping = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch * 2;  // complex

  DT *nram_para_load_pong = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch * 2;  // complex

  DT *nram_para_store_ping = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch * 2;  // complex

  DT *nram_para_store_pong = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * para_batch * 2;  // complex

  // transpose space: [radix, 2 * parrallel] -> [parrallel * 2, radix]
  // DT *nram_transpose_load = (DT *)nram_buf + nram_buf_offset;
  // nram_buf_offset += large_radix * para_batch * 2;  // complex

  // FFT_CPX_T<DT> nram_transpose_temp;
  // temp out-space before transpose
  // if last-stage:
  //                compute_id 0 r
  //                compute_id 0 i
  //                compute_id 1 r
  //                compute_id 1 i
  // else:
  //                compute_id 0 r
  //                compute_id 1 i
  //                compute_id 0 r
  //                compute_id 1 i

  DT *_nram_tw = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += large_radix * 2;  // complex

  // load dftmtx sample
  int ld_dft_radix = -1;
  const int max_radix = 64;
  DT *nram_dftmtx = (DT *)nram_buf + nram_buf_offset;
  nram_buf_offset += max_radix * max_radix * 2;  // complex

  // const int ld_dft_radix = 16;
  // DT *nram_dftmtx8 = (DT *)nram_buf + nram_buf_offset;
  // nram_buf_offset += 8 * 8 * 2;  // complex

  // for (int entry = 0;; entry++) {
  //   if (dft_table[entry].radix == 8) {
  //     __memcpy_async(nram_dftmtx8, &dft_matrix[dft_table[entry].offset * 2],
  //                    sizeof(DT) * 2 * 8 * 8, GDRAM2NRAM);
  //     break;
  //   }
  //   if (dft_table[entry].radix == -1) {
  //     break;
  //   }
  // }

  // nram space used:
  // sizeof(DT) * 2 * large_radix * (para_batch * 6 + 1) + sizeof(DT) * 2
  // * (max_radix * max_radix)
  // + sizeof(DT) * 2 * large_radix * para_batch * 4
  DT *nram_scratch = (DT *)nram_buf + nram_buf_offset;

  // DT *nram_transpose_temp = nram_scratch;
  // overlap nram_scratch
  // nram_transpose_temp = {
  //     (DT *)nram_scratch,
  //     (DT *)nram_scratch + large_radix * ((int)last_stage) +
  //         large_radix * (1 - (int)last_stage) * para_batch};
  // nram_buf_offset += large_radix * para_batch * 2;  // complex

  __memcpy_async(_nram_tw, small_twiddles, large_radix * sizeof(DT) * 2,
                 SRAM2NRAM);

  // return;
  // ceil
  int repeat_num = section_num;
  // MLULOG("repeat_num column: %d\n", repeat_num);
  for (int repeat_id = 0; repeat_id < repeat_num + 2; ++repeat_id) {
    // pipeline: load-stage

    if (repeat_id < repeat_num) {
      // MLULOG("pipeline: load-stage.\n");
      int i = repeat_id;
      DT *nram_para_load =
          (repeat_id % 2 == 0) ? nram_para_load_ping : nram_para_load_pong;

      // DT *nram_dftmtx =
      //     (repeat_id % 2 == 0) ? nram_dftmtx_ping : nram_dftmtx_pong;

      //   if (section_num == 1) {
      //     __memcpy_async(nram_para_load, input, sizeof(DT) * 2 * large_radix,
      //                    GDRAM2NRAM);
      //   } else {
      //     // gather load
      //     // 2d memcpy
      //     // 0 1 2 3 4 ... 1023
      //     // GDRAM -> NRAM
      //     // 8bytes radix-1024
      //     // 64bytes

      //     __memcpy_async(nram_para_load, input + i * 2,
      //                    sizeof(DT) * 2 * para_batch, GDRAM2NRAM,
      //                    sizeof(DT) * 2 * para_batch,
      //                    large_in_stride * sizeof(DT) * 2, large_radix - 1);
      //   }

      // if(0)
      // __memcpy_async(nram_para_load, input, sizeof(DT) * 2 * large_radix *
      // para_batch,
      //                GDRAM2NRAM);
      //                 if(0)
      __memcpy_async(nram_para_load, input + i * 2 * nb1,
                     sizeof(DT) * 2 * para_batch, GDRAM2NRAM,
                     sizeof(DT) * 2 * para_batch,
                     nb1 * large_in_stride * sizeof(DT) * 2, large_radix - 1);
    }

    // pipeline: store-stage

    if (repeat_id >= 2) {
      // MLULOG("pipeline: store-stage.\n");
      int i = (repeat_id - 2);

      DT *nram_para_store =
          (repeat_id % 2 == 0) ? nram_para_store_ping : nram_para_store_pong;

      if (last_stage) {
        // if(0)
        __memcpy_async(output + i * large_radix * 2 * nb1, nram_para_store,
                       para_batch * sizeof(DT) * 2, NRAM2GDRAM,
                       nb1 * 2 * sizeof(DT), para_batch * sizeof(DT) * 2,
                       large_radix - 1);
      } else {
        // // real
        // __memcpy_async(output + i * large_radix * nb1, nram_para_store,
        //                para_batch * sizeof(DT), NRAM2GDRAM, nb1 * sizeof(DT),
        //                para_batch * sizeof(DT), large_radix - 1);
        // // imag
        // __memcpy_async(output + i * large_radix * nb1 + nb0 * nb1,
        //                nram_para_store + para_batch * large_radix,
        //                para_batch * sizeof(DT), NRAM2GDRAM, nb1 * sizeof(DT),
        //                para_batch * sizeof(DT), large_radix - 1);
        // real
        __memcpy_async(output + i * large_radix * para_batch, nram_para_store,
                       para_batch * large_radix * sizeof(DT), NRAM2GDRAM);
        // imag
        __memcpy_async(
            output + i * large_radix * para_batch + nfft * para_batch,
            nram_para_store + para_batch * large_radix,
            para_batch * large_radix * sizeof(DT), NRAM2GDRAM);
      }
    }

    // pipeline: compute-stage

    if (repeat_id >= 1 && repeat_id < repeat_num + 1) {
      //   int i = (repeat_id - 1);

      DT *nram_para_load =
          (repeat_id % 2 != 0) ? nram_para_load_ping : nram_para_load_pong;
      DT *nram_para_store =
          (repeat_id % 2 != 0) ? nram_para_store_ping : nram_para_store_pong;

      // // [large_radix, para_batch, 2] -> [para_batch, 2, large_radix]
      // __bang_transpose(nram_transpose_load, nram_para_load, large_radix,
      //                  2 * para_batch);

      // [large_radix, para_batch, 2] -> [2, para_batch, large_radix]
      // overlap nram_out_r
      // DT *nram_transpose_load = nram_out_r;
      // __bang_transpose(nram_transpose_load, nram_para_load,
      //                  large_radix * para_batch, 2);
      // // [large_radix, para_batch] -> [para_batch, large_radix]
      // __bang_transpose(nram_in_r, nram_transpose_load, large_radix,
      //                  para_batch);
      // __bang_transpose(nram_in_i,
      //                  nram_transpose_load + large_radix * para_batch,
      //                  large_radix, para_batch);

      // DT *nram_transpose_load = nram_in_r;
      __bang_transpose(nram_in_r, nram_para_load, large_radix * para_batch, 2);
      // [large_radix, para_batch] -> [para_batch, large_radix]
      // __bang_transpose(nram_in_r, nram_transpose_load, large_radix,
      //                  para_batch);
      // __bang_transpose(nram_in_i,
      //                  nram_transpose_load + large_radix * para_batch,
      //                  large_radix, para_batch);

      for (int compute_id = 0; compute_id < para_batch;
           compute_id += para_batch) {
        // load real & imag

        radix = small_factors[4];
        small_section_num = small_factors[5];
        small_in_stride = small_factors[7];
        small_stage_count = _small_stage_count;

        // __memcpy(nram_in_r,
        //         nram_transpose_load + compute_id * large_radix * 2,
        //          large_radix * sizeof(DT) * 2, NRAM2NRAM);

        // first stage

        if (ld_dft_radix != radix) {
          ld_dft_radix = radix;
          for (int entry = 0;; entry++) {
            if (dft_table[entry].radix == ld_dft_radix) {
              __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                       sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix, SRAM2NRAM);
              break;
            }

            if (dft_table[entry].radix == -1) {
              break;
            }
          }
        }

        switch (radix) {
          default:
            MLULOG("computeGenericButterflyFirststageMat: %d.\n", radix);
            computeGenericButterflyFirststageMat(
                nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                nram_dftmtx, small_section_num * para_batch,
                small_section_num * para_batch, 1, dir, radix);
            break;
        }
        //           for (int j = 0; j < large_radix; j++) {
        //   MLULOG("output i: (%f, %f).\n", nram_out_r[j], nram_out_i[j]);
        // }

        // [radix, small_section_num, para_batch] ->
        // [small_section_num, para_batch, radix] -> [para_batch,
        // small_section_num, radix]

        // __memcpy(nram_out_r + para_ldst_id * small_section_num * radix,
        //          nram_in_r + para_ldst_id * radix, sizeof(DT) * radix,
        //          NRAM2NRAM, sizeof(DT) * radix,
        //          para_batch * radix * sizeof(DT), small_section_num - 1);

        // __memcpy(nram_out_i + para_ldst_id * small_section_num * radix,
        //          nram_in_i + para_ldst_id * radix, sizeof(DT) * radix,
        //          NRAM2NRAM, sizeof(DT) * radix,
        //          para_batch * radix * sizeof(DT), small_section_num - 1);
        // __sync_move();

        small_stage_count--;
        if (small_stage_count == 0) {
          // nram to gdram
          FFT_SWAP_PTR(nram_out_r, nram_in_r);
          FFT_SWAP_PTR(nram_out_i, nram_in_i);
          __bang_transpose(nram_out_r, nram_in_r, para_batch, large_radix);
          __bang_transpose(nram_out_i, nram_in_i, para_batch, large_radix);

          if (last_stage) {
            //  [2, para_batch, large_radix] -> [para_batch, large_radix,
            //  2]
            // DT* nram_transpose_store = nram_in_r;

            __bang_transpose(nram_para_store, nram_out_r, 2,
                             para_batch * large_radix);

          } else {
            //  [2, para_batch, large_radix] -> [2, para_batch,
            //  large_radix]
            // TODO(zrg): redundant move
            __memcpy(nram_para_store, nram_out_r,
                     para_batch * large_radix * sizeof(DT), NRAM2NRAM);
            __memcpy(nram_para_store + para_batch * large_radix, nram_out_i,
                     para_batch * large_radix * sizeof(DT), NRAM2NRAM);
          }

          continue;
        }

        FFT_SWAP_PTR(nram_out_r, nram_in_r);
        FFT_SWAP_PTR(nram_out_i, nram_in_i);

        TRANSPOSE_XYZ2YXZ_PAIR(nram_out_r, nram_out_i, nram_in_r, nram_in_i,
                               small_section_num, para_batch, radix, DT)

        value_mul = 8;
        // DT *sram_tw = (DT *)sram_buffer;
        DT *nram_tw = _nram_tw;

        for (; small_stage_count > 1; small_stage_count--) {
          FFT_SWAP_PTR(nram_out_r, nram_in_r);
          FFT_SWAP_PTR(nram_out_i, nram_in_i);

          // value_mul = (_small_stage_count - small_stage_count + 1) << 2;

          // // update parameter
          radix = small_factors[value_mul++];
          small_section_num = small_factors[value_mul++];
          small_butterfly_num = small_factors[value_mul++];
          small_in_stride = small_factors[value_mul++];
          // value_mul += 4;
          // copy GDRAM2SRAM

          // if (compute_id == 0 && repeat_id == 1 && 0) {
          //   __memcpy(nram_tw, small_twiddles,
          //            small_butterfly_num * (radix - 1) * sizeof(DT) * 2,
          //            GDRAM2NRAM);
          //   small_twiddles += small_butterfly_num * (radix - 1) * 2;
          // }

          if (ld_dft_radix != radix) {
            ld_dft_radix = radix;
            for (int entry = 0;; entry++) {
              if (dft_table[entry].radix == ld_dft_radix) {
                __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                         sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix,
                         SRAM2NRAM);
                break;
              }

              if (dft_table[entry].radix == -1) {
                break;
              }
            }
          }

          switch (radix) {
            case 3:
              computeRadix3ButterflyOtherstages(
                  nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                  nram_tw, small_section_num, small_butterfly_num,
                  small_in_stride, dir);
              break;
            default:
              // computeGenericButterflyOtherstages(Fout, buffer, twiddles,
              // radix, section_num, butterfly_num, in_stride, 0, dir);
              MLULOG("computeGenericButterflyOtherstagesMat: %d.\n", radix);
              computeGenericButterflyOtherstagesMat(
                  nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                  nram_dftmtx, nram_tw, small_section_num, small_butterfly_num,
                  para_batch, small_in_stride, dir, radix);
              break;
          }

          nram_tw += small_butterfly_num * (radix - 1) * 2;
        }  // for (stage_count)

        //           for (int j = 0; j < large_radix; j++) {
        //   MLULOG("output i: (%f, %f).\n", nram_out_r[j], nram_out_i[j]);
        // }

        // MLULOG("butterfly id: %d\n", i);
        // last stage
        {
          FFT_SWAP_PTR(nram_out_r, nram_in_r);
          FFT_SWAP_PTR(nram_out_i, nram_in_i);

          // copy GDRAM2SRAM

          // update parameter
          // value_mul = _small_stage_count << 2;
          radix = small_factors[value_mul++];
          small_section_num = small_factors[value_mul++];
          small_butterfly_num = small_factors[value_mul++];
          small_in_stride = small_factors[value_mul];

          // if (compute_id == 0 && repeat_id == 1 && 0) {
          //   __memcpy(nram_tw, small_twiddles,
          //            small_butterfly_num * (radix - 1) * sizeof(DT) * 2,
          //            GDRAM2NRAM);
          // }
          if (ld_dft_radix != radix) {
            ld_dft_radix = radix;
            for (int entry = 0;; entry++) {
              if (dft_table[entry].radix == ld_dft_radix) {
                __memcpy(nram_dftmtx, &dft_matrix[dft_table[entry].offset * 2],
                         sizeof(DT) * 2 * ld_dft_radix * ld_dft_radix,
                         SRAM2NRAM);
                break;
              }

              if (dft_table[entry].radix == -1) {
                break;
              }
            }
          }

          switch (radix) {
            case 2:
              break;

            default:
              MLULOG("computeGenericButterflyLaststageMat: %d.\n", radix);
              computeGenericButterflyLaststageMat(
                  nram_out_r, nram_out_i, nram_in_r, nram_in_i, nram_scratch,
                  nram_dftmtx, nram_tw, small_section_num, small_butterfly_num,
                  para_batch, small_in_stride, dir, radix);
              MLULOG("computeGenericButterflyLaststageMat: %d End.\n", radix);
              break;
          }

          // [2, para_batch, large_radix] -> [2, large_radix, para_batch]

          if (last_stage) {
            // TRANSPOSE_XYZ2YXZ_PAIR(nram_out_r, nram_out_i, nram_in_r,
            // nram_in_i, 2, para_batch, large_radix, DT)

            //  [2, para_batch, large_radix] -> [para_batch, large_radix,
            //  2]
            // DT* nram_transpose_store = nram_in_r;

            FFT_SWAP_PTR(nram_out_r, nram_in_r);
            FFT_SWAP_PTR(nram_out_i, nram_in_i);
            __bang_transpose(nram_out_r, nram_in_r, para_batch, large_radix);
            __bang_transpose(nram_out_i, nram_in_i, para_batch, large_radix);

            __bang_transpose(nram_para_store, nram_out_r, 2,
                             para_batch * large_radix);
            //  [2, para_batch, large_radix] -> [para_batch, 2, large_radix] ->
            //  [large_radix, para_batch, 2]
            // __bang_transpose(nram_para_store, nram_out_r, para_batch * 2,
            //      large_radix);
          } else {
            //  [2, para_batch, large_radix] -> [2, para_batch,
            //  large_radix]
            // TODO(zrg): test

            __bang_transpose(nram_para_store, nram_out_r, para_batch,
                             large_radix);
            __bang_transpose(nram_para_store + para_batch * large_radix,
                             nram_out_i, para_batch, large_radix);

            // __memcpy(nram_para_store, nram_out_r,
            //          para_batch * large_radix * sizeof(DT), NRAM2NRAM);
            // __memcpy(nram_para_store + para_batch * large_radix, nram_out_i,
            //          para_batch * large_radix * sizeof(DT), NRAM2NRAM);
          }
        }
      }
    }

    __sync();
  }
}

template <typename DT>
__mlu_func__ void computeMutiStageOnchipColumn(
    DT *input, DT *output, int *factors, DT *twiddles, DT *twiddles_end,
    const DT *dft_matrix, DT *buffer, int batch, int fft_flag, int direction) {
  int total_num = batch;
  int repeat_num = total_num / taskDim;
  int remain_num = total_num % taskDim;
  // MLULOG("\n\n\ntaskId: %d, taskDim: %d, coreId: %d\n\n\n", taskId, taskDim,
  //        coreId);

  // if (taskId % 4 == 0) {
  //   MLULOG("\n\n\n__is_mpu\n\n\n");
  // }

  // return;
  // __sync_cluster();
  // __nram__ uint8_t *nram_buf[NRAM_BUFFER_SIZE];
  char *nram_buf = nram_buffer;

  // Each core needs to process "t_len" blocks, "remain_num" is evenly
  // assigned to the previous "remian_num" cores.
  int t_len = repeat_num + ((remain_num > 0 && taskId < remain_num) ? 1 : 0);
  // Calculate the offset of the block at each core.
  int t_start = taskId - remain_num <= 0 ? taskId * (repeat_num + 1)
                                         : (remain_num * (repeat_num + 1) +
                                            (taskId - remain_num) * repeat_num);
  int t_end = (t_start + t_len);

  MLULOG(
      "taskId: %d, repeat_num: %d, "
      "remain_num: %d, t_len: %d, t_start: %d, t_end: %d\n",
      taskId, repeat_num, remain_num, t_len, t_start, t_end);

  int radix, section_num, butterfly_num, in_stride, stage_count, value_mul,
      small_factors_offset;
  // const int is_two_stages = (stage_count == 2);  // the variable for
  // twostages
  // const int is_in_place = (input == output);
  // const DT *_twiddles = twiddles;
  int *small_factors;
  int last_stage;
  // __sync_io();
  __nram__ int nram_factors[FFT_MAXFACTORS];
  // __mlu_shared__ int sram_factors[FFT_MAXFACTORS];
  // __mlu_shared__ DT sram_dftmtx[DFT_TABLE_SIZE];

  int sram_offset = 0;
  int *sram_factors = (int *)(sram_buffer + sram_offset);
  sram_offset += FFT_MAXFACTORS * sizeof(int);
  DT *sram_dftmtx = (DT *)(sram_buffer + sram_offset);
  sram_offset += DFT_TABLE_SIZE * sizeof(DT);
  DT *sram_twiddles = (DT *)(sram_buffer + sram_offset);
  const int twiddles_size = twiddles_end - twiddles;
  sram_offset += twiddles_size * sizeof(DT);

  const int _stage_count = factors[0];
  const int nfft = factors[1];
  if (__is_ipu()) MLULOG("nfft: %d\n", nfft);

  // first stage
  radix = factors[5 + 0];
  section_num = factors[5 + 1];
  in_stride = factors[5 + 3];
  small_factors_offset = factors[5 + 4];

  small_factors = factors + small_factors_offset;

  stage_count = _stage_count;
  last_stage = (stage_count == 1);

  if (__is_mpu()) {
    __memcpy_async(sram_factors, factors, FFT_MAXFACTORS * sizeof(int),
                   GDRAM2SRAM);
    __memcpy_async(sram_twiddles, twiddles, twiddles_size * sizeof(DT),
                   GDRAM2SRAM);
    // _small_stage_count = small_factors[0];

    const dft_table_entry *dft_table_gdram =
        (const dft_table_entry *)dft_matrix;
    int dft_matrix_offset = dft_table_gdram[0].offset;

    if (dft_matrix_offset != -1) {
      // copy the table
      __memcpy(sram_dftmtx, dft_matrix, sizeof(DT) * 2 * dft_matrix_offset,
               GDRAM2SRAM);
      const dft_table_entry *dft_table = (const dft_table_entry *)sram_dftmtx;

      for (int entry = 0;; entry++) {
        if (dft_table[entry + 1].radix == -1) {
          int last_radix = dft_table[entry].radix;
          int last_offset = dft_table[entry].offset;
          __memcpy_async(
              sram_dftmtx, dft_matrix,
              sizeof(DT) * 2 * (last_radix * last_radix + last_offset),
              GDRAM2SRAM);
          break;
        }
      }
    }
    factors = sram_factors;
    twiddles = sram_twiddles;
  }

  __sync_cluster();
  if (__is_ipu()) {
    __memcpy(nram_factors, sram_factors, FFT_MAXFACTORS * sizeof(int),
             SRAM2NRAM);
    factors = nram_factors;
  }

  if (__is_mpu()) {
    return;
  }

  DT *_twiddles = twiddles;

  DT *odd_extra_buffer;
  int max_para_batch = 375;
  //  int max_para_batch = 6000/64;

  if (__is_ipu()) {
    // FFT_CPX_T<DT> *odd_extra_buffer = (FFT_CPX_T<DT> *)buffer + batch * nfft;
    // // for in_place temp buffer
    odd_extra_buffer =
        buffer + batch * (nfft << 1);  // for in_place temp buffer
    // out_place: input -> output (1 stage)
    //            input -> buffer -> output (2 stage)
    //            input -> buffer -> odd_extra_buffer -> output (3 stage)
    //            input -> buffer -> output -> buffer -> output (4 stage)
    //            input -> buffer -> output -> buffer -> odd_extra_buffer ->
    //            output (5 stage)

    // _stage_count = stage_count;

    if (_stage_count != 1) FFT_SWAP_PTR(buffer, output);

    // int max_para_batch = (6000 + radix - 1) / radix;

    if (repeat_num > 0 || taskId < remain_num) {
      for (int t = t_start; t < t_end; t += max_para_batch) {
        // MLULOG("taskId: %d, batchId: %d\n", taskId, t);
        int para_batch =
            (max_para_batch < (t_end - t)) ? max_para_batch : (t_end - t);
        DT *input_batch = input + t * 2;
        DT *output_batch;
        if (last_stage) {
          output_batch = output + t * 2;
        } else {
          output_batch = output + t * nfft * 2;
        }
        // DT *buffer_batch = buffer + t * (nfft * 2);
        // DT *odd_extra_buffer_batch = odd_extra_buffer + t * (nfft * 2);

        // first stage
        // int radix

        // MLULOG("para_batch: %d\n", para_batch);
        int nb0 = nfft;
        int nb1 = batch;
        // if(0)
        computeLargeButterflyFirststageColumn<DT>(
            output_batch, input_batch, in_stride, section_num, twiddles,
            sram_dftmtx, (void *)nram_buf, small_factors, direction, nfft,
            last_stage, para_batch, nb0, nb1);
      }
    }
    // __sync();
  } else {
    stage_count = _stage_count;
    last_stage = (stage_count == 1);
  }
  // return;

  // sram_large_tw
  stage_count--;
  if (stage_count == 0) {
    // continue;
    return;
  }

  // if (__is_mpu()) {
  //   return;
  // }

  // sram_large_tw
  value_mul = 10;
  for (; stage_count > 1; stage_count--) {
    // fft_swap_ptr<DT>(&buffer, &output);
    // FFT_SWAP_PTR(buffer, output);
    FFT_SWAP_PTR(buffer, output);

    // if (is_two_stages && is_in_place) {
    //   if (_stage_count % 2 == 0) {
    //     if ((_stage_count - stage_count) == 2)
    //       fft_swap_ptr<DT>(&odd_extra_buffer, &output);
    //   } else {
    //     if ((_stage_count - stage_count) == 3)
    //       fft_swap_ptr<DT>(&odd_extra_buffer, &output);
    //   }
    // }

    if (stage_count == 2 && _stage_count % 2) {
      // fft_swap_ptr<DT>(&odd_extra_buffer, &output);
      FFT_SWAP_PTR(odd_extra_buffer, output);
    }

    // value_mul = (_stage_count - stage_count + 1) * 5;

    // update parameter
    radix = factors[value_mul++];
    section_num = factors[value_mul++];
    butterfly_num = factors[value_mul++];
    in_stride = factors[value_mul++];
    small_factors_offset = factors[value_mul++];

    small_factors = factors + small_factors_offset;

    if (__is_ipu()) {
      // MLULOG("other stage radix: %d \n", radix);
      // int max_para_batch = (6000 + radix - 1) / radix;
      if (repeat_num > 0 || taskId < remain_num) {
        // for (int t = t_start; t < t_end; t++) {
        //   // DT *output_batch = output + t * 2;
        //   // DT *buffer_batch = buffer + t * 2;

        //     computeLargeButterflyOtherstagesColumn<DT>(
        //         output_batch, buffer_batch, (DT *)twiddles, _twiddles,
        //         sram_dftmtx, section_num, butterfly_num, in_stride,
        //         (void *)nram_buf, small_factors, nfft, direction, 0,
        //         last_stage, para_batch, nb0, nb1);
        // }

        for (int t = t_start; t < t_end; t += max_para_batch) {
          // MLULOG("taskId: %d, batchId: %d\n", taskId, t);
          DT *output_batch = output + t * (nfft * 2);
          DT *buffer_batch = buffer + t * (nfft * 2);
          // DT *buffer_batch = buffer + t * (nfft * 2);
          // DT *odd_extra_buffer_batch = odd_extra_buffer + t * (nfft * 2);

          // first stage
          // int radix
          int para_batch =
              (max_para_batch < (t_end - t)) ? max_para_batch : (t_end - t);
          // MLULOG("para_batch: %d\n", para_batch);
          int nb0 = nfft;
          int nb1 = batch;
          // if(0)
          computeLargeButterflyOtherstagesColumn<DT>(
              output_batch, buffer_batch, (DT *)twiddles, _twiddles,
              sram_dftmtx, section_num, butterfly_num, in_stride,
              (void *)nram_buf, small_factors, nfft, direction, 0, para_batch,
              nb0, nb1);
        }
      }
    }
    twiddles += butterfly_num * (radix - 1) * 2;  // 2 for complex
  }                                               // for (stage_count)

  // __mlu_shared__ DT *sram_tw[2048];  // radix-1024
  // __mlu_shared__ DT *sram_tw[64];  // radix-1024
  // last stage
  {
    if ((_stage_count % 2 == 1)) {
      FFT_SWAP_PTR(odd_extra_buffer, buffer);
    }

    // fft_swap_ptr<DT>(&buffer, &output);
    FFT_SWAP_PTR(buffer, output);

    // update parameter
    radix = factors[value_mul++];
    section_num = factors[value_mul++];
    butterfly_num = factors[value_mul++];
    in_stride = factors[value_mul++];
    small_factors_offset = factors[value_mul];

    small_factors = factors + small_factors_offset;

    // DT * sram_la = sram_large_tw

    // if (__is_mpu()) {
    //   __memcpy(sram_tw, twiddles, sizeof(DT) * 2 * butterfly_num * (radix -
    //   1),
    //            GDRAM2SRAM);
    // }

    // __sync_cluster();
    // // imag
    // __memcpy(
    //     sram_large_tw + butterfly_num * (radix - 1),
    //     twiddles + + butterfly_num * (radix - 1),
    //     sizeof(DT) * para_ldst_num, GDRAM2NRAM,
    //     sizeof(DT) * para_ldst_num, large_out_stride * sizeof(DT),
    //     large_radix - 2);

    if (__is_ipu()) {
      MLULOG("last stage radix: %d, section_num: %d\n", radix, section_num);

      if (repeat_num > 0 || taskId < remain_num) {
        // for (int t = t_start; t < t_end; t++) {
        //   DT *output_batch = output + t * (nfft << 1);
        //   DT *buffer_batch = buffer + t * (nfft << 1);

        //   computeLargeButterflyLaststageColumn<DT>(
        //       output_batch, buffer_batch, (DT *)twiddles, _twiddles,
        //       sram_dftmtx, section_num, butterfly_num, in_stride,
        //       (void *)nram_buf, small_factors, nfft, direction,
        //       last_stage, para_batch, nb0, nb1);
        // }
        // int max_para_batch = (6000 + radix - 1) / radix;
        for (int t = t_start; t < t_end; t += max_para_batch) {
          // MLULOG("taskId: %d, batchId: %d\n", taskId, t);
          DT *output_batch = output + t * 2;
          DT *buffer_batch = buffer + t * (nfft * 2);
          // DT *buffer_batch = buffer + t * (nfft * 2);
          // DT *odd_extra_buffer_batch = odd_extra_buffer + t * (nfft * 2);

          // first stage
          // int radix
          int para_batch =
              (max_para_batch < (t_end - t)) ? max_para_batch : (t_end - t);
          // MLULOG("para_batch: %d\n", para_batch);
          int nb0 = nfft;
          int nb1 = batch;
          // if(0)
          computeLargeButterflyLaststageColumn<DT>(
              output_batch, buffer_batch, (DT *)twiddles, _twiddles,
              sram_dftmtx, section_num, butterfly_num, in_stride,
              (void *)nram_buf, small_factors, nfft, direction, para_batch, nb0,
              nb1);
        }
      }
    }
  }
}

__mlu_global__ void MLUKernelFFTButterflyColumn(void *input, void *output,
                                                int *factors, void *twiddles,
                                                void *twiddles_end,
                                                void *dft_matrix, void *buffer,
                                                int batch, int fft_flag,
                                                int direction, int dtype_size) {
  // if (__is_mpu()) return;
  // if (coreId == 0x80) {
  //     MLULOG("\n\n\nmemcore\n\n");
  // }
  // if (taskId == 0) {
  //     MLULOG("\n\n\nmemcore\n\n");
  // }

  // MLULOG("batch: %d\n", batch);
  switch (dtype_size) {
    case (MLUOP_DTYPE_COMPLEX_FLOAT):
    case (MLUOP_DTYPE_FLOAT): {
      // MLULOG("MLUOP_DTYPE_COMPLEX_FLOAT: MLUOP_DTYPE_FLOAT\n");
      computeMutiStageOnchipColumn<float>(
          (float *)input, (float *)output, factors, (float *)twiddles,
          (float *)twiddles_end, (float *)dft_matrix, (float *)buffer, batch,
          fft_flag, direction);
    }; break;
    case (MLUOP_DTYPE_COMPLEX_HALF):
    case (MLUOP_DTYPE_HALF): {
      // MLULOG("MLUOP_DTYPE_COMPLEX_HALF: MLUOP_DTYPE_HALF\n");
      computeMutiStageOnchipColumn<half>((half *)input, (half *)output, factors,
                                         (half *)twiddles, (half *)twiddles_end,
                                         (half *)dft_matrix, (half *)buffer,
                                         batch, fft_flag, direction);
    }; break;

    default: {
      MLULOG("mluOpFFT Not Implemented.");
    }
  }
}

mluOpStatus_t MLUOP_WIN_API kernelFFTButterflyColumn(
    cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue,
    mluOpFFTPlan_t fft_plan, int direction, FFTFlag flag) {
  VLOG(5) << "Launch Kernel kernelFFTButterfly <<Union" << k_type / CORE_DIM
          << ", " << k_dim.x << ", " << k_dim.y << ", " << k_dim.z << ">>>";
  KERNEL_CHECK((MLUKernelFFTButterflyColumn<<<k_dim, k_type, queue>>>(
      fft_plan->mlu_addrs.output, fft_plan->mlu_addrs.output,
      fft_plan->mlu_addrs.factors_2d, fft_plan->mlu_addrs.twiddles_2d,
      fft_plan->mlu_addrs.twiddles_2d_end, fft_plan->mlu_addrs.dft_matrix_2d,
      fft_plan->mlu_addrs.buffer_buf, fft_plan->batch_2d, flag,
      direction,  // direction, -1 means invalid(only FFT_IFFT use).
      fft_plan->output_dtype)));
  return MLUOP_STATUS_SUCCESS;
}
